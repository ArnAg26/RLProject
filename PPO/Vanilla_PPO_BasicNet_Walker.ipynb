{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-06T05:37:11.977779Z",
     "iopub.status.busy": "2024-12-06T05:37:11.976747Z",
     "iopub.status.idle": "2024-12-06T05:37:12.401103Z",
     "shell.execute_reply": "2024-12-06T05:37:12.400321Z",
     "shell.execute_reply.started": "2024-12-06T05:37:11.977729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:29:28.861320Z",
     "iopub.status.busy": "2024-12-06T08:29:28.860434Z",
     "iopub.status.idle": "2024-12-06T08:29:41.137567Z",
     "shell.execute_reply": "2024-12-06T08:29:41.136322Z",
     "shell.execute_reply.started": "2024-12-06T08:29:28.861280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:30:54.048557Z",
     "iopub.status.busy": "2024-12-06T08:30:54.047870Z",
     "iopub.status.idle": "2024-12-06T08:30:57.675563Z",
     "shell.execute_reply": "2024-12-06T08:30:57.674880Z",
     "shell.execute_reply.started": "2024-12-06T08:30:54.048508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:37:30.088788Z",
     "iopub.status.busy": "2024-12-06T05:37:30.087721Z",
     "iopub.status.idle": "2024-12-06T05:37:30.095026Z",
     "shell.execute_reply": "2024-12-06T05:37:30.093903Z",
     "shell.execute_reply.started": "2024-12-06T05:37:30.088737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Simplified Value Network\n",
    "# class ValueNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "#         super(ValueNetwork, self).__init__()\n",
    "#         self.activation = activation()\n",
    "#         self.layers = nn.ModuleList()\n",
    "        \n",
    "#         # Hidden layers\n",
    "#         input_size = state_dim\n",
    "#         for hidden_size in hidden_sizes:\n",
    "#             self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "#             input_size = hidden_size\n",
    "\n",
    "#         # Output layer\n",
    "#         self.output_layer = nn.Linear(input_size, 1)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         x = state\n",
    "#         for layer in self.layers:\n",
    "#             x = self.activation(layer(x))\n",
    "#         value = self.output_layer(x)\n",
    "#         return value\n",
    "\n",
    "\n",
    "# # Simplified Policy Network\n",
    "# class PolicyNetwork(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim, discrete=True, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "#         super(PolicyNetwork, self).__init__()\n",
    "#         self.discrete = discrete\n",
    "#         self.activation = activation()\n",
    "#         self.layers = nn.ModuleList()\n",
    "        \n",
    "#         # Hidden layers\n",
    "#         input_size = state_dim\n",
    "#         for hidden_size in hidden_sizes:\n",
    "#             self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "#             input_size = hidden_size\n",
    "\n",
    "#         # Output layer\n",
    "#         if self.discrete:\n",
    "#             # Discrete actions: output probabilities for each action\n",
    "#             self.output_layer = nn.Linear(input_size, action_dim)\n",
    "#         else:\n",
    "#             # Continuous actions: output mean and log_std for each action\n",
    "#             self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "#             self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         x = state\n",
    "#         for layer in self.layers:\n",
    "#             x = self.activation(layer(x))\n",
    "        \n",
    "#         if self.discrete:\n",
    "#             # Discrete actions: apply softmax for probabilities\n",
    "#             logits = self.output_layer(x)\n",
    "#             action_probs = F.softmax(logits, dim=-1)\n",
    "#             return action_probs\n",
    "#         else:\n",
    "#             # Continuous actions: return mean and std\n",
    "#             mean = self.mean_layer(x)\n",
    "#             std = torch.exp(self.log_std)\n",
    "#             return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:25:29.226390Z",
     "iopub.status.busy": "2024-12-06T06:25:29.225918Z",
     "iopub.status.idle": "2024-12-06T06:25:29.251027Z",
     "shell.execute_reply": "2024-12-06T06:25:29.250194Z",
     "shell.execute_reply.started": "2024-12-06T06:25:29.226360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simplified Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Hidden layers\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        value = self.output_layer(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Simplified Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.discrete = discrete\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Hidden layers\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        if self.discrete:\n",
    "            # Discrete actions: output probabilities for each action\n",
    "            self.output_layer = nn.Linear(input_size, action_dim)\n",
    "        else:\n",
    "            # Continuous actions: output mean and log_std for each action\n",
    "            self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        if self.discrete:\n",
    "            # Discrete actions: apply softmax for probabilities\n",
    "            logits = self.output_layer(x)\n",
    "            action_probs = F.softmax(logits, dim=-1)\n",
    "            return action_probs\n",
    "        else:\n",
    "            # Continuous actions: return mean and std\n",
    "            mean = self.mean_layer(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            return mean, std\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, discrete, gamma=0.99, lam=0.95, eps_clip=0.2, lr=4e-4, k_epochs=4):\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lam = lam  # GAE lambda\n",
    "        self.eps_clip = eps_clip  # Clipping epsilon\n",
    "        self.k_epochs = k_epochs  # Number of PPO epochs\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, discrete).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.value_net = ValueNetwork(state_dim).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if isinstance(self.policy_net, PolicyNetwork) and self.policy_net.discrete:\n",
    "            action_probs = self.policy_net(state)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            return action.item(), action_dist.log_prob(action)\n",
    "        else:\n",
    "            mean, std = self.policy_net(state)\n",
    "            action_dist = torch.distributions.Normal(mean, std)\n",
    "            action = action_dist.sample()\n",
    "            return action.cpu().numpy(), action_dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_advantages(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            # Initialize trajectory variables\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "            \n",
    "            # Reset environment and get the initial state\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                with torch.no_grad():\n",
    "                    value = self.value_net(state).squeeze(0)\n",
    "                    action, log_prob = self.select_action(state.cpu().numpy())\n",
    "                \n",
    "                # Interact with the environment\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Store trajectory data\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                \n",
    "                # Update the state\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Ensure valid trajectory data\n",
    "            if len(states) == 0:\n",
    "                print(\"No valid states collected; skipping this episode.\")\n",
    "                continue\n",
    "            \n",
    "            # Compute advantages and returns\n",
    "            values.append(torch.tensor([0], device=self.device))  # Bootstrap value\n",
    "            advantages = self.compute_advantages(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "            \n",
    "            # Optimize policy and value networks\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(np.array(actions), dtype=torch.float32 if not self.policy_net.discrete else torch.long).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "        \n",
    "            for _ in range(self.k_epochs):\n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i+batch_size]\n",
    "                    batch_actions = actions[i:i+batch_size]\n",
    "                    batch_log_probs = log_probs[i:i+batch_size]\n",
    "                    batch_advantages = advantages[i:i+batch_size]\n",
    "                    batch_returns = returns[i:i+batch_size]\n",
    "    \n",
    "                    # Policy update\n",
    "                    if self.policy_net.discrete:\n",
    "                        # Discrete action space\n",
    "                        action_probs = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Categorical(action_probs)\n",
    "                        new_log_probs = dist.log_prob(batch_actions)\n",
    "                    else:\n",
    "                        # Continuous action space\n",
    "                        mean, std = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Normal(mean, std)\n",
    "                        new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "                    # PPO objective\n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    # Value update\n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "    \n",
    "                    # Backpropagation\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    (policy_loss + 0.5 * value_loss).backward()\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "    \n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}\")\n",
    "\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:37:30.131423Z",
     "iopub.status.busy": "2024-12-06T05:37:30.130837Z",
     "iopub.status.idle": "2024-12-06T05:43:40.836358Z",
     "shell.execute_reply": "2024-12-06T05:43:40.835297Z",
     "shell.execute_reply.started": "2024-12-06T05:37:30.131381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -0.711169958114624, Value Loss = 3.713970184326172\n",
      "Episode 2: Policy Loss = 0.16402876377105713, Value Loss = 18.072729110717773\n",
      "Episode 3: Policy Loss = -4.6198930740356445, Value Loss = 26.06911277770996\n",
      "Episode 4: Policy Loss = -0.7623547315597534, Value Loss = 31.41605567932129\n",
      "Episode 5: Policy Loss = -5.892380237579346, Value Loss = 105.7132797241211\n",
      "Episode 6: Policy Loss = -7.026723861694336, Value Loss = 64.5262451171875\n",
      "Episode 7: Policy Loss = -8.109270095825195, Value Loss = 125.22000122070312\n",
      "Episode 8: Policy Loss = 7.952657222747803, Value Loss = 62.905513763427734\n",
      "Episode 9: Policy Loss = 3.5639991760253906, Value Loss = 96.91584014892578\n",
      "Episode 10: Policy Loss = -18.898263931274414, Value Loss = 430.29791259765625\n",
      "Episode 11: Policy Loss = 3.8940212726593018, Value Loss = 111.63114929199219\n",
      "Episode 12: Policy Loss = 2.5102245807647705, Value Loss = 84.9892578125\n",
      "Episode 13: Policy Loss = 5.120128631591797, Value Loss = 148.0777587890625\n",
      "Episode 14: Policy Loss = -9.573314666748047, Value Loss = 220.19972229003906\n",
      "Episode 15: Policy Loss = -1.2911006212234497, Value Loss = 130.82310485839844\n",
      "Episode 16: Policy Loss = -14.958755493164062, Value Loss = 289.7041931152344\n",
      "Episode 17: Policy Loss = 4.319805145263672, Value Loss = 194.75827026367188\n",
      "Episode 18: Policy Loss = 2.004000663757324, Value Loss = 299.34722900390625\n",
      "Episode 19: Policy Loss = 5.7925028800964355, Value Loss = 145.0135955810547\n",
      "Episode 20: Policy Loss = 5.016510963439941, Value Loss = 173.71925354003906\n",
      "Episode 21: Policy Loss = 3.524364948272705, Value Loss = 459.33447265625\n",
      "Episode 22: Policy Loss = 1.8625681400299072, Value Loss = 275.8102111816406\n",
      "Episode 23: Policy Loss = -0.296630859375, Value Loss = 676.3448486328125\n",
      "Episode 24: Policy Loss = 9.337926864624023, Value Loss = 303.0209655761719\n",
      "Episode 25: Policy Loss = 10.58064079284668, Value Loss = 480.7309875488281\n",
      "Episode 26: Policy Loss = 6.56870698928833, Value Loss = 437.5383605957031\n",
      "Episode 27: Policy Loss = 15.306199073791504, Value Loss = 682.6153564453125\n",
      "Episode 28: Policy Loss = 14.387687683105469, Value Loss = 720.0341186523438\n",
      "Episode 29: Policy Loss = 0.49380922317504883, Value Loss = 412.9305419921875\n",
      "Episode 30: Policy Loss = 13.870031356811523, Value Loss = 452.9894714355469\n",
      "Episode 31: Policy Loss = 14.652008056640625, Value Loss = 842.5597534179688\n",
      "Episode 32: Policy Loss = -0.8000695705413818, Value Loss = 504.2765808105469\n",
      "Episode 33: Policy Loss = 16.38992691040039, Value Loss = 605.7083740234375\n",
      "Episode 34: Policy Loss = 12.575292587280273, Value Loss = 641.341552734375\n",
      "Episode 35: Policy Loss = 7.1982622146606445, Value Loss = 559.6220092773438\n",
      "Episode 36: Policy Loss = -1.1346659660339355, Value Loss = 806.4553833007812\n",
      "Episode 37: Policy Loss = 12.375112533569336, Value Loss = 638.3071899414062\n",
      "Episode 38: Policy Loss = 24.22580337524414, Value Loss = 1312.968017578125\n",
      "Episode 39: Policy Loss = 0.8638191223144531, Value Loss = 610.6173706054688\n",
      "Episode 40: Policy Loss = -2.810330390930176, Value Loss = 725.7428588867188\n",
      "Episode 41: Policy Loss = 12.321922302246094, Value Loss = 1394.39892578125\n",
      "Episode 42: Policy Loss = 1.42291259765625, Value Loss = 687.0625\n",
      "Episode 43: Policy Loss = 20.050785064697266, Value Loss = 984.5863037109375\n",
      "Episode 44: Policy Loss = 16.66704750061035, Value Loss = 888.54443359375\n",
      "Episode 45: Policy Loss = 20.118911743164062, Value Loss = 896.2757568359375\n",
      "Episode 46: Policy Loss = 6.995206832885742, Value Loss = 744.7579956054688\n",
      "Episode 47: Policy Loss = 28.458362579345703, Value Loss = 1687.122802734375\n",
      "Episode 48: Policy Loss = 21.328346252441406, Value Loss = 987.2970581054688\n",
      "Episode 49: Policy Loss = 17.19308090209961, Value Loss = 982.67138671875\n",
      "Episode 50: Policy Loss = 15.146513938903809, Value Loss = 989.2029418945312\n",
      "Episode 51: Policy Loss = 15.408830642700195, Value Loss = 911.2216796875\n",
      "Episode 52: Policy Loss = 2.7302751541137695, Value Loss = 816.1264038085938\n",
      "Episode 53: Policy Loss = 19.614818572998047, Value Loss = 1078.218505859375\n",
      "Episode 54: Policy Loss = 0.8073835372924805, Value Loss = 819.4873046875\n",
      "Episode 55: Policy Loss = 21.612747192382812, Value Loss = 1790.484375\n",
      "Episode 56: Policy Loss = 17.82815933227539, Value Loss = 1737.852783203125\n",
      "Episode 57: Policy Loss = 19.9843807220459, Value Loss = 1156.6214599609375\n",
      "Episode 58: Policy Loss = 18.008047103881836, Value Loss = 1127.81396484375\n",
      "Episode 59: Policy Loss = 15.010843276977539, Value Loss = 1714.2720947265625\n",
      "Episode 60: Policy Loss = 33.223934173583984, Value Loss = 2036.228271484375\n",
      "Episode 61: Policy Loss = 19.30606460571289, Value Loss = 1143.8505859375\n",
      "Episode 62: Policy Loss = 6.105125904083252, Value Loss = 930.2906494140625\n",
      "Episode 63: Policy Loss = 33.03020095825195, Value Loss = 2056.414306640625\n",
      "Episode 64: Policy Loss = 3.59462833404541, Value Loss = 990.7131958007812\n",
      "Episode 65: Policy Loss = 12.498686790466309, Value Loss = 936.4382934570312\n",
      "Episode 66: Policy Loss = 22.294828414916992, Value Loss = 1317.1905517578125\n",
      "Episode 67: Policy Loss = 11.483521461486816, Value Loss = 944.98046875\n",
      "Episode 68: Policy Loss = 19.0546817779541, Value Loss = 1100.1826171875\n",
      "Episode 69: Policy Loss = 29.659141540527344, Value Loss = 2051.092529296875\n",
      "Episode 70: Policy Loss = 16.224349975585938, Value Loss = 1065.9539794921875\n",
      "Episode 71: Policy Loss = 25.640689849853516, Value Loss = 1431.911376953125\n",
      "Episode 72: Policy Loss = 32.64054870605469, Value Loss = 2080.1064453125\n",
      "Episode 73: Policy Loss = 21.64779281616211, Value Loss = 1237.9442138671875\n",
      "Episode 74: Policy Loss = 15.550111770629883, Value Loss = 1074.8182373046875\n",
      "Episode 75: Policy Loss = 25.711719512939453, Value Loss = 2100.843017578125\n",
      "Episode 76: Policy Loss = 17.55735206604004, Value Loss = 1149.227294921875\n",
      "Episode 77: Policy Loss = 30.590038299560547, Value Loss = 2186.920166015625\n",
      "Episode 78: Policy Loss = 18.27978515625, Value Loss = 1156.8714599609375\n",
      "Episode 79: Policy Loss = 20.808155059814453, Value Loss = 1226.7330322265625\n",
      "Episode 80: Policy Loss = 32.246917724609375, Value Loss = 2262.34228515625\n",
      "Episode 81: Policy Loss = 16.306406021118164, Value Loss = 1096.2630615234375\n",
      "Episode 82: Policy Loss = 27.498735427856445, Value Loss = 2208.013916015625\n",
      "Episode 83: Policy Loss = 18.22260856628418, Value Loss = 1136.3289794921875\n",
      "Episode 84: Policy Loss = 21.07946014404297, Value Loss = 1282.0692138671875\n",
      "Episode 85: Policy Loss = 14.74879264831543, Value Loss = 1086.973388671875\n",
      "Episode 86: Policy Loss = 15.139507293701172, Value Loss = 1092.0450439453125\n",
      "Episode 87: Policy Loss = 34.15746307373047, Value Loss = 2150.529541015625\n",
      "Episode 88: Policy Loss = 20.553028106689453, Value Loss = 1321.6748046875\n",
      "Episode 89: Policy Loss = 24.368427276611328, Value Loss = 1655.443359375\n",
      "Episode 90: Policy Loss = 11.726300239562988, Value Loss = 1009.380126953125\n",
      "Episode 91: Policy Loss = 14.002446174621582, Value Loss = 1060.98291015625\n",
      "Episode 92: Policy Loss = 17.862585067749023, Value Loss = 389.568603515625\n",
      "Episode 93: Policy Loss = 34.78943634033203, Value Loss = 2191.948974609375\n",
      "Episode 94: Policy Loss = 15.218669891357422, Value Loss = 1078.520751953125\n",
      "Episode 95: Policy Loss = 20.54237937927246, Value Loss = 1264.8717041015625\n",
      "Episode 96: Policy Loss = 23.877857208251953, Value Loss = 1332.6654052734375\n",
      "Episode 97: Policy Loss = 22.1524600982666, Value Loss = 1398.8697509765625\n",
      "Episode 98: Policy Loss = 29.55060577392578, Value Loss = 1871.12744140625\n",
      "Episode 99: Policy Loss = 24.630229949951172, Value Loss = 2084.52978515625\n",
      "Episode 100: Policy Loss = 18.258543014526367, Value Loss = 1135.3826904296875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    VanillaAgent = PPOAgent(state_dim, action_dim, discrete)\n",
    "    VanillaAgent.train(env, max_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:03.543911Z",
     "iopub.status.busy": "2024-12-06T12:46:03.543562Z",
     "iopub.status.idle": "2024-12-06T12:46:03.553876Z",
     "shell.execute_reply": "2024-12-06T12:46:03.552918Z",
     "shell.execute_reply.started": "2024-12-06T12:46:03.543881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_state_value_attack(env, policy_net, value_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a State Value Attack using a value network.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        value_net (torch.nn.Module): The trained value network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under the state value attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute value for the perturbed state\n",
    "                value = value_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Minimize or maximize the value\n",
    "                loss = -value.mean()  # Gradient ascent to maximize adversarial effect\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                action_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(action_output, tuple):\n",
    "                    action = action_output[0]  # Extract mean for continuous actions\n",
    "                else:\n",
    "                    action = action_output\n",
    "\n",
    "                action = action.squeeze().cpu().numpy()  # Ensure the action is in NumPy format\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under State Value Attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:43:40.851051Z",
     "iopub.status.busy": "2024-12-06T05:43:40.850764Z",
     "iopub.status.idle": "2024-12-06T05:48:45.406574Z",
     "shell.execute_reply": "2024-12-06T05:48:45.405619Z",
     "shell.execute_reply.started": "2024-12-06T05:43:40.851024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 277.5040888230642\n",
      "Episode 2/200: Reward = 275.21869895097734\n",
      "Episode 3/200: Reward = 280.26230589007884\n",
      "Episode 4/200: Reward = 267.99155310376904\n",
      "Episode 5/200: Reward = 274.5132213680195\n",
      "Episode 6/200: Reward = 273.1316942692741\n",
      "Episode 7/200: Reward = 275.98681348046676\n",
      "Episode 8/200: Reward = 277.20342597783485\n",
      "Episode 9/200: Reward = 277.8727785435308\n",
      "Episode 10/200: Reward = 271.5660568502503\n",
      "Episode 11/200: Reward = 267.644295070306\n",
      "Episode 12/200: Reward = 272.9572465155501\n",
      "Episode 13/200: Reward = 274.47898012237863\n",
      "Episode 14/200: Reward = 271.8896562712158\n",
      "Episode 15/200: Reward = 267.07459428541245\n",
      "Episode 16/200: Reward = 272.6806804096167\n",
      "Episode 17/200: Reward = 275.87171640103696\n",
      "Episode 18/200: Reward = 271.30452303080966\n",
      "Episode 19/200: Reward = 268.70303329237987\n",
      "Episode 20/200: Reward = 282.21880840957255\n",
      "Episode 21/200: Reward = 273.4949409441501\n",
      "Episode 22/200: Reward = 276.93942956535017\n",
      "Episode 23/200: Reward = 267.8565779169697\n",
      "Episode 24/200: Reward = 277.68826423794724\n",
      "Episode 25/200: Reward = 276.3167830710323\n",
      "Episode 26/200: Reward = 281.3281297191955\n",
      "Episode 27/200: Reward = 267.6283011212238\n",
      "Episode 28/200: Reward = 269.1537919395831\n",
      "Episode 29/200: Reward = 265.66384144225015\n",
      "Episode 30/200: Reward = 282.07479322922717\n",
      "Episode 31/200: Reward = 275.2910247091648\n",
      "Episode 32/200: Reward = 263.26101075709573\n",
      "Episode 33/200: Reward = 271.07917102565466\n",
      "Episode 34/200: Reward = 273.3823031911577\n",
      "Episode 35/200: Reward = 273.7043637151236\n",
      "Episode 36/200: Reward = 277.3327874581681\n",
      "Episode 37/200: Reward = 274.95968341713603\n",
      "Episode 38/200: Reward = 268.7456678523874\n",
      "Episode 39/200: Reward = 276.84173232106474\n",
      "Episode 40/200: Reward = 279.23039029158616\n",
      "Episode 41/200: Reward = 271.29487646277994\n",
      "Episode 42/200: Reward = 272.3505839725973\n",
      "Episode 43/200: Reward = 275.1345025893225\n",
      "Episode 44/200: Reward = 275.776068847619\n",
      "Episode 45/200: Reward = 279.028219161955\n",
      "Episode 46/200: Reward = 273.73392186489315\n",
      "Episode 47/200: Reward = 274.4555720849789\n",
      "Episode 48/200: Reward = 270.5811078516464\n",
      "Episode 49/200: Reward = 276.4632947990585\n",
      "Episode 50/200: Reward = 276.8030685790745\n",
      "Episode 51/200: Reward = 276.08135972685596\n",
      "Episode 52/200: Reward = 271.6494216819588\n",
      "Episode 53/200: Reward = 275.10965151622287\n",
      "Episode 54/200: Reward = 275.542652399192\n",
      "Episode 55/200: Reward = 269.8625093280917\n",
      "Episode 56/200: Reward = 271.39366986098366\n",
      "Episode 57/200: Reward = 273.2386667385343\n",
      "Episode 58/200: Reward = 273.17758649201403\n",
      "Episode 59/200: Reward = 275.49772726110825\n",
      "Episode 60/200: Reward = 273.4883001380036\n",
      "Episode 61/200: Reward = 273.2018687919135\n",
      "Episode 62/200: Reward = 272.4935869812407\n",
      "Episode 63/200: Reward = 272.7715082240025\n",
      "Episode 64/200: Reward = 271.202942824022\n",
      "Episode 65/200: Reward = 271.997207268187\n",
      "Episode 66/200: Reward = 272.41154538382153\n",
      "Episode 67/200: Reward = 275.4252645061666\n",
      "Episode 68/200: Reward = 277.55455823963604\n",
      "Episode 69/200: Reward = 284.70392820689455\n",
      "Episode 70/200: Reward = 262.5311596177625\n",
      "Episode 71/200: Reward = 281.2347169706837\n",
      "Episode 72/200: Reward = 275.20053800143074\n",
      "Episode 73/200: Reward = 269.0882147223217\n",
      "Episode 74/200: Reward = 279.46453600068577\n",
      "Episode 75/200: Reward = 267.5862602597501\n",
      "Episode 76/200: Reward = 275.15978246649286\n",
      "Episode 77/200: Reward = 270.6809362071289\n",
      "Episode 78/200: Reward = 269.17307574056855\n",
      "Episode 79/200: Reward = 274.4692387997374\n",
      "Episode 80/200: Reward = 271.6173036904955\n",
      "Episode 81/200: Reward = 268.65346256505745\n",
      "Episode 82/200: Reward = 267.48235124650967\n",
      "Episode 83/200: Reward = 270.5019472504587\n",
      "Episode 84/200: Reward = 273.10773233621063\n",
      "Episode 85/200: Reward = 273.2643982417624\n",
      "Episode 86/200: Reward = 277.4890970217402\n",
      "Episode 87/200: Reward = 276.70599798195957\n",
      "Episode 88/200: Reward = 277.0994466763543\n",
      "Episode 89/200: Reward = 273.47048572094377\n",
      "Episode 90/200: Reward = 270.689936302579\n",
      "Episode 91/200: Reward = 279.820578370485\n",
      "Episode 92/200: Reward = 273.02902623393027\n",
      "Episode 93/200: Reward = 273.95720978506785\n",
      "Episode 94/200: Reward = 275.6727565409622\n",
      "Episode 95/200: Reward = 267.6742932553027\n",
      "Episode 96/200: Reward = 274.52362957334464\n",
      "Episode 97/200: Reward = 270.92154387498147\n",
      "Episode 98/200: Reward = 271.1192450510547\n",
      "Episode 99/200: Reward = 274.7798763094198\n",
      "Episode 100/200: Reward = 277.92829290844224\n",
      "Episode 101/200: Reward = 270.80457684944463\n",
      "Episode 102/200: Reward = 286.89080187366574\n",
      "Episode 103/200: Reward = 269.5864628469311\n",
      "Episode 104/200: Reward = 265.86286444566383\n",
      "Episode 105/200: Reward = 265.5078571398456\n",
      "Episode 106/200: Reward = 279.5019172912322\n",
      "Episode 107/200: Reward = 270.7863811310843\n",
      "Episode 108/200: Reward = 275.63358279178965\n",
      "Episode 109/200: Reward = 273.1489096317037\n",
      "Episode 110/200: Reward = 272.4125063465447\n",
      "Episode 111/200: Reward = 271.9527083612548\n",
      "Episode 112/200: Reward = 273.45978206856427\n",
      "Episode 113/200: Reward = 273.1569904390771\n",
      "Episode 114/200: Reward = 270.40373416034436\n",
      "Episode 115/200: Reward = 278.3884149788831\n",
      "Episode 116/200: Reward = 271.2632445273743\n",
      "Episode 117/200: Reward = 268.94309858936117\n",
      "Episode 118/200: Reward = 278.3878653017693\n",
      "Episode 119/200: Reward = 276.07782024260973\n",
      "Episode 120/200: Reward = 277.23607504932136\n",
      "Episode 121/200: Reward = 270.8058056132141\n",
      "Episode 122/200: Reward = 276.7289122624657\n",
      "Episode 123/200: Reward = 269.75806578769647\n",
      "Episode 124/200: Reward = 278.20808444309614\n",
      "Episode 125/200: Reward = 273.3318546715081\n",
      "Episode 126/200: Reward = 273.8464754129027\n",
      "Episode 127/200: Reward = 272.5595351698881\n",
      "Episode 128/200: Reward = 273.0211043810181\n",
      "Episode 129/200: Reward = 274.7618122445996\n",
      "Episode 130/200: Reward = 270.36947337218504\n",
      "Episode 131/200: Reward = 281.62995713607523\n",
      "Episode 132/200: Reward = 279.9703145076937\n",
      "Episode 133/200: Reward = 273.1382769560258\n",
      "Episode 134/200: Reward = 273.6582538270411\n",
      "Episode 135/200: Reward = 272.0244143804332\n",
      "Episode 136/200: Reward = 271.44051519786296\n",
      "Episode 137/200: Reward = 269.8324447581703\n",
      "Episode 138/200: Reward = 278.9555715577445\n",
      "Episode 139/200: Reward = 278.55512828692224\n",
      "Episode 140/200: Reward = 283.0738071321943\n",
      "Episode 141/200: Reward = 275.20824158188617\n",
      "Episode 142/200: Reward = 272.93960345714186\n",
      "Episode 143/200: Reward = 267.9019052355981\n",
      "Episode 144/200: Reward = 270.4306624782223\n",
      "Episode 145/200: Reward = 270.7461506000101\n",
      "Episode 146/200: Reward = 273.6661265936504\n",
      "Episode 147/200: Reward = 268.7223021812607\n",
      "Episode 148/200: Reward = 287.2327310960375\n",
      "Episode 149/200: Reward = 270.8494376756631\n",
      "Episode 150/200: Reward = 270.5881343202282\n",
      "Episode 151/200: Reward = 269.16902838975295\n",
      "Episode 152/200: Reward = 267.0156951429695\n",
      "Episode 153/200: Reward = 275.97862975841866\n",
      "Episode 154/200: Reward = 275.43473998090013\n",
      "Episode 155/200: Reward = 270.1393249796681\n",
      "Episode 156/200: Reward = 277.0159725529108\n",
      "Episode 157/200: Reward = 271.6949025733869\n",
      "Episode 158/200: Reward = 271.4630138100757\n",
      "Episode 159/200: Reward = 269.5293256716527\n",
      "Episode 160/200: Reward = 279.6462493010607\n",
      "Episode 161/200: Reward = 276.537909151386\n",
      "Episode 162/200: Reward = 280.33490304403534\n",
      "Episode 163/200: Reward = 273.19311208895\n",
      "Episode 164/200: Reward = 274.8072525697018\n",
      "Episode 165/200: Reward = 279.6246950721968\n",
      "Episode 166/200: Reward = 271.4576004994638\n",
      "Episode 167/200: Reward = 270.55503166949353\n",
      "Episode 168/200: Reward = 268.1593507121884\n",
      "Episode 169/200: Reward = 266.7085923740535\n",
      "Episode 170/200: Reward = 275.0869273661603\n",
      "Episode 171/200: Reward = 273.4180440291989\n",
      "Episode 172/200: Reward = 267.5099478058582\n",
      "Episode 173/200: Reward = 270.4035635184016\n",
      "Episode 174/200: Reward = 273.107036898551\n",
      "Episode 175/200: Reward = 269.9109350209483\n",
      "Episode 176/200: Reward = 276.527079702852\n",
      "Episode 177/200: Reward = 268.1429559774046\n",
      "Episode 178/200: Reward = 274.58142762244347\n",
      "Episode 179/200: Reward = 276.73103876386756\n",
      "Episode 180/200: Reward = 272.4538056785929\n",
      "Episode 181/200: Reward = 278.4283801682715\n",
      "Episode 182/200: Reward = 274.5341829716731\n",
      "Episode 183/200: Reward = 270.21363560337284\n",
      "Episode 184/200: Reward = 275.2509117885068\n",
      "Episode 185/200: Reward = 277.47853965856257\n",
      "Episode 186/200: Reward = 270.81441955931615\n",
      "Episode 187/200: Reward = 272.87691760725505\n",
      "Episode 188/200: Reward = 287.62841904519246\n",
      "Episode 189/200: Reward = 280.4804750412953\n",
      "Episode 190/200: Reward = 268.55745774469494\n",
      "Episode 191/200: Reward = 276.0103993899671\n",
      "Episode 192/200: Reward = 275.7796224476473\n",
      "Episode 193/200: Reward = 273.39124683504133\n",
      "Episode 194/200: Reward = 273.4210572864529\n",
      "Episode 195/200: Reward = 271.9332394312368\n",
      "Episode 196/200: Reward = 271.58266968482314\n",
      "Episode 197/200: Reward = 268.8389457942397\n",
      "Episode 198/200: Reward = 276.8963932335166\n",
      "Episode 199/200: Reward = 282.23764637932805\n",
      "Episode 200/200: Reward = 272.28103421260556\n",
      "Average Reward under State Value Attack: 273.71451891245914\n",
      "Average Reward under State Action Value Attack: 273.71451891245914\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    value_net=VanillaAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:11.252972Z",
     "iopub.status.busy": "2024-12-06T12:46:11.252193Z",
     "iopub.status.idle": "2024-12-06T12:46:11.264259Z",
     "shell.execute_reply": "2024-12-06T12:46:11.263325Z",
     "shell.execute_reply.started": "2024-12-06T12:46:11.252937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_target_policy_attack(env, policy_net, target_action, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Target Policy Misclassification attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        target_action (torch.Tensor): The target action to force the policy to output.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Target Policy Misclassification attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Get policy output for the perturbed state\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    logits = policy_output  # For discrete actions\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, target_action)  # Cross-entropy loss\n",
    "                elif isinstance(env.action_space, gym.spaces.Box):\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Mean and std\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "                # Backpropagate to compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action = torch.argmax(policy_output, dim=1).item()  # Discrete action\n",
    "                else:\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()  # Continuous action\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Target Policy Misclassification attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:48:45.421563Z",
     "iopub.status.busy": "2024-12-06T05:48:45.421302Z",
     "iopub.status.idle": "2024-12-06T05:54:12.394206Z",
     "shell.execute_reply": "2024-12-06T05:54:12.393297Z",
     "shell.execute_reply.started": "2024-12-06T05:48:45.421538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/1613303488.py:43: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 294.04799967616464\n",
      "Episode 2/200: Reward = 302.19924315959685\n",
      "Episode 3/200: Reward = 295.3919874894358\n",
      "Episode 4/200: Reward = 297.3427071901092\n",
      "Episode 5/200: Reward = 305.0504361685314\n",
      "Episode 6/200: Reward = 301.88588423142096\n",
      "Episode 7/200: Reward = 292.3706116292908\n",
      "Episode 8/200: Reward = 286.34070526822785\n",
      "Episode 9/200: Reward = 291.3673676993464\n",
      "Episode 10/200: Reward = 293.11689825145464\n",
      "Episode 11/200: Reward = 301.3684701442525\n",
      "Episode 12/200: Reward = 294.7610087724757\n",
      "Episode 13/200: Reward = 292.1902766792224\n",
      "Episode 14/200: Reward = 293.4538750586081\n",
      "Episode 15/200: Reward = 294.98247649596715\n",
      "Episode 16/200: Reward = 286.99936446585286\n",
      "Episode 17/200: Reward = 298.28207171112064\n",
      "Episode 18/200: Reward = 290.7818941724389\n",
      "Episode 19/200: Reward = 299.02443986407354\n",
      "Episode 20/200: Reward = 290.8173754706158\n",
      "Episode 21/200: Reward = 301.5903577852143\n",
      "Episode 22/200: Reward = 301.01717421047283\n",
      "Episode 23/200: Reward = 289.82356399374964\n",
      "Episode 24/200: Reward = 294.53803754964946\n",
      "Episode 25/200: Reward = 295.96985311024196\n",
      "Episode 26/200: Reward = 293.32179621794904\n",
      "Episode 27/200: Reward = 292.53820636252124\n",
      "Episode 28/200: Reward = 294.26976200821383\n",
      "Episode 29/200: Reward = 287.38429989429454\n",
      "Episode 30/200: Reward = 293.5060729708049\n",
      "Episode 31/200: Reward = 296.47319289623795\n",
      "Episode 32/200: Reward = 293.9577142406301\n",
      "Episode 33/200: Reward = 292.4715246982109\n",
      "Episode 34/200: Reward = 299.1865225029634\n",
      "Episode 35/200: Reward = 305.0693116673495\n",
      "Episode 36/200: Reward = 293.4241778110183\n",
      "Episode 37/200: Reward = 290.6733380481017\n",
      "Episode 38/200: Reward = 291.92388682288646\n",
      "Episode 39/200: Reward = 291.4492506358983\n",
      "Episode 40/200: Reward = 294.1801270655929\n",
      "Episode 41/200: Reward = 294.39761769035675\n",
      "Episode 42/200: Reward = 298.6418457588395\n",
      "Episode 43/200: Reward = 299.7768210750939\n",
      "Episode 44/200: Reward = 287.0332826594699\n",
      "Episode 45/200: Reward = 290.31586691863475\n",
      "Episode 46/200: Reward = 296.9072178591459\n",
      "Episode 47/200: Reward = 293.4710063089407\n",
      "Episode 48/200: Reward = 284.93470198576756\n",
      "Episode 49/200: Reward = 295.649580452872\n",
      "Episode 50/200: Reward = 296.46583664118134\n",
      "Episode 51/200: Reward = 290.05225599129335\n",
      "Episode 52/200: Reward = 291.76237965206354\n",
      "Episode 53/200: Reward = 299.2370342934752\n",
      "Episode 54/200: Reward = 288.2705005796311\n",
      "Episode 55/200: Reward = 294.992456768824\n",
      "Episode 56/200: Reward = 301.02625038002566\n",
      "Episode 57/200: Reward = 305.3873864579485\n",
      "Episode 58/200: Reward = 289.078795203058\n",
      "Episode 59/200: Reward = 294.949765010159\n",
      "Episode 60/200: Reward = 298.8845903544411\n",
      "Episode 61/200: Reward = 289.79184103804397\n",
      "Episode 62/200: Reward = 301.5832535503402\n",
      "Episode 63/200: Reward = 302.33121620833776\n",
      "Episode 64/200: Reward = 299.85223594657685\n",
      "Episode 65/200: Reward = 282.7618189894448\n",
      "Episode 66/200: Reward = 294.2024825241229\n",
      "Episode 67/200: Reward = 296.73023678734916\n",
      "Episode 68/200: Reward = 300.8809278435189\n",
      "Episode 69/200: Reward = 293.6332555694974\n",
      "Episode 70/200: Reward = 284.4447599943184\n",
      "Episode 71/200: Reward = 294.91532563029443\n",
      "Episode 72/200: Reward = 297.7727639231048\n",
      "Episode 73/200: Reward = 290.9465376833015\n",
      "Episode 74/200: Reward = 295.57769237863556\n",
      "Episode 75/200: Reward = 291.9907290159499\n",
      "Episode 76/200: Reward = 291.2001630279319\n",
      "Episode 77/200: Reward = 303.1852699337533\n",
      "Episode 78/200: Reward = 290.140443583923\n",
      "Episode 79/200: Reward = 296.5770187016284\n",
      "Episode 80/200: Reward = 293.1300610122533\n",
      "Episode 81/200: Reward = 284.01827138665095\n",
      "Episode 82/200: Reward = 288.2146219048164\n",
      "Episode 83/200: Reward = 291.0905428696692\n",
      "Episode 84/200: Reward = 301.79468734355885\n",
      "Episode 85/200: Reward = 303.72080998303613\n",
      "Episode 86/200: Reward = 294.91399593151874\n",
      "Episode 87/200: Reward = 300.8601171727836\n",
      "Episode 88/200: Reward = 301.1394472524899\n",
      "Episode 89/200: Reward = 300.7200432482009\n",
      "Episode 90/200: Reward = 293.1855409679602\n",
      "Episode 91/200: Reward = 296.91567717481496\n",
      "Episode 92/200: Reward = 306.0853422657604\n",
      "Episode 93/200: Reward = 288.972755714928\n",
      "Episode 94/200: Reward = 293.64265617330193\n",
      "Episode 95/200: Reward = 298.76035861029237\n",
      "Episode 96/200: Reward = 292.7679678473045\n",
      "Episode 97/200: Reward = 290.3899139179703\n",
      "Episode 98/200: Reward = 286.3060433509823\n",
      "Episode 99/200: Reward = 295.8058213710128\n",
      "Episode 100/200: Reward = 293.33046764264026\n",
      "Episode 101/200: Reward = 299.57388812643654\n",
      "Episode 102/200: Reward = 289.64764473012406\n",
      "Episode 103/200: Reward = 287.6709936578627\n",
      "Episode 104/200: Reward = 288.9012251195656\n",
      "Episode 105/200: Reward = 291.0000868251332\n",
      "Episode 106/200: Reward = 289.9894292770379\n",
      "Episode 107/200: Reward = 289.0222021594142\n",
      "Episode 108/200: Reward = 286.39119171253475\n",
      "Episode 109/200: Reward = 293.9002103498916\n",
      "Episode 110/200: Reward = 288.9768533966667\n",
      "Episode 111/200: Reward = 291.28214691626175\n",
      "Episode 112/200: Reward = 301.5629687235289\n",
      "Episode 113/200: Reward = 303.539506230909\n",
      "Episode 114/200: Reward = 299.5125618197229\n",
      "Episode 115/200: Reward = 287.9332053219594\n",
      "Episode 116/200: Reward = 297.816958200087\n",
      "Episode 117/200: Reward = 288.572104616501\n",
      "Episode 118/200: Reward = 292.089698880701\n",
      "Episode 119/200: Reward = 295.3476236060178\n",
      "Episode 120/200: Reward = 302.01646939567104\n",
      "Episode 121/200: Reward = 302.3503937436158\n",
      "Episode 122/200: Reward = 299.076145043997\n",
      "Episode 123/200: Reward = 293.9313071431578\n",
      "Episode 124/200: Reward = 301.6430784502922\n",
      "Episode 125/200: Reward = 288.1950817604045\n",
      "Episode 126/200: Reward = 297.48161548549234\n",
      "Episode 127/200: Reward = 291.18492474761905\n",
      "Episode 128/200: Reward = 298.84225067809246\n",
      "Episode 129/200: Reward = 291.2954505936243\n",
      "Episode 130/200: Reward = 293.07106093317617\n",
      "Episode 131/200: Reward = 289.287375678252\n",
      "Episode 132/200: Reward = 299.1189553787104\n",
      "Episode 133/200: Reward = 295.9211178051573\n",
      "Episode 134/200: Reward = 292.5299666716801\n",
      "Episode 135/200: Reward = 291.83419572886197\n",
      "Episode 136/200: Reward = 303.7775307415905\n",
      "Episode 137/200: Reward = 300.36690656921417\n",
      "Episode 138/200: Reward = 297.3398769364413\n",
      "Episode 139/200: Reward = 288.49535170185203\n",
      "Episode 140/200: Reward = 304.0135527349573\n",
      "Episode 141/200: Reward = 296.71084658940924\n",
      "Episode 142/200: Reward = 300.336762042534\n",
      "Episode 143/200: Reward = 293.14025148032385\n",
      "Episode 144/200: Reward = 292.09976058711317\n",
      "Episode 145/200: Reward = 301.0088943841256\n",
      "Episode 146/200: Reward = 307.84951043942436\n",
      "Episode 147/200: Reward = 287.6271384170219\n",
      "Episode 148/200: Reward = 290.7169886998275\n",
      "Episode 149/200: Reward = 288.96315965137364\n",
      "Episode 150/200: Reward = 288.19374227927494\n",
      "Episode 151/200: Reward = 285.7295075033918\n",
      "Episode 152/200: Reward = 303.0480515773202\n",
      "Episode 153/200: Reward = 289.76926091488195\n",
      "Episode 154/200: Reward = 290.2654039040763\n",
      "Episode 155/200: Reward = 295.61132420014457\n",
      "Episode 156/200: Reward = 305.25823430207083\n",
      "Episode 157/200: Reward = 288.4736164176456\n",
      "Episode 158/200: Reward = 292.9136897372697\n",
      "Episode 159/200: Reward = 293.7017238018465\n",
      "Episode 160/200: Reward = 297.6019683387094\n",
      "Episode 161/200: Reward = 293.5248826134078\n",
      "Episode 162/200: Reward = 290.3007193907593\n",
      "Episode 163/200: Reward = 298.71905426529815\n",
      "Episode 164/200: Reward = 293.0480627594142\n",
      "Episode 165/200: Reward = 291.4439447074252\n",
      "Episode 166/200: Reward = 292.3621821640869\n",
      "Episode 167/200: Reward = 296.06683514761994\n",
      "Episode 168/200: Reward = 291.09829224991154\n",
      "Episode 169/200: Reward = 291.62019109695086\n",
      "Episode 170/200: Reward = 289.5022514994313\n",
      "Episode 171/200: Reward = 291.22813876146415\n",
      "Episode 172/200: Reward = 300.6183061663604\n",
      "Episode 173/200: Reward = 287.8262745365528\n",
      "Episode 174/200: Reward = 291.7512318852565\n",
      "Episode 175/200: Reward = 299.26470283250603\n",
      "Episode 176/200: Reward = 298.69982133536774\n",
      "Episode 177/200: Reward = 292.0854527539517\n",
      "Episode 178/200: Reward = 293.69029595570544\n",
      "Episode 179/200: Reward = 300.543096712241\n",
      "Episode 180/200: Reward = 297.7120322301525\n",
      "Episode 181/200: Reward = 297.87972678109026\n",
      "Episode 182/200: Reward = 299.4244138309682\n",
      "Episode 183/200: Reward = 298.55707817693536\n",
      "Episode 184/200: Reward = 298.9348324016375\n",
      "Episode 185/200: Reward = 296.9961309732906\n",
      "Episode 186/200: Reward = 299.31120800323856\n",
      "Episode 187/200: Reward = 301.2486059057175\n",
      "Episode 188/200: Reward = 290.10665540975646\n",
      "Episode 189/200: Reward = 288.61256066596803\n",
      "Episode 190/200: Reward = 291.8113733985118\n",
      "Episode 191/200: Reward = 288.6593535024228\n",
      "Episode 192/200: Reward = 283.8599739900847\n",
      "Episode 193/200: Reward = 298.197608530288\n",
      "Episode 194/200: Reward = 288.4723011971366\n",
      "Episode 195/200: Reward = 292.17409039772247\n",
      "Episode 196/200: Reward = 287.8918725465862\n",
      "Episode 197/200: Reward = 288.62019013890034\n",
      "Episode 198/200: Reward = 286.4425652352623\n",
      "Episode 199/200: Reward = 289.2480909052409\n",
      "Episode 200/200: Reward = 288.98300898245486\n",
      "Average Reward under Target Policy Misclassification attack: 294.4506096225025\n",
      "Average Reward under Target Policy Misclassification Attack: 294.4506096225025\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:16.924799Z",
     "iopub.status.busy": "2024-12-06T12:46:16.923874Z",
     "iopub.status.idle": "2024-12-06T12:46:16.934473Z",
     "shell.execute_reply": "2024-12-06T12:46:16.933715Z",
     "shell.execute_reply.started": "2024-12-06T12:46:16.924750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:54:12.407725Z",
     "iopub.status.busy": "2024-12-06T05:54:12.407317Z",
     "iopub.status.idle": "2024-12-06T05:54:38.818528Z",
     "shell.execute_reply": "2024-12-06T05:54:38.817551Z",
     "shell.execute_reply.started": "2024-12-06T05:54:12.407680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 254.40977962990627\n",
      "Episode 2: Reward = 264.13689669908604\n",
      "Episode 3: Reward = 299.4865645926969\n",
      "Episode 4: Reward = 355.0286549288328\n",
      "Episode 5: Reward = 179.81828760811976\n",
      "Episode 6: Reward = 236.80741369409384\n",
      "Episode 7: Reward = 248.10659302611415\n",
      "Episode 8: Reward = 226.10548908501383\n",
      "Episode 9: Reward = 227.53613913390663\n",
      "Episode 10: Reward = 244.3685996700178\n",
      "Episode 11: Reward = 213.96220880049148\n",
      "Episode 12: Reward = 236.0869379636241\n",
      "Episode 13: Reward = 337.2372850092061\n",
      "Episode 14: Reward = 492.0983238620915\n",
      "Episode 15: Reward = 246.44593457797646\n",
      "Episode 16: Reward = 309.2160013779429\n",
      "Episode 17: Reward = 465.0841666927874\n",
      "Episode 18: Reward = 429.7256147411774\n",
      "Episode 19: Reward = 297.4300886102363\n",
      "Episode 20: Reward = 246.20035126983012\n",
      "Episode 21: Reward = 271.0104967796694\n",
      "Episode 22: Reward = 276.7822899219639\n",
      "Episode 23: Reward = 280.8108561592708\n",
      "Episode 24: Reward = 253.31546730127414\n",
      "Episode 25: Reward = 338.21880187422687\n",
      "Episode 26: Reward = 318.5499321382446\n",
      "Episode 27: Reward = 253.18694634896335\n",
      "Episode 28: Reward = 208.12487856462812\n",
      "Episode 29: Reward = 339.07078851087164\n",
      "Episode 30: Reward = 222.3503984779533\n",
      "Episode 31: Reward = 290.8627801718041\n",
      "Episode 32: Reward = 353.61445044914865\n",
      "Episode 33: Reward = 295.1330934200147\n",
      "Episode 34: Reward = 440.05366961280225\n",
      "Episode 35: Reward = 238.96412283851262\n",
      "Episode 36: Reward = 234.9337398353422\n",
      "Episode 37: Reward = 251.24215621811686\n",
      "Episode 38: Reward = 306.59705048550614\n",
      "Episode 39: Reward = 232.49812605890511\n",
      "Episode 40: Reward = 295.99684028539207\n",
      "Episode 41: Reward = 376.7253144485378\n",
      "Episode 42: Reward = 260.2575780082546\n",
      "Episode 43: Reward = 367.9092877762911\n",
      "Episode 44: Reward = 268.93588684445075\n",
      "Episode 45: Reward = 382.6001331838429\n",
      "Episode 46: Reward = 262.76168980267727\n",
      "Episode 47: Reward = 263.52417240174435\n",
      "Episode 48: Reward = 308.53267763103474\n",
      "Episode 49: Reward = 296.468202631842\n",
      "Episode 50: Reward = 240.54432108665318\n",
      "Episode 51: Reward = 119.82513729627483\n",
      "Episode 52: Reward = 321.478418183598\n",
      "Episode 53: Reward = 327.0570461462203\n",
      "Episode 54: Reward = 266.1503713693933\n",
      "Episode 55: Reward = 273.9685445401596\n",
      "Episode 56: Reward = 249.84252529099552\n",
      "Episode 57: Reward = 303.8925812593613\n",
      "Episode 58: Reward = 293.8166028252919\n",
      "Episode 59: Reward = 215.71812730341287\n",
      "Episode 60: Reward = 289.92410271830653\n",
      "Episode 61: Reward = 301.0762068256908\n",
      "Episode 62: Reward = 291.74007129263316\n",
      "Episode 63: Reward = 275.3254813144613\n",
      "Episode 64: Reward = 598.3136984156382\n",
      "Episode 65: Reward = 275.9552043473587\n",
      "Episode 66: Reward = 315.52234012085654\n",
      "Episode 67: Reward = 284.79309150677784\n",
      "Episode 68: Reward = 293.0914366626356\n",
      "Episode 69: Reward = 288.6857888077292\n",
      "Episode 70: Reward = 290.41514801314844\n",
      "Episode 71: Reward = 331.74421876676627\n",
      "Episode 72: Reward = 276.27080198837814\n",
      "Episode 73: Reward = 285.0765657221694\n",
      "Episode 74: Reward = 330.95830333441285\n",
      "Episode 75: Reward = 181.37774570147928\n",
      "Episode 76: Reward = 407.85936091763887\n",
      "Episode 77: Reward = 326.3379011330268\n",
      "Episode 78: Reward = 249.89842218387759\n",
      "Episode 79: Reward = 282.17632287023287\n",
      "Episode 80: Reward = 283.5588723175759\n",
      "Episode 81: Reward = 277.913007841272\n",
      "Episode 82: Reward = 291.46099913004446\n",
      "Episode 83: Reward = 232.54894021687744\n",
      "Episode 84: Reward = 243.77984146811616\n",
      "Episode 85: Reward = 313.1867791483364\n",
      "Episode 86: Reward = 264.4512464772428\n",
      "Episode 87: Reward = 282.1897273672769\n",
      "Episode 88: Reward = 374.06107032152346\n",
      "Episode 89: Reward = 233.55273327901375\n",
      "Episode 90: Reward = 294.0791195398738\n",
      "Episode 91: Reward = 364.9700863752429\n",
      "Episode 92: Reward = 322.46463795226185\n",
      "Episode 93: Reward = 280.38371784295384\n",
      "Episode 94: Reward = 275.2077060344763\n",
      "Episode 95: Reward = 258.47882226720816\n",
      "Episode 96: Reward = 239.15480537750096\n",
      "Episode 97: Reward = 274.7056868459147\n",
      "Episode 98: Reward = 301.79455551679183\n",
      "Episode 99: Reward = 355.67607021269595\n",
      "Episode 100: Reward = 397.38515800559423\n",
      "Episode 101: Reward = 258.82545297434336\n",
      "Episode 102: Reward = 266.2832325775631\n",
      "Episode 103: Reward = 335.73187663408237\n",
      "Episode 104: Reward = 284.85108559555925\n",
      "Episode 105: Reward = 236.9923090399109\n",
      "Episode 106: Reward = 220.80206967059394\n",
      "Episode 107: Reward = 333.3653456730975\n",
      "Episode 108: Reward = 263.27218649056005\n",
      "Episode 109: Reward = 284.093804245881\n",
      "Episode 110: Reward = 320.262263694139\n",
      "Episode 111: Reward = 275.65584823618684\n",
      "Episode 112: Reward = 253.48611992408533\n",
      "Episode 113: Reward = 268.18694492726615\n",
      "Episode 114: Reward = 268.0445101228901\n",
      "Episode 115: Reward = 207.40056285005033\n",
      "Episode 116: Reward = 265.81106400752844\n",
      "Episode 117: Reward = 206.35232055769313\n",
      "Episode 118: Reward = 284.7597246925654\n",
      "Episode 119: Reward = 276.50115301059566\n",
      "Episode 120: Reward = 278.4166733823472\n",
      "Episode 121: Reward = 313.4390940297197\n",
      "Episode 122: Reward = 253.73171913842216\n",
      "Episode 123: Reward = 494.20731787999273\n",
      "Episode 124: Reward = 225.65164934770849\n",
      "Episode 125: Reward = 269.17018627436204\n",
      "Episode 126: Reward = 245.15359699652709\n",
      "Episode 127: Reward = 244.3361698844075\n",
      "Episode 128: Reward = 357.04426626144374\n",
      "Episode 129: Reward = 245.9560266904974\n",
      "Episode 130: Reward = 253.7060022798035\n",
      "Episode 131: Reward = 303.1849917786927\n",
      "Episode 132: Reward = 336.0751834384154\n",
      "Episode 133: Reward = 341.1657350944688\n",
      "Episode 134: Reward = 377.83107806612674\n",
      "Episode 135: Reward = 345.63474513323376\n",
      "Episode 136: Reward = 360.1442708465664\n",
      "Episode 137: Reward = 91.16919152871618\n",
      "Episode 138: Reward = 315.60027788761886\n",
      "Episode 139: Reward = 266.9829606893316\n",
      "Episode 140: Reward = 387.8167197437556\n",
      "Episode 141: Reward = 292.9910613582351\n",
      "Episode 142: Reward = 273.3846011128254\n",
      "Episode 143: Reward = 260.82986645260854\n",
      "Episode 144: Reward = 273.6820928703789\n",
      "Episode 145: Reward = 273.2726517277456\n",
      "Episode 146: Reward = 279.5792799892179\n",
      "Episode 147: Reward = 312.14836482079835\n",
      "Episode 148: Reward = 201.42202801443207\n",
      "Episode 149: Reward = 325.7058317851739\n",
      "Episode 150: Reward = 273.3785836824582\n",
      "Episode 151: Reward = 282.129750163158\n",
      "Episode 152: Reward = 337.39599622228224\n",
      "Episode 153: Reward = 360.8013813743333\n",
      "Episode 154: Reward = 291.93331266062313\n",
      "Episode 155: Reward = 296.34805684347407\n",
      "Episode 156: Reward = 256.31167847950337\n",
      "Episode 157: Reward = 254.9164303442175\n",
      "Episode 158: Reward = 276.71799296514075\n",
      "Episode 159: Reward = 240.4334665820354\n",
      "Episode 160: Reward = 94.39277530534923\n",
      "Episode 161: Reward = 266.0525311003856\n",
      "Episode 162: Reward = 280.0645482951488\n",
      "Episode 163: Reward = 261.0645451402646\n",
      "Episode 164: Reward = 273.03174264327015\n",
      "Episode 165: Reward = 294.70774204179725\n",
      "Episode 166: Reward = 273.05584734235725\n",
      "Episode 167: Reward = 269.7981756460815\n",
      "Episode 168: Reward = 448.26877283074907\n",
      "Episode 169: Reward = 284.82163096176913\n",
      "Episode 170: Reward = 250.81723276495913\n",
      "Episode 171: Reward = 359.0205980964431\n",
      "Episode 172: Reward = 343.292758950342\n",
      "Episode 173: Reward = 357.108624232464\n",
      "Episode 174: Reward = 311.9034183278973\n",
      "Episode 175: Reward = 276.61910810730814\n",
      "Episode 176: Reward = 257.64645106048965\n",
      "Episode 177: Reward = 108.98373451649688\n",
      "Episode 178: Reward = 254.52882939222573\n",
      "Episode 179: Reward = 282.9191194302363\n",
      "Episode 180: Reward = 307.2039303112121\n",
      "Episode 181: Reward = 310.02669262153086\n",
      "Episode 182: Reward = 347.65294553877504\n",
      "Episode 183: Reward = 317.3001796800982\n",
      "Episode 184: Reward = 272.63068123854\n",
      "Episode 185: Reward = 255.76858531009486\n",
      "Episode 186: Reward = 302.8517208935244\n",
      "Episode 187: Reward = 205.29640876006528\n",
      "Episode 188: Reward = 306.7023387441459\n",
      "Episode 189: Reward = 367.18648614613323\n",
      "Episode 190: Reward = 169.13149798048764\n",
      "Episode 191: Reward = 330.9093367422656\n",
      "Episode 192: Reward = 304.2385530644319\n",
      "Episode 193: Reward = 295.2455114385617\n",
      "Episode 194: Reward = 251.15647129848256\n",
      "Episode 195: Reward = 301.90103630974767\n",
      "Episode 196: Reward = 256.1772815089914\n",
      "Episode 197: Reward = 333.3145728973483\n",
      "Episode 198: Reward = 522.1675231588298\n",
      "Episode 199: Reward = 306.5729427688541\n",
      "Episode 200: Reward = 291.12071800835395\n",
      "Average Reward over 200 Episodes: 289.6564719499436\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, VanillaAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:21.638824Z",
     "iopub.status.busy": "2024-12-06T12:46:21.637955Z",
     "iopub.status.idle": "2024-12-06T12:46:21.646800Z",
     "shell.execute_reply": "2024-12-06T12:46:21.645977Z",
     "shell.execute_reply.started": "2024-12-06T12:46:21.638787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:26:01.262622Z",
     "iopub.status.busy": "2024-12-06T06:26:01.262211Z",
     "iopub.status.idle": "2024-12-06T06:26:01.267101Z",
     "shell.execute_reply": "2024-12-06T06:26:01.266175Z",
     "shell.execute_reply.started": "2024-12-06T06:26:01.262587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:37.578281Z",
     "iopub.status.busy": "2024-12-06T12:46:37.577966Z",
     "iopub.status.idle": "2024-12-06T12:46:37.587555Z",
     "shell.execute_reply": "2024-12-06T12:46:37.586746Z",
     "shell.execute_reply.started": "2024-12-06T12:46:37.578255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:42.203975Z",
     "iopub.status.busy": "2024-12-06T12:46:42.203648Z",
     "iopub.status.idle": "2024-12-06T12:46:42.214077Z",
     "shell.execute_reply": "2024-12-06T12:46:42.213222Z",
     "shell.execute_reply.started": "2024-12-06T12:46:42.203945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T05:54:57.221651Z",
     "iopub.status.busy": "2024-12-06T05:54:57.221370Z",
     "iopub.status.idle": "2024-12-06T06:09:42.920515Z",
     "shell.execute_reply": "2024-12-06T06:09:42.919483Z",
     "shell.execute_reply.started": "2024-12-06T05:54:57.221625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 280.819949675743\n",
      "Episode 2/200: Reward = 277.97796180130507\n",
      "Episode 3/200: Reward = 227.64199456681746\n",
      "Episode 4/200: Reward = 318.30939854718036\n",
      "Episode 5/200: Reward = 235.62580962604096\n",
      "Episode 6/200: Reward = 265.9183547088665\n",
      "Episode 7/200: Reward = 269.74162427665283\n",
      "Episode 8/200: Reward = 233.25828530955715\n",
      "Episode 9/200: Reward = 292.9691218725707\n",
      "Episode 10/200: Reward = 272.23674098395117\n",
      "Episode 11/200: Reward = 288.72159161028\n",
      "Episode 12/200: Reward = 238.34586970047505\n",
      "Episode 13/200: Reward = 276.33567558040255\n",
      "Episode 14/200: Reward = 256.87207741480796\n",
      "Episode 15/200: Reward = 220.26945101761643\n",
      "Episode 16/200: Reward = 344.5049774545304\n",
      "Episode 17/200: Reward = 198.0653803210857\n",
      "Episode 18/200: Reward = 347.1588328691325\n",
      "Episode 19/200: Reward = 222.55184856514973\n",
      "Episode 20/200: Reward = 271.9213460186277\n",
      "Episode 21/200: Reward = 301.0110247558971\n",
      "Episode 22/200: Reward = 212.71376774473038\n",
      "Episode 23/200: Reward = 204.17656403671307\n",
      "Episode 24/200: Reward = 164.91169739428219\n",
      "Episode 25/200: Reward = 310.19989095927\n",
      "Episode 26/200: Reward = 363.3483317713964\n",
      "Episode 27/200: Reward = 322.71195491938437\n",
      "Episode 28/200: Reward = 512.7005462973586\n",
      "Episode 29/200: Reward = 262.42376169220387\n",
      "Episode 30/200: Reward = 230.33867826743275\n",
      "Episode 31/200: Reward = 369.2250008741026\n",
      "Episode 32/200: Reward = 258.4223028078196\n",
      "Episode 33/200: Reward = 298.8022788597811\n",
      "Episode 34/200: Reward = 299.7235535252663\n",
      "Episode 35/200: Reward = 279.2848496218533\n",
      "Episode 36/200: Reward = 316.3938842379826\n",
      "Episode 37/200: Reward = 272.40016446744875\n",
      "Episode 38/200: Reward = 614.4893365138224\n",
      "Episode 39/200: Reward = 268.09976558068126\n",
      "Episode 40/200: Reward = 315.78673821703575\n",
      "Episode 41/200: Reward = 360.73233438222934\n",
      "Episode 42/200: Reward = 280.4855857558561\n",
      "Episode 43/200: Reward = 287.7131389321018\n",
      "Episode 44/200: Reward = 222.7598500446215\n",
      "Episode 45/200: Reward = 232.96526058734293\n",
      "Episode 46/200: Reward = 439.0595101966395\n",
      "Episode 47/200: Reward = 360.5906263353114\n",
      "Episode 48/200: Reward = 290.8264197296132\n",
      "Episode 49/200: Reward = 287.490531724912\n",
      "Episode 50/200: Reward = 266.673194060814\n",
      "Episode 51/200: Reward = 284.6202458653274\n",
      "Episode 52/200: Reward = 276.2128039522721\n",
      "Episode 53/200: Reward = 239.3156758425965\n",
      "Episode 54/200: Reward = 265.75942909022694\n",
      "Episode 55/200: Reward = 203.05039582879613\n",
      "Episode 56/200: Reward = 276.59310369814017\n",
      "Episode 57/200: Reward = 321.7228004703964\n",
      "Episode 58/200: Reward = 263.44006878332016\n",
      "Episode 59/200: Reward = 303.42681549645476\n",
      "Episode 60/200: Reward = 256.62777480703517\n",
      "Episode 61/200: Reward = 347.00716767538495\n",
      "Episode 62/200: Reward = 299.4509388375395\n",
      "Episode 63/200: Reward = 322.5559649190021\n",
      "Episode 64/200: Reward = 258.56704859708634\n",
      "Episode 65/200: Reward = 321.22153858888595\n",
      "Episode 66/200: Reward = 239.87334248705085\n",
      "Episode 67/200: Reward = 306.6364283352738\n",
      "Episode 68/200: Reward = 256.5728345055975\n",
      "Episode 69/200: Reward = 357.5523365248582\n",
      "Episode 70/200: Reward = 166.4163609081818\n",
      "Episode 71/200: Reward = 401.495486588143\n",
      "Episode 72/200: Reward = 279.32029211627184\n",
      "Episode 73/200: Reward = 285.6846552884732\n",
      "Episode 74/200: Reward = 266.7323341532589\n",
      "Episode 75/200: Reward = 284.3247413186382\n",
      "Episode 76/200: Reward = 232.15370891394332\n",
      "Episode 77/200: Reward = 246.98855146906084\n",
      "Episode 78/200: Reward = 340.61340361763405\n",
      "Episode 79/200: Reward = 422.4591954702013\n",
      "Episode 80/200: Reward = 280.8228618856226\n",
      "Episode 81/200: Reward = 254.28972884906293\n",
      "Episode 82/200: Reward = 270.64656041772514\n",
      "Episode 83/200: Reward = 103.45479233012968\n",
      "Episode 84/200: Reward = 344.01715186147504\n",
      "Episode 85/200: Reward = 287.7176774590102\n",
      "Episode 86/200: Reward = 269.1748696514478\n",
      "Episode 87/200: Reward = 222.03425606954158\n",
      "Episode 88/200: Reward = 424.80254644265955\n",
      "Episode 89/200: Reward = 292.45122656971574\n",
      "Episode 90/200: Reward = 52.55896820559269\n",
      "Episode 91/200: Reward = 330.60984583892116\n",
      "Episode 92/200: Reward = 291.8820323777936\n",
      "Episode 93/200: Reward = 241.14539193151035\n",
      "Episode 94/200: Reward = 79.36052412494418\n",
      "Episode 95/200: Reward = 407.52574976427707\n",
      "Episode 96/200: Reward = 292.3811301002451\n",
      "Episode 97/200: Reward = 295.71766939804945\n",
      "Episode 98/200: Reward = 307.7495133632031\n",
      "Episode 99/200: Reward = 288.8033503137025\n",
      "Episode 100/200: Reward = 216.49262193986038\n",
      "Episode 101/200: Reward = 303.59829567560405\n",
      "Episode 102/200: Reward = 259.04567751063826\n",
      "Episode 103/200: Reward = 260.29963810787996\n",
      "Episode 104/200: Reward = 364.9917629922235\n",
      "Episode 105/200: Reward = 266.85828239404555\n",
      "Episode 106/200: Reward = 247.13937602615607\n",
      "Episode 107/200: Reward = 317.9795049560676\n",
      "Episode 108/200: Reward = 340.16196358184135\n",
      "Episode 109/200: Reward = 264.7418231961344\n",
      "Episode 110/200: Reward = 329.04707220378197\n",
      "Episode 111/200: Reward = 282.6972157603086\n",
      "Episode 112/200: Reward = 222.38094116992428\n",
      "Episode 113/200: Reward = 357.843814682873\n",
      "Episode 114/200: Reward = 257.40312914661826\n",
      "Episode 115/200: Reward = 243.39371201892854\n",
      "Episode 116/200: Reward = 279.6128259159309\n",
      "Episode 117/200: Reward = 229.85585596627666\n",
      "Episode 118/200: Reward = 268.2649948576302\n",
      "Episode 119/200: Reward = 252.24353980557945\n",
      "Episode 120/200: Reward = 342.82346455729396\n",
      "Episode 121/200: Reward = 303.99124093889327\n",
      "Episode 122/200: Reward = 174.4616031295538\n",
      "Episode 123/200: Reward = 52.97695422646424\n",
      "Episode 124/200: Reward = 315.4408649007442\n",
      "Episode 125/200: Reward = 304.36420155866585\n",
      "Episode 126/200: Reward = 306.0399876454933\n",
      "Episode 127/200: Reward = 307.7707942812701\n",
      "Episode 128/200: Reward = 340.3538641303016\n",
      "Episode 129/200: Reward = 308.4645094593541\n",
      "Episode 130/200: Reward = 388.553406796773\n",
      "Episode 131/200: Reward = 260.4140576987262\n",
      "Episode 132/200: Reward = 265.17884869759484\n",
      "Episode 133/200: Reward = 314.8873385593519\n",
      "Episode 134/200: Reward = 265.54609989313684\n",
      "Episode 135/200: Reward = 319.9994223096407\n",
      "Episode 136/200: Reward = 254.9485201361495\n",
      "Episode 137/200: Reward = 228.09301695387904\n",
      "Episode 138/200: Reward = 302.8859865613741\n",
      "Episode 139/200: Reward = 294.49770730491394\n",
      "Episode 140/200: Reward = 229.14328865081387\n",
      "Episode 141/200: Reward = 348.5387084478922\n",
      "Episode 142/200: Reward = 393.18415287095894\n",
      "Episode 143/200: Reward = 530.3373042582167\n",
      "Episode 144/200: Reward = 211.93016887048566\n",
      "Episode 145/200: Reward = 292.8117318346411\n",
      "Episode 146/200: Reward = 370.24665712791983\n",
      "Episode 147/200: Reward = 315.08112432209015\n",
      "Episode 148/200: Reward = 409.3159022995308\n",
      "Episode 149/200: Reward = 305.0536497541785\n",
      "Episode 150/200: Reward = 303.346347235668\n",
      "Episode 151/200: Reward = 271.1882202081597\n",
      "Episode 152/200: Reward = 153.52747318254148\n",
      "Episode 153/200: Reward = 273.41754481759017\n",
      "Episode 154/200: Reward = 26.884511782299914\n",
      "Episode 155/200: Reward = 213.86943088795076\n",
      "Episode 156/200: Reward = 353.44160780137435\n",
      "Episode 157/200: Reward = 261.03425847628296\n",
      "Episode 158/200: Reward = 302.40226848111797\n",
      "Episode 159/200: Reward = 264.80947041962895\n",
      "Episode 160/200: Reward = 303.2808275279017\n",
      "Episode 161/200: Reward = 189.3081012319354\n",
      "Episode 162/200: Reward = 247.15564150229002\n",
      "Episode 163/200: Reward = 211.76081740294345\n",
      "Episode 164/200: Reward = 286.71171218481516\n",
      "Episode 165/200: Reward = 352.54429407577055\n",
      "Episode 166/200: Reward = 252.38514962717286\n",
      "Episode 167/200: Reward = 240.97679795288096\n",
      "Episode 168/200: Reward = 219.2560370070938\n",
      "Episode 169/200: Reward = 283.62686955241685\n",
      "Episode 170/200: Reward = 478.1613602891522\n",
      "Episode 171/200: Reward = 253.92964097432215\n",
      "Episode 172/200: Reward = 200.02448468660202\n",
      "Episode 173/200: Reward = 380.2854724079932\n",
      "Episode 174/200: Reward = 286.69701563911406\n",
      "Episode 175/200: Reward = 252.19392309986873\n",
      "Episode 176/200: Reward = 346.3962526121247\n",
      "Episode 177/200: Reward = 292.7242135234784\n",
      "Episode 178/200: Reward = 348.3582959633533\n",
      "Episode 179/200: Reward = 247.54112545077436\n",
      "Episode 180/200: Reward = 430.64342466887683\n",
      "Episode 181/200: Reward = 21.56852650066085\n",
      "Episode 182/200: Reward = 274.8737326269005\n",
      "Episode 183/200: Reward = 293.0819629507225\n",
      "Episode 184/200: Reward = 325.3665237772931\n",
      "Episode 185/200: Reward = 285.5918371907003\n",
      "Episode 186/200: Reward = 261.0420371299352\n",
      "Episode 187/200: Reward = 286.93271277429295\n",
      "Episode 188/200: Reward = 297.62902530958246\n",
      "Episode 189/200: Reward = 194.25624380484197\n",
      "Episode 190/200: Reward = 335.63912569869467\n",
      "Episode 191/200: Reward = 257.09794405591725\n",
      "Episode 192/200: Reward = 365.4974761960754\n",
      "Episode 193/200: Reward = 243.13792656227938\n",
      "Episode 194/200: Reward = 371.944075350026\n",
      "Episode 195/200: Reward = 240.02317756991314\n",
      "Episode 196/200: Reward = 277.06976989901136\n",
      "Episode 197/200: Reward = 212.96324707638027\n",
      "Episode 198/200: Reward = 264.72837702939654\n",
      "Episode 199/200: Reward = 273.80988137547195\n",
      "Episode 200/200: Reward = 299.0989127766799\n",
      "Average Reward under MAD attack: 283.6993492986565\n",
      "Final Average Reward under MAD Attack: 283.6993492986565\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:47.988214Z",
     "iopub.status.busy": "2024-12-06T12:46:47.987910Z",
     "iopub.status.idle": "2024-12-06T12:46:48.004934Z",
     "shell.execute_reply": "2024-12-06T12:46:48.004061Z",
     "shell.execute_reply.started": "2024-12-06T12:46:47.988188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:52.845746Z",
     "iopub.status.busy": "2024-12-06T12:46:52.845000Z",
     "iopub.status.idle": "2024-12-06T12:46:52.857999Z",
     "shell.execute_reply": "2024-12-06T12:46:52.856992Z",
     "shell.execute_reply.started": "2024-12-06T12:46:52.845709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:09:42.967755Z",
     "iopub.status.busy": "2024-12-06T06:09:42.967426Z",
     "iopub.status.idle": "2024-12-06T06:14:30.117660Z",
     "shell.execute_reply": "2024-12-06T06:14:30.116885Z",
     "shell.execute_reply.started": "2024-12-06T06:09:42.967715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/4284193347.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000, TD Loss: 4.3310, Robust Loss: 0.0000\n",
      "Step 100/5000, TD Loss: 3.3553, Robust Loss: 0.1050\n",
      "Step 200/5000, TD Loss: 3.9885, Robust Loss: 0.3281\n",
      "Step 300/5000, TD Loss: 3.7331, Robust Loss: 0.6373\n",
      "Step 400/5000, TD Loss: 3.0604, Robust Loss: 0.7190\n",
      "Step 500/5000, TD Loss: 3.2349, Robust Loss: 1.2653\n",
      "Step 600/5000, TD Loss: 2.8970, Robust Loss: 1.5562\n",
      "Step 700/5000, TD Loss: 3.3306, Robust Loss: 2.3945\n",
      "Step 800/5000, TD Loss: 2.2871, Robust Loss: 2.0078\n",
      "Step 900/5000, TD Loss: 1.6863, Robust Loss: 2.6127\n",
      "Step 1000/5000, TD Loss: 2.1967, Robust Loss: 4.2389\n",
      "Step 1100/5000, TD Loss: 2.3882, Robust Loss: 2.3108\n",
      "Step 1200/5000, TD Loss: 1.8464, Robust Loss: 3.0430\n",
      "Step 1300/5000, TD Loss: 1.8825, Robust Loss: 3.6038\n",
      "Step 1400/5000, TD Loss: 1.3751, Robust Loss: 2.1003\n",
      "Step 1500/5000, TD Loss: 1.3362, Robust Loss: 3.2616\n",
      "Step 1600/5000, TD Loss: 1.3975, Robust Loss: 3.1535\n",
      "Step 1700/5000, TD Loss: 1.7389, Robust Loss: 2.4229\n",
      "Step 1800/5000, TD Loss: 1.4715, Robust Loss: 2.1467\n",
      "Step 1900/5000, TD Loss: 1.4649, Robust Loss: 2.8492\n",
      "Step 2000/5000, TD Loss: 1.1962, Robust Loss: 1.6831\n",
      "Step 2100/5000, TD Loss: 0.9877, Robust Loss: 1.8743\n",
      "Step 2200/5000, TD Loss: 1.5325, Robust Loss: 4.9001\n",
      "Step 2300/5000, TD Loss: 0.8902, Robust Loss: 3.2262\n",
      "Step 2400/5000, TD Loss: 0.6423, Robust Loss: 2.7832\n",
      "Step 2500/5000, TD Loss: 1.1555, Robust Loss: 2.4545\n",
      "Step 2600/5000, TD Loss: 1.1123, Robust Loss: 4.1821\n",
      "Step 2700/5000, TD Loss: 0.8942, Robust Loss: 1.8143\n",
      "Step 2800/5000, TD Loss: 1.1064, Robust Loss: 1.8575\n",
      "Step 2900/5000, TD Loss: 0.8935, Robust Loss: 2.8988\n",
      "Step 3000/5000, TD Loss: 0.6205, Robust Loss: 1.7268\n",
      "Step 3100/5000, TD Loss: 0.6005, Robust Loss: 1.6130\n",
      "Step 3200/5000, TD Loss: 0.5272, Robust Loss: 2.7182\n",
      "Step 3300/5000, TD Loss: 0.9209, Robust Loss: 2.7476\n",
      "Step 3400/5000, TD Loss: 0.8861, Robust Loss: 2.8175\n",
      "Step 3500/5000, TD Loss: 0.4580, Robust Loss: 2.1260\n",
      "Step 3600/5000, TD Loss: 0.5957, Robust Loss: 1.6207\n",
      "Step 3700/5000, TD Loss: 0.4730, Robust Loss: 1.4727\n",
      "Step 3800/5000, TD Loss: 0.5972, Robust Loss: 1.9687\n",
      "Step 3900/5000, TD Loss: 0.9819, Robust Loss: 4.5626\n",
      "Step 4000/5000, TD Loss: 0.8370, Robust Loss: 2.2052\n",
      "Step 4100/5000, TD Loss: 0.6688, Robust Loss: 2.0182\n",
      "Step 4200/5000, TD Loss: 0.5934, Robust Loss: 2.0204\n",
      "Step 4300/5000, TD Loss: 0.5318, Robust Loss: 2.5427\n",
      "Step 4400/5000, TD Loss: 0.6964, Robust Loss: 2.5332\n",
      "Step 4500/5000, TD Loss: 0.5879, Robust Loss: 1.5583\n",
      "Step 4600/5000, TD Loss: 2.1691, Robust Loss: 1.5008\n",
      "Step 4700/5000, TD Loss: 0.5774, Robust Loss: 2.7594\n",
      "Step 4800/5000, TD Loss: 0.7462, Robust Loss: 2.0984\n",
      "Step 4900/5000, TD Loss: 0.6447, Robust Loss: 1.6800\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:14:30.119569Z",
     "iopub.status.busy": "2024-12-06T06:14:30.119265Z",
     "iopub.status.idle": "2024-12-06T06:19:45.488198Z",
     "shell.execute_reply": "2024-12-06T06:19:45.487135Z",
     "shell.execute_reply.started": "2024-12-06T06:14:30.119541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 284.6235436201942\n",
      "Episode 2/200: Reward = 278.7098898097919\n",
      "Episode 3/200: Reward = 270.79058966267814\n",
      "Episode 4/200: Reward = 288.45116032586617\n",
      "Episode 5/200: Reward = 281.54298989280346\n",
      "Episode 6/200: Reward = 276.0320334474719\n",
      "Episode 7/200: Reward = 281.76446317156496\n",
      "Episode 8/200: Reward = 283.25777296391726\n",
      "Episode 9/200: Reward = 281.93904153187356\n",
      "Episode 10/200: Reward = 281.4979143733671\n",
      "Episode 11/200: Reward = 285.53657156020637\n",
      "Episode 12/200: Reward = 270.6550284999157\n",
      "Episode 13/200: Reward = 282.36337877680415\n",
      "Episode 14/200: Reward = 285.3086459278199\n",
      "Episode 15/200: Reward = 278.1046304589377\n",
      "Episode 16/200: Reward = 280.58099244112935\n",
      "Episode 17/200: Reward = 276.85128973894547\n",
      "Episode 18/200: Reward = 282.6953145284702\n",
      "Episode 19/200: Reward = 274.4318149693389\n",
      "Episode 20/200: Reward = 291.04140163392174\n",
      "Episode 21/200: Reward = 284.87258697260717\n",
      "Episode 22/200: Reward = 283.6506600767076\n",
      "Episode 23/200: Reward = 275.9215221010505\n",
      "Episode 24/200: Reward = 278.2934085506695\n",
      "Episode 25/200: Reward = 282.2417563973162\n",
      "Episode 26/200: Reward = 278.66586838362724\n",
      "Episode 27/200: Reward = 274.6068091116029\n",
      "Episode 28/200: Reward = 281.250339322551\n",
      "Episode 29/200: Reward = 288.6294043072404\n",
      "Episode 30/200: Reward = 279.03853761959095\n",
      "Episode 31/200: Reward = 286.17916430225733\n",
      "Episode 32/200: Reward = 283.3872780612582\n",
      "Episode 33/200: Reward = 284.4658965459346\n",
      "Episode 34/200: Reward = 284.5614876535537\n",
      "Episode 35/200: Reward = 288.19118955024186\n",
      "Episode 36/200: Reward = 282.02337647481573\n",
      "Episode 37/200: Reward = 283.62854783374786\n",
      "Episode 38/200: Reward = 282.8092347605356\n",
      "Episode 39/200: Reward = 283.99765935384335\n",
      "Episode 40/200: Reward = 281.61229414618737\n",
      "Episode 41/200: Reward = 281.97007975011064\n",
      "Episode 42/200: Reward = 276.63768986010774\n",
      "Episode 43/200: Reward = 281.96725734658\n",
      "Episode 44/200: Reward = 284.6722681972481\n",
      "Episode 45/200: Reward = 279.29080279203635\n",
      "Episode 46/200: Reward = 277.601562757413\n",
      "Episode 47/200: Reward = 291.98777586684866\n",
      "Episode 48/200: Reward = 276.6740192248715\n",
      "Episode 49/200: Reward = 281.225795071727\n",
      "Episode 50/200: Reward = 289.9435857469168\n",
      "Episode 51/200: Reward = 286.5963574147925\n",
      "Episode 52/200: Reward = 281.24870035782\n",
      "Episode 53/200: Reward = 292.33495264593455\n",
      "Episode 54/200: Reward = 277.7226745635992\n",
      "Episode 55/200: Reward = 282.5277060243644\n",
      "Episode 56/200: Reward = 278.03399635931913\n",
      "Episode 57/200: Reward = 289.34343779655796\n",
      "Episode 58/200: Reward = 284.82574743342826\n",
      "Episode 59/200: Reward = 280.4419956034142\n",
      "Episode 60/200: Reward = 280.8758538919679\n",
      "Episode 61/200: Reward = 280.60070751379374\n",
      "Episode 62/200: Reward = 277.8170434333617\n",
      "Episode 63/200: Reward = 277.6844299770947\n",
      "Episode 64/200: Reward = 274.32497660051246\n",
      "Episode 65/200: Reward = 279.47141386953257\n",
      "Episode 66/200: Reward = 281.03057057616826\n",
      "Episode 67/200: Reward = 283.28762261495126\n",
      "Episode 68/200: Reward = 275.85166789309085\n",
      "Episode 69/200: Reward = 276.193744472631\n",
      "Episode 70/200: Reward = 275.7528780564936\n",
      "Episode 71/200: Reward = 280.7444301666079\n",
      "Episode 72/200: Reward = 289.33042211404194\n",
      "Episode 73/200: Reward = 281.3583280286438\n",
      "Episode 74/200: Reward = 283.26675159070453\n",
      "Episode 75/200: Reward = 285.10952327421097\n",
      "Episode 76/200: Reward = 285.7486362494708\n",
      "Episode 77/200: Reward = 277.20107340635025\n",
      "Episode 78/200: Reward = 278.33621308920294\n",
      "Episode 79/200: Reward = 276.8881699235248\n",
      "Episode 80/200: Reward = 282.50241809161724\n",
      "Episode 81/200: Reward = 280.2162722629074\n",
      "Episode 82/200: Reward = 281.6090044912731\n",
      "Episode 83/200: Reward = 279.7595236717864\n",
      "Episode 84/200: Reward = 276.4654320942892\n",
      "Episode 85/200: Reward = 279.52872928512676\n",
      "Episode 86/200: Reward = 288.6926765601337\n",
      "Episode 87/200: Reward = 288.4682793792876\n",
      "Episode 88/200: Reward = 288.89602601108305\n",
      "Episode 89/200: Reward = 279.0287333936891\n",
      "Episode 90/200: Reward = 287.934879695959\n",
      "Episode 91/200: Reward = 285.02772832233643\n",
      "Episode 92/200: Reward = 278.70739168748787\n",
      "Episode 93/200: Reward = 277.72771231878545\n",
      "Episode 94/200: Reward = 277.1995349850238\n",
      "Episode 95/200: Reward = 279.20964014574486\n",
      "Episode 96/200: Reward = 288.3764484881424\n",
      "Episode 97/200: Reward = 283.3829855386982\n",
      "Episode 98/200: Reward = 278.5596134766548\n",
      "Episode 99/200: Reward = 291.1605492888328\n",
      "Episode 100/200: Reward = 285.8018689433115\n",
      "Episode 101/200: Reward = 281.56199889036293\n",
      "Episode 102/200: Reward = 284.7047716015636\n",
      "Episode 103/200: Reward = 280.09573719533375\n",
      "Episode 104/200: Reward = 281.4554861602041\n",
      "Episode 105/200: Reward = 289.0292073099411\n",
      "Episode 106/200: Reward = 276.4083198398082\n",
      "Episode 107/200: Reward = 279.10553877602365\n",
      "Episode 108/200: Reward = 273.9295682876751\n",
      "Episode 109/200: Reward = 286.81477593080416\n",
      "Episode 110/200: Reward = 280.50967896549434\n",
      "Episode 111/200: Reward = 276.255841535698\n",
      "Episode 112/200: Reward = 290.405780791166\n",
      "Episode 113/200: Reward = 285.88866822818164\n",
      "Episode 114/200: Reward = 282.90479833610783\n",
      "Episode 115/200: Reward = 284.8737830270147\n",
      "Episode 116/200: Reward = 276.40904735544996\n",
      "Episode 117/200: Reward = 274.40690762981285\n",
      "Episode 118/200: Reward = 290.82769955225865\n",
      "Episode 119/200: Reward = 276.66432797897613\n",
      "Episode 120/200: Reward = 283.59916360531895\n",
      "Episode 121/200: Reward = 281.9192214684104\n",
      "Episode 122/200: Reward = 273.727463958438\n",
      "Episode 123/200: Reward = 276.063130008509\n",
      "Episode 124/200: Reward = 283.60234832719226\n",
      "Episode 125/200: Reward = 279.0380202788297\n",
      "Episode 126/200: Reward = 287.86769340984114\n",
      "Episode 127/200: Reward = 278.4239149710368\n",
      "Episode 128/200: Reward = 281.38495636472896\n",
      "Episode 129/200: Reward = 280.5898723502594\n",
      "Episode 130/200: Reward = 284.76690647406735\n",
      "Episode 131/200: Reward = 280.19664450441206\n",
      "Episode 132/200: Reward = 282.5826252151277\n",
      "Episode 133/200: Reward = 289.8376606348348\n",
      "Episode 134/200: Reward = 277.26449627967213\n",
      "Episode 135/200: Reward = 286.42251431133474\n",
      "Episode 136/200: Reward = 278.533249116741\n",
      "Episode 137/200: Reward = 279.5725080307921\n",
      "Episode 138/200: Reward = 287.4950937296783\n",
      "Episode 139/200: Reward = 283.47654792053413\n",
      "Episode 140/200: Reward = 282.97821326447024\n",
      "Episode 141/200: Reward = 278.43978831669335\n",
      "Episode 142/200: Reward = 284.79056077359087\n",
      "Episode 143/200: Reward = 286.94078115173255\n",
      "Episode 144/200: Reward = 273.189786120778\n",
      "Episode 145/200: Reward = 280.8083110564352\n",
      "Episode 146/200: Reward = 282.41554807176755\n",
      "Episode 147/200: Reward = 282.3100052076522\n",
      "Episode 148/200: Reward = 283.682858956996\n",
      "Episode 149/200: Reward = 275.64940390788576\n",
      "Episode 150/200: Reward = 276.1211060149123\n",
      "Episode 151/200: Reward = 290.39653581188236\n",
      "Episode 152/200: Reward = 285.39703046542894\n",
      "Episode 153/200: Reward = 288.9856964775911\n",
      "Episode 154/200: Reward = 280.21675480654227\n",
      "Episode 155/200: Reward = 276.24831315173117\n",
      "Episode 156/200: Reward = 277.8924768759141\n",
      "Episode 157/200: Reward = 285.2841005738391\n",
      "Episode 158/200: Reward = 281.33252218579173\n",
      "Episode 159/200: Reward = 276.1219048467622\n",
      "Episode 160/200: Reward = 280.94723917017643\n",
      "Episode 161/200: Reward = 286.0203803336052\n",
      "Episode 162/200: Reward = 288.26663483370646\n",
      "Episode 163/200: Reward = 285.7479080629011\n",
      "Episode 164/200: Reward = 282.92762375941555\n",
      "Episode 165/200: Reward = 280.4194546047784\n",
      "Episode 166/200: Reward = 273.7954471915542\n",
      "Episode 167/200: Reward = 277.29551932000845\n",
      "Episode 168/200: Reward = 279.2231988739782\n",
      "Episode 169/200: Reward = 284.3464010379419\n",
      "Episode 170/200: Reward = 289.9866673989855\n",
      "Episode 171/200: Reward = 285.84440493074334\n",
      "Episode 172/200: Reward = 276.12031237658994\n",
      "Episode 173/200: Reward = 280.84133828105973\n",
      "Episode 174/200: Reward = 279.7374331038235\n",
      "Episode 175/200: Reward = 278.2655213212605\n",
      "Episode 176/200: Reward = 279.5254458773832\n",
      "Episode 177/200: Reward = 285.2379880542567\n",
      "Episode 178/200: Reward = 281.1584001783197\n",
      "Episode 179/200: Reward = 279.71138630735794\n",
      "Episode 180/200: Reward = 279.2351539326612\n",
      "Episode 181/200: Reward = 275.67609272002204\n",
      "Episode 182/200: Reward = 276.5908551888407\n",
      "Episode 183/200: Reward = 274.84569430309114\n",
      "Episode 184/200: Reward = 283.23247162370615\n",
      "Episode 185/200: Reward = 275.35266687827084\n",
      "Episode 186/200: Reward = 284.83916526485325\n",
      "Episode 187/200: Reward = 282.08455491886605\n",
      "Episode 188/200: Reward = 281.24760686253546\n",
      "Episode 189/200: Reward = 278.376618120449\n",
      "Episode 190/200: Reward = 279.08904248282306\n",
      "Episode 191/200: Reward = 280.47839099328957\n",
      "Episode 192/200: Reward = 275.88314621820984\n",
      "Episode 193/200: Reward = 272.46238513404194\n",
      "Episode 194/200: Reward = 285.7273818760432\n",
      "Episode 195/200: Reward = 281.613813090681\n",
      "Episode 196/200: Reward = 284.23699404304284\n",
      "Episode 197/200: Reward = 274.8981495471644\n",
      "Episode 198/200: Reward = 283.5817929617648\n",
      "Episode 199/200: Reward = 279.2140373300681\n",
      "Episode 200/200: Reward = 281.1335488183103\n",
      "Average Reward under Robust Sarsa Critic-based attack: 281.50708866132305\n",
      "Final Average Reward under Robust Sarsa Attack: 281.50708866132305\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
