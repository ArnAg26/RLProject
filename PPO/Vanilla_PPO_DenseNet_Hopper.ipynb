{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:29.039924Z",
     "iopub.status.busy": "2024-12-05T10:17:29.039557Z",
     "iopub.status.idle": "2024-12-05T10:17:30.058449Z",
     "shell.execute_reply": "2024-12-05T10:17:30.057554Z",
     "shell.execute_reply.started": "2024-12-05T10:17:29.039883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:29:45.050475Z",
     "iopub.status.busy": "2024-12-13T20:29:45.050221Z",
     "iopub.status.idle": "2024-12-13T20:29:57.104897Z",
     "shell.execute_reply": "2024-12-13T20:29:57.103830Z",
     "shell.execute_reply.started": "2024-12-13T20:29:45.050448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:31:34.808125Z",
     "iopub.status.busy": "2024-12-13T20:31:34.807807Z",
     "iopub.status.idle": "2024-12-13T20:31:38.627461Z",
     "shell.execute_reply": "2024-12-13T20:31:38.626800Z",
     "shell.execute_reply.started": "2024-12-13T20:31:34.808095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:33:27.752676Z",
     "iopub.status.busy": "2024-12-13T22:33:27.752279Z",
     "iopub.status.idle": "2024-12-13T22:33:27.774051Z",
     "shell.execute_reply": "2024-12-13T22:33:27.773110Z",
     "shell.execute_reply.started": "2024-12-13T22:33:27.752647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ValueDenseNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(ValueDenseNet, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        nn.init.orthogonal_(self.output_layer.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class CtsPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(CtsPolicy, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"tanh\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "        nn.init.orthogonal_(self.mean_layer.weight, gain=0.01)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, discrete=False, gamma=0.99, lam=0.95, eps_clip=0.2, lr=3e-4, k_epochs=4):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        # Initialize networks\n",
    "        self.value_net = ValueDenseNet(state_dim).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = CtsPolicy(state_dim, action_dim).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.policy_net(state)\n",
    "            action_dist = torch.distributions.Normal(mean, std)\n",
    "            action = action_dist.sample()\n",
    "            return action.cpu().numpy().squeeze(), action_dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_advantages(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            for _ in range(rollout_steps):\n",
    "                with torch.no_grad():\n",
    "                    value = self.value_net(state).squeeze(0)\n",
    "                    action, log_prob = self.select_action(state.cpu().numpy())\n",
    "\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            if len(states) == 0:\n",
    "                print(\"No valid states collected; skipping this episode.\")\n",
    "                continue\n",
    "\n",
    "            values.append(torch.tensor([0], device=self.device))\n",
    "            advantages = self.compute_advantages(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "\n",
    "            for _ in range(self.k_epochs):\n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i+batch_size]\n",
    "                    batch_actions = actions[i:i+batch_size]\n",
    "                    batch_log_probs = log_probs[i:i+batch_size]\n",
    "                    batch_advantages = advantages[i:i+batch_size]\n",
    "                    batch_returns = returns[i:i+batch_size]\n",
    "\n",
    "                    mean, std = self.policy_net(batch_states)\n",
    "                    dist = torch.distributions.Normal(mean, std)\n",
    "                    new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "\n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    (policy_loss + 0.5 * value_loss).backward()\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:33:32.795465Z",
     "iopub.status.busy": "2024-12-13T22:33:32.795140Z",
     "iopub.status.idle": "2024-12-13T22:39:06.080992Z",
     "shell.execute_reply": "2024-12-13T22:39:06.080138Z",
     "shell.execute_reply.started": "2024-12-13T22:33:32.795435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -3.221597671508789, Value Loss = 2.5624563694000244\n",
      "Episode 2: Policy Loss = -1.3393771648406982, Value Loss = 15.793832778930664\n",
      "Episode 3: Policy Loss = -3.514265537261963, Value Loss = 44.68801498413086\n",
      "Episode 4: Policy Loss = -10.162819862365723, Value Loss = 256.25390625\n",
      "Episode 5: Policy Loss = -5.219025135040283, Value Loss = 94.30572509765625\n",
      "Episode 6: Policy Loss = -14.415983200073242, Value Loss = 249.6623992919922\n",
      "Episode 7: Policy Loss = 0.3744044303894043, Value Loss = 292.5043640136719\n",
      "Episode 8: Policy Loss = -16.54226303100586, Value Loss = 453.032470703125\n",
      "Episode 9: Policy Loss = -1.7903311252593994, Value Loss = 643.0411376953125\n",
      "Episode 10: Policy Loss = 7.126384258270264, Value Loss = 442.3426513671875\n",
      "Episode 11: Policy Loss = -6.270569324493408, Value Loss = 350.77593994140625\n",
      "Episode 12: Policy Loss = 6.988250732421875, Value Loss = 640.8914794921875\n",
      "Episode 13: Policy Loss = 12.751995086669922, Value Loss = 729.26708984375\n",
      "Episode 14: Policy Loss = -2.757758140563965, Value Loss = 507.8702697753906\n",
      "Episode 15: Policy Loss = 5.189934730529785, Value Loss = 1110.989501953125\n",
      "Episode 16: Policy Loss = 10.90986442565918, Value Loss = 1229.398681640625\n",
      "Episode 17: Policy Loss = 8.821120262145996, Value Loss = 867.3637084960938\n",
      "Episode 18: Policy Loss = 8.935640335083008, Value Loss = 1341.0595703125\n",
      "Episode 19: Policy Loss = -0.7808942794799805, Value Loss = 700.225341796875\n",
      "Episode 20: Policy Loss = 21.669158935546875, Value Loss = 1579.678466796875\n",
      "Episode 21: Policy Loss = 10.475563049316406, Value Loss = 939.7132568359375\n",
      "Episode 22: Policy Loss = 0.29691123962402344, Value Loss = 791.9678955078125\n",
      "Episode 23: Policy Loss = 24.347562789916992, Value Loss = 1491.2584228515625\n",
      "Episode 24: Policy Loss = 21.52246856689453, Value Loss = 1713.694091796875\n",
      "Episode 25: Policy Loss = 24.925758361816406, Value Loss = 1840.1591796875\n",
      "Episode 26: Policy Loss = 28.79462432861328, Value Loss = 1728.93359375\n",
      "Episode 27: Policy Loss = 29.22569465637207, Value Loss = 1762.0408935546875\n",
      "Episode 28: Policy Loss = 30.36310577392578, Value Loss = 1815.695556640625\n",
      "Episode 29: Policy Loss = 18.184635162353516, Value Loss = 1436.99853515625\n",
      "Episode 30: Policy Loss = 19.407711029052734, Value Loss = 1556.6170654296875\n",
      "Episode 31: Policy Loss = 21.52381134033203, Value Loss = 1644.3526611328125\n",
      "Episode 32: Policy Loss = 9.641152381896973, Value Loss = 893.6201171875\n",
      "Episode 33: Policy Loss = 6.871947765350342, Value Loss = 981.137451171875\n",
      "Episode 34: Policy Loss = 24.675716400146484, Value Loss = 1955.36279296875\n",
      "Episode 35: Policy Loss = 32.689231872558594, Value Loss = 2005.132568359375\n",
      "Episode 36: Policy Loss = 11.281702995300293, Value Loss = 988.939697265625\n",
      "Episode 37: Policy Loss = 7.496140956878662, Value Loss = 909.501708984375\n",
      "Episode 38: Policy Loss = 6.3777055740356445, Value Loss = 924.225830078125\n",
      "Episode 39: Policy Loss = 6.673215389251709, Value Loss = 968.1339111328125\n",
      "Episode 40: Policy Loss = 33.186180114746094, Value Loss = 2041.1429443359375\n",
      "Episode 41: Policy Loss = 29.271759033203125, Value Loss = 2051.396484375\n",
      "Episode 42: Policy Loss = 32.63861083984375, Value Loss = 2090.360595703125\n",
      "Episode 43: Policy Loss = 6.442357540130615, Value Loss = 1064.7034912109375\n",
      "Episode 44: Policy Loss = -0.7643623352050781, Value Loss = 414.7279052734375\n",
      "Episode 45: Policy Loss = -5.255245685577393, Value Loss = 322.59429931640625\n",
      "Episode 46: Policy Loss = 2.199430465698242, Value Loss = 595.8795166015625\n",
      "Episode 47: Policy Loss = 3.3617255687713623, Value Loss = 558.6732788085938\n",
      "Episode 48: Policy Loss = 22.245994567871094, Value Loss = 1578.30810546875\n",
      "Episode 49: Policy Loss = 15.914892196655273, Value Loss = 1228.2249755859375\n",
      "Episode 50: Policy Loss = 31.37667465209961, Value Loss = 1882.2994384765625\n",
      "Episode 51: Policy Loss = 9.537596702575684, Value Loss = 1218.1654052734375\n",
      "Episode 52: Policy Loss = 21.025304794311523, Value Loss = 1410.95068359375\n",
      "Episode 53: Policy Loss = 6.730371475219727, Value Loss = 258.82965087890625\n",
      "Episode 54: Policy Loss = 36.48157501220703, Value Loss = 2277.15234375\n",
      "Episode 55: Policy Loss = 33.756195068359375, Value Loss = 3033.12353515625\n",
      "Episode 56: Policy Loss = 25.694942474365234, Value Loss = 1910.7933349609375\n",
      "Episode 57: Policy Loss = 13.08663558959961, Value Loss = 1294.0140380859375\n",
      "Episode 58: Policy Loss = 17.523616790771484, Value Loss = 2191.308837890625\n",
      "Episode 59: Policy Loss = 45.120460510253906, Value Loss = 3815.878662109375\n",
      "Episode 60: Policy Loss = 18.442386627197266, Value Loss = 735.2119140625\n",
      "Episode 61: Policy Loss = 28.85601806640625, Value Loss = 2438.141845703125\n",
      "Episode 62: Policy Loss = -3.5898637771606445, Value Loss = 129.6260528564453\n",
      "Episode 63: Policy Loss = 14.702264785766602, Value Loss = 984.013671875\n",
      "Episode 64: Policy Loss = 5.718487739562988, Value Loss = 753.5657348632812\n",
      "Episode 65: Policy Loss = 23.746978759765625, Value Loss = 2236.524658203125\n",
      "Episode 66: Policy Loss = 23.140331268310547, Value Loss = 1320.2357177734375\n",
      "Episode 67: Policy Loss = 11.567824363708496, Value Loss = 189.3300323486328\n",
      "Episode 68: Policy Loss = 25.993967056274414, Value Loss = 2208.76513671875\n",
      "Episode 69: Policy Loss = 22.143598556518555, Value Loss = 2722.46337890625\n",
      "Episode 70: Policy Loss = -9.653935432434082, Value Loss = 134.3565216064453\n",
      "Episode 71: Policy Loss = 30.6004695892334, Value Loss = 2523.90673828125\n",
      "Episode 72: Policy Loss = 21.659805297851562, Value Loss = 1886.3125\n",
      "Episode 73: Policy Loss = 39.40094757080078, Value Loss = 3204.742431640625\n",
      "Episode 74: Policy Loss = 25.16545867919922, Value Loss = 2650.9638671875\n",
      "Episode 75: Policy Loss = 12.76778793334961, Value Loss = 1737.656982421875\n",
      "Episode 76: Policy Loss = 21.9232177734375, Value Loss = 2642.82666015625\n",
      "Episode 77: Policy Loss = 22.966598510742188, Value Loss = 2445.63330078125\n",
      "Episode 78: Policy Loss = 37.949337005615234, Value Loss = 3545.794677734375\n",
      "Episode 79: Policy Loss = 34.31330108642578, Value Loss = 3186.690673828125\n",
      "Episode 80: Policy Loss = 38.18984603881836, Value Loss = 3574.650390625\n",
      "Episode 81: Policy Loss = 39.989967346191406, Value Loss = 4012.51708984375\n",
      "Episode 82: Policy Loss = 45.02095413208008, Value Loss = 4154.64013671875\n",
      "Episode 83: Policy Loss = 38.86311340332031, Value Loss = 3969.215576171875\n",
      "Episode 84: Policy Loss = 22.61033821105957, Value Loss = 1143.383056640625\n",
      "Episode 85: Policy Loss = 18.963993072509766, Value Loss = 568.9141845703125\n",
      "Episode 86: Policy Loss = 41.579978942871094, Value Loss = 4289.75634765625\n",
      "Episode 87: Policy Loss = 32.11476135253906, Value Loss = 3674.497314453125\n",
      "Episode 88: Policy Loss = 28.514049530029297, Value Loss = 1763.9202880859375\n",
      "Episode 89: Policy Loss = 14.269458770751953, Value Loss = 433.3222961425781\n",
      "Episode 90: Policy Loss = 46.290802001953125, Value Loss = 4339.1875\n",
      "Episode 91: Policy Loss = 15.945355415344238, Value Loss = 1529.9444580078125\n",
      "Episode 92: Policy Loss = 5.980808258056641, Value Loss = 522.181884765625\n",
      "Episode 93: Policy Loss = 44.28315734863281, Value Loss = 4829.0986328125\n",
      "Episode 94: Policy Loss = 18.61304473876953, Value Loss = 2357.1396484375\n",
      "Episode 95: Policy Loss = 28.609012603759766, Value Loss = 2554.269287109375\n",
      "Episode 96: Policy Loss = 35.919715881347656, Value Loss = 2604.345703125\n",
      "Episode 97: Policy Loss = 46.15869903564453, Value Loss = 4732.68115234375\n",
      "Episode 98: Policy Loss = 41.27790832519531, Value Loss = 3440.6943359375\n",
      "Episode 99: Policy Loss = 37.02561569213867, Value Loss = 4167.84765625\n",
      "Episode 100: Policy Loss = 31.76502799987793, Value Loss = 2699.60693359375\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    VanillaAgent = PPOAgent(state_dim, action_dim, discrete)\n",
    "    VanillaAgent.train(env, max_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:02.758553Z",
     "iopub.status.busy": "2024-12-13T20:32:02.757781Z",
     "iopub.status.idle": "2024-12-13T20:32:02.767686Z",
     "shell.execute_reply": "2024-12-13T20:32:02.766074Z",
     "shell.execute_reply.started": "2024-12-13T20:32:02.758522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:59:28.150987Z",
     "iopub.status.busy": "2024-12-13T09:59:28.150681Z",
     "iopub.status.idle": "2024-12-13T09:59:53.824325Z",
     "shell.execute_reply": "2024-12-13T09:59:53.823526Z",
     "shell.execute_reply.started": "2024-12-13T09:59:28.150962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 652.4459488506427\n",
      "Episode 2: Reward = 614.1448293515381\n",
      "Episode 3: Reward = 618.8522705967769\n",
      "Episode 4: Reward = 497.6870875703817\n",
      "Episode 5: Reward = 503.4381155461898\n",
      "Episode 6: Reward = 528.4523871505037\n",
      "Episode 7: Reward = 514.3586293978371\n",
      "Episode 8: Reward = 479.29329991601674\n",
      "Episode 9: Reward = 663.3597141500392\n",
      "Episode 10: Reward = 366.9392410524427\n",
      "Episode 11: Reward = 648.2870680529188\n",
      "Episode 12: Reward = 467.85167534552886\n",
      "Episode 13: Reward = 539.6809275309286\n",
      "Episode 14: Reward = 638.4396768014072\n",
      "Episode 15: Reward = 514.6691291934592\n",
      "Episode 16: Reward = 399.6046965200381\n",
      "Episode 17: Reward = 694.8830025179152\n",
      "Episode 18: Reward = 607.8410732441221\n",
      "Episode 19: Reward = 623.4726910840731\n",
      "Episode 20: Reward = 727.1431331529661\n",
      "Episode 21: Reward = 496.8581921074278\n",
      "Episode 22: Reward = 552.4797958454277\n",
      "Episode 23: Reward = 468.66632572329735\n",
      "Episode 24: Reward = 355.0780346627685\n",
      "Episode 25: Reward = 612.3787668102184\n",
      "Episode 26: Reward = 248.06644442091797\n",
      "Episode 27: Reward = 375.0424938456358\n",
      "Episode 28: Reward = 575.7716958739063\n",
      "Episode 29: Reward = 621.620806562799\n",
      "Episode 30: Reward = 546.259104471181\n",
      "Episode 31: Reward = 459.7464745590066\n",
      "Episode 32: Reward = 378.7784253513172\n",
      "Episode 33: Reward = 439.6895593809668\n",
      "Episode 34: Reward = 609.2886071658405\n",
      "Episode 35: Reward = 254.3398311651075\n",
      "Episode 36: Reward = 627.6281500014327\n",
      "Episode 37: Reward = 584.1770662517974\n",
      "Episode 38: Reward = 569.5898182142864\n",
      "Episode 39: Reward = 490.69689966048185\n",
      "Episode 40: Reward = 602.9362381414008\n",
      "Episode 41: Reward = 446.6189997280344\n",
      "Episode 42: Reward = 552.6638670593577\n",
      "Episode 43: Reward = 671.7455858217031\n",
      "Episode 44: Reward = 419.4988965018068\n",
      "Episode 45: Reward = 583.18884055281\n",
      "Episode 46: Reward = 488.64235589709773\n",
      "Episode 47: Reward = 608.8704808649013\n",
      "Episode 48: Reward = 499.2783709841269\n",
      "Episode 49: Reward = 533.5536823781672\n",
      "Episode 50: Reward = 599.4056916919711\n",
      "Episode 51: Reward = 417.645103727515\n",
      "Episode 52: Reward = 632.0265683024882\n",
      "Episode 53: Reward = 389.0194939912257\n",
      "Episode 54: Reward = 783.7209403803149\n",
      "Episode 55: Reward = 572.1050402014873\n",
      "Episode 56: Reward = 441.5325066260471\n",
      "Episode 57: Reward = 815.727466958996\n",
      "Episode 58: Reward = 484.0679642896098\n",
      "Episode 59: Reward = 510.312377627633\n",
      "Episode 60: Reward = 382.3028594728261\n",
      "Episode 61: Reward = 476.01417986300436\n",
      "Episode 62: Reward = 481.9046125046403\n",
      "Episode 63: Reward = 441.38240737383865\n",
      "Episode 64: Reward = 446.62306409034977\n",
      "Episode 65: Reward = 371.52839611339954\n",
      "Episode 66: Reward = 490.13810006270523\n",
      "Episode 67: Reward = 559.7181805015044\n",
      "Episode 68: Reward = 1039.1046548118613\n",
      "Episode 69: Reward = 650.8203882161692\n",
      "Episode 70: Reward = 550.6585861590605\n",
      "Episode 71: Reward = 607.840228731543\n",
      "Episode 72: Reward = 585.2469422439775\n",
      "Episode 73: Reward = 693.2609050038837\n",
      "Episode 74: Reward = 532.1471578653425\n",
      "Episode 75: Reward = 540.4885453629469\n",
      "Episode 76: Reward = 520.7354322126268\n",
      "Episode 77: Reward = 534.1421416639834\n",
      "Episode 78: Reward = 595.7768110219884\n",
      "Episode 79: Reward = 573.3581538249297\n",
      "Episode 80: Reward = 589.3397109300157\n",
      "Episode 81: Reward = 819.0441502011748\n",
      "Episode 82: Reward = 502.0214408314815\n",
      "Episode 83: Reward = 500.01323568073207\n",
      "Episode 84: Reward = 437.8482180932091\n",
      "Episode 85: Reward = 556.5356424341716\n",
      "Episode 86: Reward = 690.0882062539761\n",
      "Episode 87: Reward = 615.5471095032839\n",
      "Episode 88: Reward = 571.4594036163511\n",
      "Episode 89: Reward = 346.0511936425022\n",
      "Episode 90: Reward = 588.821276069195\n",
      "Episode 91: Reward = 511.03090710119466\n",
      "Episode 92: Reward = 471.5535587554886\n",
      "Episode 93: Reward = 541.3760862907371\n",
      "Episode 94: Reward = 610.0968883629242\n",
      "Episode 95: Reward = 581.9619061067983\n",
      "Episode 96: Reward = 459.81679853388323\n",
      "Episode 97: Reward = 543.4784863716475\n",
      "Episode 98: Reward = 462.7689625998887\n",
      "Episode 99: Reward = 514.3687254288948\n",
      "Episode 100: Reward = 573.118360333385\n",
      "Episode 101: Reward = 406.0714655553442\n",
      "Episode 102: Reward = 526.9504727664317\n",
      "Episode 103: Reward = 525.3346429626534\n",
      "Episode 104: Reward = 545.8092526546828\n",
      "Episode 105: Reward = 1280.0969478508832\n",
      "Episode 106: Reward = 406.96533384096557\n",
      "Episode 107: Reward = 532.1196009172776\n",
      "Episode 108: Reward = 496.28566202566395\n",
      "Episode 109: Reward = 569.0319290084519\n",
      "Episode 110: Reward = 514.529388065548\n",
      "Episode 111: Reward = 596.1076645929109\n",
      "Episode 112: Reward = 640.3726257966517\n",
      "Episode 113: Reward = 559.1266349443077\n",
      "Episode 114: Reward = 548.3799416224148\n",
      "Episode 115: Reward = 385.94372556983296\n",
      "Episode 116: Reward = 496.61327341879775\n",
      "Episode 117: Reward = 432.6276543379779\n",
      "Episode 118: Reward = 412.38867866987533\n",
      "Episode 119: Reward = 600.3474610585998\n",
      "Episode 120: Reward = 686.1570846764977\n",
      "Episode 121: Reward = 507.22845980051903\n",
      "Episode 122: Reward = 474.29399949283976\n",
      "Episode 123: Reward = 468.4016289758638\n",
      "Episode 124: Reward = 491.9824813652241\n",
      "Episode 125: Reward = 468.50661867993034\n",
      "Episode 126: Reward = 536.0660807169495\n",
      "Episode 127: Reward = 835.7457965262844\n",
      "Episode 128: Reward = 459.89134562867713\n",
      "Episode 129: Reward = 619.7197122792044\n",
      "Episode 130: Reward = 720.4182409059921\n",
      "Episode 131: Reward = 369.1894902502238\n",
      "Episode 132: Reward = 527.4676242821453\n",
      "Episode 133: Reward = 908.1676249878478\n",
      "Episode 134: Reward = 362.156105743883\n",
      "Episode 135: Reward = 532.9263735138874\n",
      "Episode 136: Reward = 606.9151360423685\n",
      "Episode 137: Reward = 688.5263693333627\n",
      "Episode 138: Reward = 443.32542694697764\n",
      "Episode 139: Reward = 796.5309132374348\n",
      "Episode 140: Reward = 805.6287110213326\n",
      "Episode 141: Reward = 289.3452681546068\n",
      "Episode 142: Reward = 608.5186871525516\n",
      "Episode 143: Reward = 480.6193263311104\n",
      "Episode 144: Reward = 505.71962037218213\n",
      "Episode 145: Reward = 712.9371364196855\n",
      "Episode 146: Reward = 513.7990239301305\n",
      "Episode 147: Reward = 469.9594954403177\n",
      "Episode 148: Reward = 390.11879099816537\n",
      "Episode 149: Reward = 486.21574088332153\n",
      "Episode 150: Reward = 465.09432387610946\n",
      "Episode 151: Reward = 587.3863138070448\n",
      "Episode 152: Reward = 577.8762653804063\n",
      "Episode 153: Reward = 678.6288482554602\n",
      "Episode 154: Reward = 647.7896849358293\n",
      "Episode 155: Reward = 517.3218408698848\n",
      "Episode 156: Reward = 435.4555326337792\n",
      "Episode 157: Reward = 213.6781375043289\n",
      "Episode 158: Reward = 582.7227338172065\n",
      "Episode 159: Reward = 736.0706677308204\n",
      "Episode 160: Reward = 388.61171563960806\n",
      "Episode 161: Reward = 444.3343302151743\n",
      "Episode 162: Reward = 231.38308674885786\n",
      "Episode 163: Reward = 419.9755494208692\n",
      "Episode 164: Reward = 411.5693701579848\n",
      "Episode 165: Reward = 394.12004596532427\n",
      "Episode 166: Reward = 537.4393959481843\n",
      "Episode 167: Reward = 554.5475023547427\n",
      "Episode 168: Reward = 779.0997241515396\n",
      "Episode 169: Reward = 498.20136765435126\n",
      "Episode 170: Reward = 544.2623742914204\n",
      "Episode 171: Reward = 478.9255223521735\n",
      "Episode 172: Reward = 589.1507399929865\n",
      "Episode 173: Reward = 539.7858003696267\n",
      "Episode 174: Reward = 537.4037388309907\n",
      "Episode 175: Reward = 339.0877818805043\n",
      "Episode 176: Reward = 391.1770448350572\n",
      "Episode 177: Reward = 504.5649171197911\n",
      "Episode 178: Reward = 644.6249174050254\n",
      "Episode 179: Reward = 622.4828086923189\n",
      "Episode 180: Reward = 493.9025643594469\n",
      "Episode 181: Reward = 904.1956832383203\n",
      "Episode 182: Reward = 583.7194605857771\n",
      "Episode 183: Reward = 653.2387069858025\n",
      "Episode 184: Reward = 597.8862589548568\n",
      "Episode 185: Reward = 601.8296785704882\n",
      "Episode 186: Reward = 588.1145877305162\n",
      "Episode 187: Reward = 676.0664095945222\n",
      "Episode 188: Reward = 569.3773813544288\n",
      "Episode 189: Reward = 510.4265137602717\n",
      "Episode 190: Reward = 432.99356348290667\n",
      "Episode 191: Reward = 458.7555719151999\n",
      "Episode 192: Reward = 242.82080102843904\n",
      "Episode 193: Reward = 394.2940240649702\n",
      "Episode 194: Reward = 390.78220166999245\n",
      "Episode 195: Reward = 440.26161852588183\n",
      "Episode 196: Reward = 517.9052852083396\n",
      "Episode 197: Reward = 519.526768970504\n",
      "Episode 198: Reward = 617.4203072563597\n",
      "Episode 199: Reward = 805.1051547032849\n",
      "Episode 200: Reward = 575.5950993010157\n",
      "Average Reward over 200 Episodes: 541.4383198445764\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, VanillaAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:12.712285Z",
     "iopub.status.busy": "2024-12-13T20:32:12.711500Z",
     "iopub.status.idle": "2024-12-13T20:32:12.720295Z",
     "shell.execute_reply": "2024-12-13T20:32:12.719499Z",
     "shell.execute_reply.started": "2024-12-13T20:32:12.712252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:00:16.822773Z",
     "iopub.status.busy": "2024-12-13T10:00:16.822060Z",
     "iopub.status.idle": "2024-12-13T10:00:16.826520Z",
     "shell.execute_reply": "2024-12-13T10:00:16.825633Z",
     "shell.execute_reply.started": "2024-12-13T10:00:16.822738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:23.267019Z",
     "iopub.status.busy": "2024-12-13T20:32:23.266492Z",
     "iopub.status.idle": "2024-12-13T20:32:23.275941Z",
     "shell.execute_reply": "2024-12-13T20:32:23.274980Z",
     "shell.execute_reply.started": "2024-12-13T20:32:23.266982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:30.917581Z",
     "iopub.status.busy": "2024-12-13T20:32:30.916832Z",
     "iopub.status.idle": "2024-12-13T20:32:30.927764Z",
     "shell.execute_reply": "2024-12-13T20:32:30.926729Z",
     "shell.execute_reply.started": "2024-12-13T20:32:30.917549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:02:10.160840Z",
     "iopub.status.busy": "2024-12-13T10:02:10.160159Z",
     "iopub.status.idle": "2024-12-13T10:16:39.654103Z",
     "shell.execute_reply": "2024-12-13T10:16:39.653192Z",
     "shell.execute_reply.started": "2024-12-13T10:02:10.160806Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 565.8911017242077\n",
      "Episode 2/200: Reward = 203.0939214698481\n",
      "Episode 3/200: Reward = 560.9117905908154\n",
      "Episode 4/200: Reward = 336.68652499511256\n",
      "Episode 5/200: Reward = 464.91528429878673\n",
      "Episode 6/200: Reward = 357.3243534537256\n",
      "Episode 7/200: Reward = 517.971137756409\n",
      "Episode 8/200: Reward = 393.93948255403467\n",
      "Episode 9/200: Reward = 594.6585801138092\n",
      "Episode 10/200: Reward = 486.21103213414386\n",
      "Episode 11/200: Reward = 529.4676886784894\n",
      "Episode 12/200: Reward = 592.0173987298522\n",
      "Episode 13/200: Reward = 573.754178204873\n",
      "Episode 14/200: Reward = 372.339100769359\n",
      "Episode 15/200: Reward = 484.3235076948758\n",
      "Episode 16/200: Reward = 257.6114740936725\n",
      "Episode 17/200: Reward = 557.844127738401\n",
      "Episode 18/200: Reward = 473.15564369383156\n",
      "Episode 19/200: Reward = 547.8489128107001\n",
      "Episode 20/200: Reward = 568.7821236909003\n",
      "Episode 21/200: Reward = 864.7967880302562\n",
      "Episode 22/200: Reward = 604.710646202576\n",
      "Episode 23/200: Reward = 515.5672332289886\n",
      "Episode 24/200: Reward = 630.2956360406477\n",
      "Episode 25/200: Reward = 498.3691641612913\n",
      "Episode 26/200: Reward = 619.9884470119745\n",
      "Episode 27/200: Reward = 566.0082033255773\n",
      "Episode 28/200: Reward = 361.91764584363693\n",
      "Episode 29/200: Reward = 562.8451003684389\n",
      "Episode 30/200: Reward = 672.7742958489736\n",
      "Episode 31/200: Reward = 844.227526456384\n",
      "Episode 32/200: Reward = 420.5360296177657\n",
      "Episode 33/200: Reward = 529.701526178728\n",
      "Episode 34/200: Reward = 522.11030923478\n",
      "Episode 35/200: Reward = 492.20644687023986\n",
      "Episode 36/200: Reward = 591.373485058083\n",
      "Episode 37/200: Reward = 624.7892547772376\n",
      "Episode 38/200: Reward = 645.6961696797433\n",
      "Episode 39/200: Reward = 398.0093797872092\n",
      "Episode 40/200: Reward = 397.3247334890689\n",
      "Episode 41/200: Reward = 626.3858565748201\n",
      "Episode 42/200: Reward = 612.5984593218235\n",
      "Episode 43/200: Reward = 544.5692020333236\n",
      "Episode 44/200: Reward = 575.2480589052084\n",
      "Episode 45/200: Reward = 667.3447305693422\n",
      "Episode 46/200: Reward = 597.8211741667071\n",
      "Episode 47/200: Reward = 512.0410304757318\n",
      "Episode 48/200: Reward = 615.9374072871336\n",
      "Episode 49/200: Reward = 569.6073559754287\n",
      "Episode 50/200: Reward = 637.3061669679571\n",
      "Episode 51/200: Reward = 594.8093200259252\n",
      "Episode 52/200: Reward = 507.41584090010355\n",
      "Episode 53/200: Reward = 514.3311717972485\n",
      "Episode 54/200: Reward = 503.3403330359613\n",
      "Episode 55/200: Reward = 246.26067695526243\n",
      "Episode 56/200: Reward = 358.15679392515744\n",
      "Episode 57/200: Reward = 591.135576921827\n",
      "Episode 58/200: Reward = 370.3855773100063\n",
      "Episode 59/200: Reward = 622.708131606732\n",
      "Episode 60/200: Reward = 453.9425077532812\n",
      "Episode 61/200: Reward = 282.9837973998624\n",
      "Episode 62/200: Reward = 389.5121419729669\n",
      "Episode 63/200: Reward = 598.3545969516264\n",
      "Episode 64/200: Reward = 654.1850666775608\n",
      "Episode 65/200: Reward = 471.44270363749513\n",
      "Episode 66/200: Reward = 647.457851126049\n",
      "Episode 67/200: Reward = 323.3299907319619\n",
      "Episode 68/200: Reward = 509.9232665046501\n",
      "Episode 69/200: Reward = 571.3393057090215\n",
      "Episode 70/200: Reward = 404.13170593508903\n",
      "Episode 71/200: Reward = 541.5853794907647\n",
      "Episode 72/200: Reward = 584.5356311172773\n",
      "Episode 73/200: Reward = 634.1382389225117\n",
      "Episode 74/200: Reward = 512.4269942334793\n",
      "Episode 75/200: Reward = 353.2245835673846\n",
      "Episode 76/200: Reward = 579.334209608392\n",
      "Episode 77/200: Reward = 756.5277847508775\n",
      "Episode 78/200: Reward = 399.90027799216864\n",
      "Episode 79/200: Reward = 304.0337926136516\n",
      "Episode 80/200: Reward = 559.5371257655756\n",
      "Episode 81/200: Reward = 358.00602838313034\n",
      "Episode 82/200: Reward = 433.79865711126985\n",
      "Episode 83/200: Reward = 405.317709682154\n",
      "Episode 84/200: Reward = 379.0637270353624\n",
      "Episode 85/200: Reward = 393.50195304544525\n",
      "Episode 86/200: Reward = 980.5157313024807\n",
      "Episode 87/200: Reward = 503.02225510584896\n",
      "Episode 88/200: Reward = 481.9844705242695\n",
      "Episode 89/200: Reward = 525.4958180382625\n",
      "Episode 90/200: Reward = 472.4025263204692\n",
      "Episode 91/200: Reward = 676.8343073465168\n",
      "Episode 92/200: Reward = 757.7555848901645\n",
      "Episode 93/200: Reward = 618.3463570400563\n",
      "Episode 94/200: Reward = 674.6104570582319\n",
      "Episode 95/200: Reward = 602.5181362832652\n",
      "Episode 96/200: Reward = 582.5914359467573\n",
      "Episode 97/200: Reward = 556.6194871620711\n",
      "Episode 98/200: Reward = 260.39835532549387\n",
      "Episode 99/200: Reward = 512.8008969933925\n",
      "Episode 100/200: Reward = 826.0751471799805\n",
      "Episode 101/200: Reward = 692.0089352851707\n",
      "Episode 102/200: Reward = 620.5762709617644\n",
      "Episode 103/200: Reward = 395.08025272581943\n",
      "Episode 104/200: Reward = 611.8569154320421\n",
      "Episode 105/200: Reward = 517.6966629186511\n",
      "Episode 106/200: Reward = 564.8252013476123\n",
      "Episode 107/200: Reward = 517.8959638519865\n",
      "Episode 108/200: Reward = 405.97920551276223\n",
      "Episode 109/200: Reward = 630.9035401243864\n",
      "Episode 110/200: Reward = 679.3018169445185\n",
      "Episode 111/200: Reward = 578.2408603515017\n",
      "Episode 112/200: Reward = 536.7857344254815\n",
      "Episode 113/200: Reward = 305.55323713401265\n",
      "Episode 114/200: Reward = 708.4702327064277\n",
      "Episode 115/200: Reward = 606.1265862331308\n",
      "Episode 116/200: Reward = 718.7559383377214\n",
      "Episode 117/200: Reward = 544.4955964012894\n",
      "Episode 118/200: Reward = 499.70569836270295\n",
      "Episode 119/200: Reward = 444.1379244542159\n",
      "Episode 120/200: Reward = 634.9984466223533\n",
      "Episode 121/200: Reward = 724.4017790946125\n",
      "Episode 122/200: Reward = 549.112960183789\n",
      "Episode 123/200: Reward = 544.2982169331915\n",
      "Episode 124/200: Reward = 551.4148818467555\n",
      "Episode 125/200: Reward = 667.8192048989781\n",
      "Episode 126/200: Reward = 580.3908755113613\n",
      "Episode 127/200: Reward = 619.9222788201091\n",
      "Episode 128/200: Reward = 214.16793002766323\n",
      "Episode 129/200: Reward = 596.6162829288336\n",
      "Episode 130/200: Reward = 606.4183208708251\n",
      "Episode 131/200: Reward = 524.9503051769128\n",
      "Episode 132/200: Reward = 491.56389842078846\n",
      "Episode 133/200: Reward = 585.2616499762804\n",
      "Episode 134/200: Reward = 550.0576971540817\n",
      "Episode 135/200: Reward = 212.02687935359356\n",
      "Episode 136/200: Reward = 522.1095260665933\n",
      "Episode 137/200: Reward = 184.47960691495683\n",
      "Episode 138/200: Reward = 355.8438820330994\n",
      "Episode 139/200: Reward = 368.8647716348059\n",
      "Episode 140/200: Reward = 649.7043673891815\n",
      "Episode 141/200: Reward = 517.26781475184\n",
      "Episode 142/200: Reward = 611.9576498829719\n",
      "Episode 143/200: Reward = 395.64687404738214\n",
      "Episode 144/200: Reward = 641.5966783260561\n",
      "Episode 145/200: Reward = 228.77879885463722\n",
      "Episode 146/200: Reward = 413.95750366563243\n",
      "Episode 147/200: Reward = 389.1397979890151\n",
      "Episode 148/200: Reward = 379.9352277366283\n",
      "Episode 149/200: Reward = 624.2483089488268\n",
      "Episode 150/200: Reward = 467.555543360405\n",
      "Episode 151/200: Reward = 600.6420630483331\n",
      "Episode 152/200: Reward = 599.6094311164998\n",
      "Episode 153/200: Reward = 619.0851053210906\n",
      "Episode 154/200: Reward = 388.95917173114077\n",
      "Episode 155/200: Reward = 230.41360014305292\n",
      "Episode 156/200: Reward = 654.4921438213058\n",
      "Episode 157/200: Reward = 566.7752314030829\n",
      "Episode 158/200: Reward = 201.41865026328574\n",
      "Episode 159/200: Reward = 376.31722164637404\n",
      "Episode 160/200: Reward = 485.35900969186133\n",
      "Episode 161/200: Reward = 382.64456473817876\n",
      "Episode 162/200: Reward = 247.9803754200265\n",
      "Episode 163/200: Reward = 614.3470785926551\n",
      "Episode 164/200: Reward = 584.5151045963929\n",
      "Episode 165/200: Reward = 697.7964188431758\n",
      "Episode 166/200: Reward = 499.7513353625996\n",
      "Episode 167/200: Reward = 463.2762034746154\n",
      "Episode 168/200: Reward = 840.3605171963554\n",
      "Episode 169/200: Reward = 620.2475710919217\n",
      "Episode 170/200: Reward = 588.9977443824699\n",
      "Episode 171/200: Reward = 455.4033262369561\n",
      "Episode 172/200: Reward = 495.1318490564468\n",
      "Episode 173/200: Reward = 596.2135601768213\n",
      "Episode 174/200: Reward = 463.68844597265763\n",
      "Episode 175/200: Reward = 484.0092126553599\n",
      "Episode 176/200: Reward = 425.85463537285307\n",
      "Episode 177/200: Reward = 563.8444434163025\n",
      "Episode 178/200: Reward = 582.2012626412758\n",
      "Episode 179/200: Reward = 598.5427671573501\n",
      "Episode 180/200: Reward = 602.1787882776789\n",
      "Episode 181/200: Reward = 574.3789859384706\n",
      "Episode 182/200: Reward = 403.13426893716286\n",
      "Episode 183/200: Reward = 499.8844298501216\n",
      "Episode 184/200: Reward = 253.30030541130228\n",
      "Episode 185/200: Reward = 765.7758225723173\n",
      "Episode 186/200: Reward = 416.7626591721754\n",
      "Episode 187/200: Reward = 475.6747937391106\n",
      "Episode 188/200: Reward = 465.5281378656092\n",
      "Episode 189/200: Reward = 603.5894942075246\n",
      "Episode 190/200: Reward = 594.6135052215343\n",
      "Episode 191/200: Reward = 611.4518849259323\n",
      "Episode 192/200: Reward = 565.834666273034\n",
      "Episode 193/200: Reward = 300.9946929049197\n",
      "Episode 194/200: Reward = 386.6147865542884\n",
      "Episode 195/200: Reward = 503.8023495619597\n",
      "Episode 196/200: Reward = 572.0686504579586\n",
      "Episode 197/200: Reward = 517.2653423479543\n",
      "Episode 198/200: Reward = 654.5317474187634\n",
      "Episode 199/200: Reward = 262.8096412437877\n",
      "Episode 200/200: Reward = 377.7182798123171\n",
      "Average Reward under MAD attack: 519.6080002779859\n",
      "Final Average Reward under MAD Attack: 519.6080002779859\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:37.433291Z",
     "iopub.status.busy": "2024-12-13T20:32:37.432506Z",
     "iopub.status.idle": "2024-12-13T20:32:37.449963Z",
     "shell.execute_reply": "2024-12-13T20:32:37.449087Z",
     "shell.execute_reply.started": "2024-12-13T20:32:37.433258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:44.214611Z",
     "iopub.status.busy": "2024-12-13T20:32:44.213911Z",
     "iopub.status.idle": "2024-12-13T20:32:44.226927Z",
     "shell.execute_reply": "2024-12-13T20:32:44.225957Z",
     "shell.execute_reply.started": "2024-12-13T20:32:44.214578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:16:53.381964Z",
     "iopub.status.busy": "2024-12-13T10:16:53.381625Z",
     "iopub.status.idle": "2024-12-13T10:21:06.657379Z",
     "shell.execute_reply": "2024-12-13T10:21:06.656644Z",
     "shell.execute_reply.started": "2024-12-13T10:16:53.381934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000, TD Loss: 6.1303, Robust Loss: 0.0000\n",
      "Step 100/5000, TD Loss: 6.0068, Robust Loss: 0.0505\n",
      "Step 200/5000, TD Loss: 5.3464, Robust Loss: 0.2554\n",
      "Step 300/5000, TD Loss: 5.2837, Robust Loss: 0.6106\n",
      "Step 400/5000, TD Loss: 4.1731, Robust Loss: 0.9373\n",
      "Step 500/5000, TD Loss: 3.7205, Robust Loss: 1.2716\n",
      "Step 600/5000, TD Loss: 3.3885, Robust Loss: 1.6301\n",
      "Step 700/5000, TD Loss: 2.9450, Robust Loss: 2.4541\n",
      "Step 800/5000, TD Loss: 3.1512, Robust Loss: 2.6789\n",
      "Step 900/5000, TD Loss: 2.5853, Robust Loss: 2.4085\n",
      "Step 1000/5000, TD Loss: 2.4569, Robust Loss: 3.4228\n",
      "Step 1100/5000, TD Loss: 2.0217, Robust Loss: 2.6433\n",
      "Step 1200/5000, TD Loss: 1.7027, Robust Loss: 3.9412\n",
      "Step 1300/5000, TD Loss: 1.7142, Robust Loss: 3.8536\n",
      "Step 1400/5000, TD Loss: 1.3394, Robust Loss: 3.1270\n",
      "Step 1500/5000, TD Loss: 1.4653, Robust Loss: 6.9941\n",
      "Step 1600/5000, TD Loss: 1.1773, Robust Loss: 3.7761\n",
      "Step 1700/5000, TD Loss: 1.2107, Robust Loss: 3.6375\n",
      "Step 1800/5000, TD Loss: 0.9130, Robust Loss: 5.1886\n",
      "Step 1900/5000, TD Loss: 0.9851, Robust Loss: 5.3264\n",
      "Step 2000/5000, TD Loss: 0.9557, Robust Loss: 2.6898\n",
      "Step 2100/5000, TD Loss: 0.6840, Robust Loss: 5.1500\n",
      "Step 2200/5000, TD Loss: 0.6997, Robust Loss: 3.9265\n",
      "Step 2300/5000, TD Loss: 0.5557, Robust Loss: 4.0791\n",
      "Step 2400/5000, TD Loss: 0.7204, Robust Loss: 3.6873\n",
      "Step 2500/5000, TD Loss: 0.6535, Robust Loss: 3.4217\n",
      "Step 2600/5000, TD Loss: 0.4886, Robust Loss: 4.6345\n",
      "Step 2700/5000, TD Loss: 0.3385, Robust Loss: 4.8633\n",
      "Step 2800/5000, TD Loss: 0.3487, Robust Loss: 3.8741\n",
      "Step 2900/5000, TD Loss: 0.4235, Robust Loss: 4.7498\n",
      "Step 3000/5000, TD Loss: 0.3988, Robust Loss: 2.2804\n",
      "Step 3100/5000, TD Loss: 0.3103, Robust Loss: 4.0893\n",
      "Step 3200/5000, TD Loss: 0.3120, Robust Loss: 3.3530\n",
      "Step 3300/5000, TD Loss: 0.3111, Robust Loss: 3.5964\n",
      "Step 3400/5000, TD Loss: 0.2420, Robust Loss: 2.0003\n",
      "Step 3500/5000, TD Loss: 0.1919, Robust Loss: 3.8847\n",
      "Step 3600/5000, TD Loss: 0.1981, Robust Loss: 2.5201\n",
      "Step 3700/5000, TD Loss: 0.2558, Robust Loss: 1.6639\n",
      "Step 3800/5000, TD Loss: 0.2423, Robust Loss: 3.4063\n",
      "Step 3900/5000, TD Loss: 0.2854, Robust Loss: 1.8608\n",
      "Step 4000/5000, TD Loss: 0.1911, Robust Loss: 2.6318\n",
      "Step 4100/5000, TD Loss: 0.1665, Robust Loss: 2.5015\n",
      "Step 4200/5000, TD Loss: 0.2350, Robust Loss: 2.4493\n",
      "Step 4300/5000, TD Loss: 0.1976, Robust Loss: 3.1407\n",
      "Step 4400/5000, TD Loss: 0.1584, Robust Loss: 2.5208\n",
      "Step 4500/5000, TD Loss: 0.1520, Robust Loss: 1.7478\n",
      "Step 4600/5000, TD Loss: 0.2583, Robust Loss: 1.6007\n",
      "Step 4700/5000, TD Loss: 0.2511, Robust Loss: 2.2550\n",
      "Step 4800/5000, TD Loss: 0.1376, Robust Loss: 1.8453\n",
      "Step 4900/5000, TD Loss: 0.1887, Robust Loss: 1.9886\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:21:15.422662Z",
     "iopub.status.busy": "2024-12-13T10:21:15.422344Z",
     "iopub.status.idle": "2024-12-13T11:11:17.487013Z",
     "shell.execute_reply": "2024-12-13T11:11:17.486122Z",
     "shell.execute_reply.started": "2024-12-13T10:21:15.422637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 3268.293101765089\n",
      "Episode 2/200: Reward = 5838.918512543528\n",
      "Episode 3/200: Reward = 2681.847133577918\n",
      "Episode 4/200: Reward = 1283.921933200141\n",
      "Episode 5/200: Reward = 2437.822984142992\n",
      "Episode 6/200: Reward = 1260.6117289126946\n",
      "Episode 7/200: Reward = 1700.6048950162947\n",
      "Episode 8/200: Reward = 4488.603671185932\n",
      "Episode 9/200: Reward = 2470.11661810934\n",
      "Episode 10/200: Reward = 1688.1917542642843\n",
      "Episode 11/200: Reward = 1042.6257590856567\n",
      "Episode 12/200: Reward = 2079.4519080643618\n",
      "Episode 13/200: Reward = 9783.766255112618\n",
      "Episode 14/200: Reward = 3245.469740953984\n",
      "Episode 15/200: Reward = 1656.512138379635\n",
      "Episode 16/200: Reward = 5855.553541727958\n",
      "Episode 17/200: Reward = 5670.223882305832\n",
      "Episode 18/200: Reward = 13291.53984783727\n",
      "Episode 19/200: Reward = 1235.792551095557\n",
      "Episode 20/200: Reward = 3686.6908974833177\n",
      "Episode 21/200: Reward = 10914.951993228275\n",
      "Episode 22/200: Reward = 1259.479015863773\n",
      "Episode 23/200: Reward = 2290.580576544661\n",
      "Episode 24/200: Reward = 3273.9260356521763\n",
      "Episode 25/200: Reward = 1836.514849813772\n",
      "Episode 26/200: Reward = 4099.823676096284\n",
      "Episode 27/200: Reward = 4793.489349853783\n",
      "Episode 28/200: Reward = 4003.745088427862\n",
      "Episode 29/200: Reward = 1248.491619354819\n",
      "Episode 30/200: Reward = 2063.731230607662\n",
      "Episode 31/200: Reward = 4070.953959807182\n",
      "Episode 32/200: Reward = 2895.4282455478256\n",
      "Episode 33/200: Reward = 3903.98201161549\n",
      "Episode 34/200: Reward = 1277.3775436255694\n",
      "Episode 35/200: Reward = 1832.5362292820676\n",
      "Episode 36/200: Reward = 9040.333343255063\n",
      "Episode 37/200: Reward = 2017.8634591287469\n",
      "Episode 38/200: Reward = 1255.3922382009887\n",
      "Episode 39/200: Reward = 13506.84978071683\n",
      "Episode 40/200: Reward = 1639.1694477120197\n",
      "Episode 41/200: Reward = 2442.0557873136977\n",
      "Episode 42/200: Reward = 1811.0580814732414\n",
      "Episode 43/200: Reward = 1032.99007302989\n",
      "Episode 44/200: Reward = 3026.73955710841\n",
      "Episode 45/200: Reward = 2269.960754474786\n",
      "Episode 46/200: Reward = 1675.99570919454\n",
      "Episode 47/200: Reward = 9074.81631173385\n",
      "Episode 48/200: Reward = 1269.2734914997936\n",
      "Episode 49/200: Reward = 2310.806749188903\n",
      "Episode 50/200: Reward = 6248.610138655341\n",
      "Episode 51/200: Reward = 3395.9574306147547\n",
      "Episode 52/200: Reward = 2655.893975694518\n",
      "Episode 53/200: Reward = 4343.987950340612\n",
      "Episode 54/200: Reward = 2688.283282764208\n",
      "Episode 55/200: Reward = 1025.6245579437877\n",
      "Episode 56/200: Reward = 1278.22256713293\n",
      "Episode 57/200: Reward = 7954.801348993997\n",
      "Episode 58/200: Reward = 1270.4164297043278\n",
      "Episode 59/200: Reward = 2313.488108781181\n",
      "Episode 60/200: Reward = 3677.9180079371768\n",
      "Episode 61/200: Reward = 1022.9754336951652\n",
      "Episode 62/200: Reward = 1271.9085765046946\n",
      "Episode 63/200: Reward = 4731.539978219784\n",
      "Episode 64/200: Reward = 3917.1876788953964\n",
      "Episode 65/200: Reward = 3701.849537229882\n",
      "Episode 66/200: Reward = 6699.301499598173\n",
      "Episode 67/200: Reward = 3280.295103998373\n",
      "Episode 68/200: Reward = 3082.999160901056\n",
      "Episode 69/200: Reward = 1466.3689884352516\n",
      "Episode 70/200: Reward = 2048.5810181029597\n",
      "Episode 71/200: Reward = 1869.4695105805981\n",
      "Episode 72/200: Reward = 1293.0992493115657\n",
      "Episode 73/200: Reward = 1267.212387498107\n",
      "Episode 74/200: Reward = 2873.412776287699\n",
      "Episode 75/200: Reward = 2818.1284220145244\n",
      "Episode 76/200: Reward = 4629.18806816358\n",
      "Episode 77/200: Reward = 1428.4374317199026\n",
      "Episode 78/200: Reward = 4471.072673151603\n",
      "Episode 79/200: Reward = 1272.7384185777516\n",
      "Episode 80/200: Reward = 1232.8518347366964\n",
      "Episode 81/200: Reward = 2488.489297981891\n",
      "Episode 82/200: Reward = 1683.949660862133\n",
      "Episode 83/200: Reward = 1842.9941790140103\n",
      "Episode 84/200: Reward = 2278.2926245493527\n",
      "Episode 85/200: Reward = 2459.610986369333\n",
      "Episode 86/200: Reward = 7790.98717423898\n",
      "Episode 87/200: Reward = 1664.0233171203915\n",
      "Episode 88/200: Reward = 2396.6204232938258\n",
      "Episode 89/200: Reward = 1258.6680197232106\n",
      "Episode 90/200: Reward = 2723.8248906152025\n",
      "Episode 91/200: Reward = 6676.494927766849\n",
      "Episode 92/200: Reward = 2767.553417643362\n",
      "Episode 93/200: Reward = 2605.0300919608717\n",
      "Episode 94/200: Reward = 1259.999483361956\n",
      "Episode 95/200: Reward = 2636.541460883137\n",
      "Episode 96/200: Reward = 4827.08704654769\n",
      "Episode 97/200: Reward = 4888.965011151181\n",
      "Episode 98/200: Reward = 4056.339077099296\n",
      "Episode 99/200: Reward = 1027.2829397558032\n",
      "Episode 100/200: Reward = 2440.3521477890104\n",
      "Episode 101/200: Reward = 1273.202513849595\n",
      "Episode 102/200: Reward = 1629.806220153404\n",
      "Episode 103/200: Reward = 3883.5951535098893\n",
      "Episode 104/200: Reward = 3813.34257965168\n",
      "Episode 105/200: Reward = 4080.804855986433\n",
      "Episode 106/200: Reward = 3855.046110389062\n",
      "Episode 107/200: Reward = 2439.88550873168\n",
      "Episode 108/200: Reward = 821.0459448729155\n",
      "Episode 109/200: Reward = 5769.012765200984\n",
      "Episode 110/200: Reward = 3646.593491084815\n",
      "Episode 111/200: Reward = 3959.99645657471\n",
      "Episode 112/200: Reward = 3228.2901835004686\n",
      "Episode 113/200: Reward = 1677.6769067463645\n",
      "Episode 114/200: Reward = 2019.315806803212\n",
      "Episode 115/200: Reward = 2254.2549020336014\n",
      "Episode 116/200: Reward = 4305.122353676594\n",
      "Episode 117/200: Reward = 1915.2787518314344\n",
      "Episode 118/200: Reward = 362.7490886832889\n",
      "Episode 119/200: Reward = 1609.3559658184092\n",
      "Episode 120/200: Reward = 1656.3723681327988\n",
      "Episode 121/200: Reward = 2228.434963820199\n",
      "Episode 122/200: Reward = 3600.910007689409\n",
      "Episode 123/200: Reward = 3881.411118365074\n",
      "Episode 124/200: Reward = 8977.271700154983\n",
      "Episode 125/200: Reward = 2092.059064384654\n",
      "Episode 126/200: Reward = 5499.839174024245\n",
      "Episode 127/200: Reward = 2652.5620053762404\n",
      "Episode 128/200: Reward = 1040.594643416538\n",
      "Episode 129/200: Reward = 9774.74327553111\n",
      "Episode 130/200: Reward = 3283.029196564748\n",
      "Episode 131/200: Reward = 2719.473817431932\n",
      "Episode 132/200: Reward = 1034.9861219071815\n",
      "Episode 133/200: Reward = 1644.2196482393836\n",
      "Episode 134/200: Reward = 7221.639646737143\n",
      "Episode 135/200: Reward = 9220.296779333963\n",
      "Episode 136/200: Reward = 5069.636314738478\n",
      "Episode 137/200: Reward = 4867.758675150726\n",
      "Episode 138/200: Reward = 3419.2309583441934\n",
      "Episode 139/200: Reward = 3093.1086638729903\n",
      "Episode 140/200: Reward = 1630.4227089832616\n",
      "Episode 141/200: Reward = 2434.0633962855636\n",
      "Episode 142/200: Reward = 1896.9397566190603\n",
      "Episode 143/200: Reward = 4359.2045243922075\n",
      "Episode 144/200: Reward = 1640.0707575748775\n",
      "Episode 145/200: Reward = 4310.77503998273\n",
      "Episode 146/200: Reward = 1456.8754371404032\n",
      "Episode 147/200: Reward = 2718.7033780350866\n",
      "Episode 148/200: Reward = 2859.2505726995387\n",
      "Episode 149/200: Reward = 2452.570921394317\n",
      "Episode 150/200: Reward = 1254.062212708951\n",
      "Episode 151/200: Reward = 9660.020236669312\n",
      "Episode 152/200: Reward = 1802.4347088092961\n",
      "Episode 153/200: Reward = 1266.6162179207727\n",
      "Episode 154/200: Reward = 6576.131501800931\n",
      "Episode 155/200: Reward = 1855.9756117553316\n",
      "Episode 156/200: Reward = 8025.3364750796645\n",
      "Episode 157/200: Reward = 4248.647572126082\n",
      "Episode 158/200: Reward = 4110.0825926034195\n",
      "Episode 159/200: Reward = 1020.4146886853226\n",
      "Episode 160/200: Reward = 1633.01307203599\n",
      "Episode 161/200: Reward = 2917.1408202523626\n",
      "Episode 162/200: Reward = 1851.1535804484397\n",
      "Episode 163/200: Reward = 1726.2624690357234\n",
      "Episode 164/200: Reward = 3067.7246385032154\n",
      "Episode 165/200: Reward = 4663.474552254117\n",
      "Episode 166/200: Reward = 2408.751498536396\n",
      "Episode 167/200: Reward = 5662.982764707779\n",
      "Episode 168/200: Reward = 1644.3756332585406\n",
      "Episode 169/200: Reward = 1820.6742205068788\n",
      "Episode 170/200: Reward = 4796.349261141645\n",
      "Episode 171/200: Reward = 3105.5834841278943\n",
      "Episode 172/200: Reward = 1268.4413219495939\n",
      "Episode 173/200: Reward = 1448.9440828732963\n",
      "Episode 174/200: Reward = 4052.163975519788\n",
      "Episode 175/200: Reward = 7246.958278814596\n",
      "Episode 176/200: Reward = 3479.1808947126747\n",
      "Episode 177/200: Reward = 2900.9010938051138\n",
      "Episode 178/200: Reward = 7094.0697627900545\n",
      "Episode 179/200: Reward = 3094.2943628997837\n",
      "Episode 180/200: Reward = 2870.700444091027\n",
      "Episode 181/200: Reward = 5342.660591108523\n",
      "Episode 182/200: Reward = 1892.424934172173\n",
      "Episode 183/200: Reward = 2888.716858789372\n",
      "Episode 184/200: Reward = 1044.105206425316\n",
      "Episode 185/200: Reward = 4955.504156742716\n",
      "Episode 186/200: Reward = 4687.554518428705\n",
      "Episode 187/200: Reward = 6039.506095542668\n",
      "Episode 188/200: Reward = 7026.181375087009\n",
      "Episode 189/200: Reward = 4160.833842501139\n",
      "Episode 190/200: Reward = 4452.529731904174\n",
      "Episode 191/200: Reward = 1652.289277337619\n",
      "Episode 192/200: Reward = 12474.371977631336\n",
      "Episode 193/200: Reward = 1652.36340356635\n",
      "Episode 194/200: Reward = 3053.5795706354997\n",
      "Episode 195/200: Reward = 4819.395921827249\n",
      "Episode 196/200: Reward = 2708.10499973014\n",
      "Episode 197/200: Reward = 3120.563981939693\n",
      "Episode 198/200: Reward = 5986.331589212185\n",
      "Episode 199/200: Reward = 1263.0475253750667\n",
      "Episode 200/200: Reward = 6552.086584847226\n",
      "Average Reward under Robust Sarsa Critic-based attack: 3417.1782357833936\n",
      "Final Average Reward under Robust Sarsa Attack: 3417.1782357833936\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T18:10:25.437027Z",
     "iopub.status.busy": "2024-12-13T18:10:25.436672Z",
     "iopub.status.idle": "2024-12-13T18:10:25.466133Z",
     "shell.execute_reply": "2024-12-13T18:10:25.465385Z",
     "shell.execute_reply.started": "2024-12-13T18:10:25.436999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import sophisticated networks\n",
    "class ValueDenseNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(ValueDenseNet, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        nn.init.orthogonal_(self.output_layer.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class CtsPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(CtsPolicy, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"tanh\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "        nn.init.orthogonal_(self.mean_layer.weight, gain=0.01)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SAPPOAgent:\n",
    "#     def __init__(self, state_dim, action_dim, discrete=True, lr=3e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#         # Actor and critic networks\n",
    "#         self.policy_net = CtsPolicy(state_dim, action_dim, hidden_sizes=(128, 128), activation=nn.Tanh).to(self.device)\n",
    "#         self.value_net = ValueDenseNet(state_dim, hidden_sizes=(128, 128), activation=nn.Tanh).to(self.device)\n",
    "\n",
    "#         self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "#         self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.lam = lam\n",
    "#         self.eps_clip = eps_clip\n",
    "#         self.k_epochs = k_epochs\n",
    "\n",
    "#         self.sgld_steps = sgld_steps\n",
    "#         self.sgld_lr = sgld_lr\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         state = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             mean, std = self.policy_net(state)\n",
    "#             dist = torch.distributions.Normal(mean, std)\n",
    "#             action = dist.sample()\n",
    "#             return action.cpu().numpy().squeeze(), dist.log_prob(action).sum()\n",
    "\n",
    "#     def compute_gae(self, rewards, values, dones):\n",
    "#         advantages = []\n",
    "#         advantage = 0\n",
    "#         for t in reversed(range(len(rewards))):\n",
    "#             delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "#             advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "#             advantages.insert(0, advantage)\n",
    "#         return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "#     def sgld_step(self, state, epsilon):\n",
    "#         \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "#         perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "    \n",
    "#         for _ in range(self.sgld_steps):\n",
    "#             if perturbed_state.grad is not None:\n",
    "#                 perturbed_state.grad.zero_()\n",
    "    \n",
    "#             # Compute KL divergence between original and perturbed policies\n",
    "#             with torch.no_grad():\n",
    "#                 original_logits = self.policy_net(state)\n",
    "#             perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "#             if self.policy_net.discrete:\n",
    "#                 original_policy = dist.Categorical(original_logits)\n",
    "#                 perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "#             else:\n",
    "#                 original_mean, original_std = original_logits\n",
    "#                 perturbed_mean, perturbed_std = perturbed_logits\n",
    "#                 original_policy = dist.Normal(original_mean, original_std)\n",
    "#                 perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "#             kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "    \n",
    "#             # Backpropagate KL divergence\n",
    "#             kl_div.backward()\n",
    "    \n",
    "#             # Update perturbed state using gradient and noise\n",
    "#             perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "#             perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "    \n",
    "#         return perturbed_state.detach()\n",
    "\n",
    "#     def compute_kl_regularization(self, states, actions):\n",
    "#         \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "#         if len(states) == 0:\n",
    "#             return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "#         kl_div_total = 0\n",
    "#         for state in states:\n",
    "#             perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "    \n",
    "#             with torch.no_grad():\n",
    "#                 original_logits = self.policy_net(state)\n",
    "#             perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "#             if self.policy_net.discrete:\n",
    "#                 original_policy = dist.Categorical(original_logits)\n",
    "#                 perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "#             else:\n",
    "#                 original_mean, original_std = original_logits\n",
    "#                 perturbed_mean, perturbed_std = perturbed_logits\n",
    "#                 original_policy = dist.Normal(original_mean, original_std)\n",
    "#                 perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "#             kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "#             kl_div_total += kl_div\n",
    "    \n",
    "#         return kl_div_total / len(states)\n",
    "#     def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "#         for episode in range(max_episodes):\n",
    "#             states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    \n",
    "#             # Reset the environment\n",
    "#             state, _ = env.reset()\n",
    "#             state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "#             # Rollout phase: Collect trajectories\n",
    "#             for _ in range(rollout_steps):\n",
    "#                 value = self.value_net(state).squeeze(0).detach()  # Detach the value tensor\n",
    "#                 action, log_prob = self.select_action(state.cpu().numpy())\n",
    "    \n",
    "#                 next_state, reward, done, truncated, _ = env.step(action)\n",
    "    \n",
    "#                 # Append data to lists\n",
    "#                 states.append(state.clone().detach())\n",
    "#                 actions.append(action)\n",
    "#                 rewards.append(reward)\n",
    "#                 dones.append(done or truncated)\n",
    "#                 log_probs.append(log_prob.clone().detach())\n",
    "#                 values.append(value)\n",
    "    \n",
    "#                 # Update state\n",
    "#                 state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "#                 if done or truncated:\n",
    "#                     state, _ = env.reset()\n",
    "#                     state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "#             # Add a final value estimate\n",
    "#             values.append(torch.tensor([0], device=self.device).detach())\n",
    "    \n",
    "#             # Compute advantages and returns\n",
    "#             advantages = self.compute_gae(rewards, values, dones)\n",
    "#             returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "#             # Convert lists to tensors\n",
    "#             states = torch.stack(states).to(self.device)\n",
    "#             actions = torch.tensor(\n",
    "#                 np.array(actions),\n",
    "#                 dtype=torch.float32  # Always float32 for continuous actions\n",
    "#             ).to(self.device)\n",
    "#             log_probs = torch.stack(log_probs).to(self.device)\n",
    "    \n",
    "#             # Optimization phase\n",
    "#             for _ in range(self.k_epochs):\n",
    "#                 kl_reg = self.compute_kl_regularization(states, actions)\n",
    "    \n",
    "#                 for i in range(0, rollout_steps, batch_size):\n",
    "#                     batch_states = states[i:i + batch_size]\n",
    "#                     batch_actions = actions[i:i + batch_size]\n",
    "#                     batch_log_probs = log_probs[i:i + batch_size]\n",
    "#                     batch_advantages = advantages[i:i + batch_size]\n",
    "#                     batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "#                     mean, std = self.policy_net(batch_states)\n",
    "#                     dist = torch.distributions.Normal(mean, std)\n",
    "#                     new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "#                     ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "#                     surr1 = ratio * batch_advantages\n",
    "#                     surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "#                     policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "#                     value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "#                     value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "    \n",
    "#                     # Detach kl_reg to prevent graph accumulation\n",
    "#                     kl_reg = kl_reg.detach()\n",
    "\n",
    "#                     total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg\n",
    "    \n",
    "#                     self.policy_optimizer.zero_grad()\n",
    "#                     self.value_optimizer.zero_grad()\n",
    "#                     total_loss.backward(retain_graph=False)  # No need to retain the graph here\n",
    "#                     self.policy_optimizer.step()\n",
    "#                     self.value_optimizer.step()\n",
    "    \n",
    "#             print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "     \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:33:01.312865Z",
     "iopub.status.busy": "2024-12-13T20:33:01.312531Z",
     "iopub.status.idle": "2024-12-13T20:33:01.332905Z",
     "shell.execute_reply": "2024-12-13T20:33:01.332042Z",
     "shell.execute_reply.started": "2024-12-13T20:33:01.312838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Actor and critic networks\n",
    "        self.policy_net = CtsPolicy(state_dim, action_dim).to(self.device)\n",
    "        self.value_net = ValueDenseNet(state_dim).to(self.device)\n",
    "\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        self.sgld_steps = sgld_steps\n",
    "        self.sgld_lr = sgld_lr\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        mean, std = self.policy_net(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return action.cpu().numpy(), dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def sgld_step(self, state, epsilon):\n",
    "        \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "        perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "\n",
    "        for _ in range(self.sgld_steps):\n",
    "            if perturbed_state.grad is not None:\n",
    "                perturbed_state.grad.zero_()\n",
    "\n",
    "            # Compute KL divergence between original and perturbed policies\n",
    "            with torch.no_grad():\n",
    "                original_mean, original_std = self.policy_net(state)\n",
    "            perturbed_mean, perturbed_std = self.policy_net(perturbed_state)\n",
    "\n",
    "            original_policy = dist.Normal(original_mean, original_std)\n",
    "            perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "\n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "\n",
    "            # Backpropagate KL divergence\n",
    "            kl_div.backward()\n",
    "\n",
    "            # Update perturbed state using gradient and noise\n",
    "            perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "            perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    def compute_kl_regularization(self, states, actions):\n",
    "        \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "        if len(states) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        kl_div_total = 0\n",
    "        for state in states:\n",
    "            perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                original_mean, original_std = self.policy_net(state)\n",
    "            perturbed_mean, perturbed_std = self.policy_net(perturbed_state)\n",
    "\n",
    "            original_policy = dist.Normal(original_mean, original_std)\n",
    "            perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "\n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "            kl_div_total += kl_div\n",
    "\n",
    "        return kl_div_total / len(states)\n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "\n",
    "            # Reset the environment\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Rollout phase: Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                value = self.value_net(state).squeeze(0).detach()\n",
    "                action, log_prob = self.select_action(state.cpu().numpy())\n",
    "\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                states.append(state.clone().detach())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob.clone().detach())\n",
    "                values.append(value)\n",
    "\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Add a final value estimate\n",
    "            values.append(torch.tensor([0], device=self.device).detach())\n",
    "\n",
    "            # Compute advantages and returns\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "            # Convert lists to tensors\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "\n",
    "            # Optimization phase\n",
    "            for _ in range(self.k_epochs):\n",
    "                kl_reg = self.compute_kl_regularization(states, actions)\n",
    "\n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i + batch_size]\n",
    "                    batch_actions = actions[i:i + batch_size]\n",
    "                    batch_log_probs = log_probs[i:i + batch_size]\n",
    "                    batch_advantages = advantages[i:i + batch_size]\n",
    "                    batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "                    mean, std = self.policy_net(batch_states)\n",
    "                    dist = torch.distributions.Normal(mean, std)\n",
    "                    new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "\n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "\n",
    "                    total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg.detach()\n",
    "\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:33:30.086476Z",
     "iopub.status.busy": "2024-12-13T20:33:30.085649Z",
     "iopub.status.idle": "2024-12-13T21:25:34.456941Z",
     "shell.execute_reply": "2024-12-13T21:25:34.456034Z",
     "shell.execute_reply.started": "2024-12-13T20:33:30.086442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -3.2936959266662598, Value Loss = 16.149124145507812, KL Reg = 5.190177176928046e-09\n",
      "Episode 2: Policy Loss = -8.31801986694336, Value Loss = 89.69292449951172, KL Reg = 5.093167665393139e-09\n",
      "Episode 3: Policy Loss = -7.369718551635742, Value Loss = 75.51033020019531, KL Reg = 4.908844442041982e-09\n",
      "Episode 4: Policy Loss = -4.391140937805176, Value Loss = 30.305814743041992, KL Reg = 5.617031728633037e-09\n",
      "Episode 5: Policy Loss = -6.125728607177734, Value Loss = 61.661521911621094, KL Reg = 5.258087742987527e-09\n",
      "Episode 6: Policy Loss = -3.5035226345062256, Value Loss = 21.29940414428711, KL Reg = 5.403604230735937e-09\n",
      "Episode 7: Policy Loss = -5.351285457611084, Value Loss = 35.21036911010742, KL Reg = 5.995378860035316e-09\n",
      "Episode 8: Policy Loss = -8.764406204223633, Value Loss = 108.15007781982422, KL Reg = 5.733447494549182e-09\n",
      "Episode 9: Policy Loss = -3.085789442062378, Value Loss = 20.388507843017578, KL Reg = 6.451332801304943e-09\n",
      "Episode 10: Policy Loss = -4.299745559692383, Value Loss = 26.281293869018555, KL Reg = 5.81105386032732e-09\n",
      "Episode 11: Policy Loss = -4.671839237213135, Value Loss = 29.836835861206055, KL Reg = 5.471511244081739e-09\n",
      "Episode 12: Policy Loss = -5.500026702880859, Value Loss = 40.940818786621094, KL Reg = 6.014782449881295e-09\n",
      "Episode 13: Policy Loss = -6.769908905029297, Value Loss = 56.955989837646484, KL Reg = 5.481215037406173e-09\n",
      "Episode 14: Policy Loss = -4.875415802001953, Value Loss = 31.442584991455078, KL Reg = 5.054358709344342e-09\n",
      "Episode 15: Policy Loss = -6.8178229331970215, Value Loss = 62.05345916748047, KL Reg = 5.617032616811457e-09\n",
      "Episode 16: Policy Loss = -4.13370418548584, Value Loss = 24.26702308654785, KL Reg = 5.364801047846868e-09\n",
      "Episode 17: Policy Loss = -4.6641082763671875, Value Loss = 29.337697982788086, KL Reg = 5.9177698297219195e-09\n",
      "Episode 18: Policy Loss = -5.132699966430664, Value Loss = 37.11642837524414, KL Reg = 5.849862816376117e-09\n",
      "Episode 19: Policy Loss = -5.060760498046875, Value Loss = 38.175315856933594, KL Reg = 5.18047915676334e-09\n",
      "Episode 20: Policy Loss = -3.8164849281311035, Value Loss = 21.906307220458984, KL Reg = 6.0729843376350345e-09\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    RobustAgent = SAPPOAgent(state_dim, action_dim, discrete)\n",
    "    RobustAgent.train(env, max_episodes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:58:59.470006Z",
     "iopub.status.busy": "2024-12-13T21:58:59.469289Z",
     "iopub.status.idle": "2024-12-13T21:59:02.086684Z",
     "shell.execute_reply": "2024-12-13T21:59:02.085725Z",
     "shell.execute_reply.started": "2024-12-13T21:58:59.469972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 8.305492123964871\n",
      "Episode 2: Reward = 25.747382310612124\n",
      "Episode 3: Reward = 3.2708648496659745\n",
      "Episode 4: Reward = 6.6823732225626244\n",
      "Episode 5: Reward = 10.078487054843045\n",
      "Episode 6: Reward = 9.429944475378923\n",
      "Episode 7: Reward = 9.619057926258394\n",
      "Episode 8: Reward = 9.45160716691598\n",
      "Episode 9: Reward = 14.935010016757237\n",
      "Episode 10: Reward = 6.438583452859067\n",
      "Episode 11: Reward = 6.774514376270599\n",
      "Episode 12: Reward = 8.481237161450133\n",
      "Episode 13: Reward = 12.686266890047545\n",
      "Episode 14: Reward = 6.1580337376665515\n",
      "Episode 15: Reward = 7.811738139567256\n",
      "Episode 16: Reward = 6.082923122193883\n",
      "Episode 17: Reward = 21.28172084748687\n",
      "Episode 18: Reward = 8.16732417631659\n",
      "Episode 19: Reward = 5.917934371751272\n",
      "Episode 20: Reward = 9.265311478868544\n",
      "Episode 21: Reward = 6.061407251017224\n",
      "Episode 22: Reward = 19.45414598822266\n",
      "Episode 23: Reward = 9.029031308873432\n",
      "Episode 24: Reward = 18.00142902981201\n",
      "Episode 25: Reward = 15.822545588110108\n",
      "Episode 26: Reward = 36.52550376445238\n",
      "Episode 27: Reward = 14.397145684281778\n",
      "Episode 28: Reward = 7.888907339842615\n",
      "Episode 29: Reward = 5.163227974919941\n",
      "Episode 30: Reward = 17.787624272026306\n",
      "Episode 31: Reward = 18.135466380777185\n",
      "Episode 32: Reward = 24.970184878746817\n",
      "Episode 33: Reward = 7.52559545664795\n",
      "Episode 34: Reward = 6.4645824913348875\n",
      "Episode 35: Reward = 15.862298831833353\n",
      "Episode 36: Reward = 12.868714562812135\n",
      "Episode 37: Reward = 16.842013729799103\n",
      "Episode 38: Reward = 46.132683142270224\n",
      "Episode 39: Reward = 10.21075457817568\n",
      "Episode 40: Reward = 35.127404543744355\n",
      "Episode 41: Reward = 63.11262926427967\n",
      "Episode 42: Reward = 16.49565721245691\n",
      "Episode 43: Reward = 10.620361993738998\n",
      "Episode 44: Reward = 43.11942529250384\n",
      "Episode 45: Reward = 12.86680729879124\n",
      "Episode 46: Reward = 6.9610585881324445\n",
      "Episode 47: Reward = 9.333976783153457\n",
      "Episode 48: Reward = 7.9702124740659945\n",
      "Episode 49: Reward = 7.94514962718813\n",
      "Episode 50: Reward = 8.49680781452098\n",
      "Episode 51: Reward = 8.293772218230515\n",
      "Episode 52: Reward = 7.044349468746992\n",
      "Episode 53: Reward = 10.50590585597825\n",
      "Episode 54: Reward = 14.675928876309305\n",
      "Episode 55: Reward = 25.616554556664877\n",
      "Episode 56: Reward = 5.463496553554577\n",
      "Episode 57: Reward = 7.5895980300715475\n",
      "Episode 58: Reward = 9.188998474005448\n",
      "Episode 59: Reward = 8.236467837803698\n",
      "Episode 60: Reward = 10.72798227495771\n",
      "Episode 61: Reward = 6.24730718653234\n",
      "Episode 62: Reward = 20.106442429000033\n",
      "Episode 63: Reward = 10.712602833503158\n",
      "Episode 64: Reward = 7.258798332972573\n",
      "Episode 65: Reward = 9.533296114002065\n",
      "Episode 66: Reward = 11.736311938410456\n",
      "Episode 67: Reward = 9.24950653713322\n",
      "Episode 68: Reward = 8.742750398195462\n",
      "Episode 69: Reward = 4.965315881805234\n",
      "Episode 70: Reward = 12.14780377357155\n",
      "Episode 71: Reward = 8.405252399885876\n",
      "Episode 72: Reward = 17.767398669522663\n",
      "Episode 73: Reward = 13.602850815213397\n",
      "Episode 74: Reward = 16.680568969790283\n",
      "Episode 75: Reward = 10.735206409226125\n",
      "Episode 76: Reward = 7.887441152271544\n",
      "Episode 77: Reward = 11.866639723064253\n",
      "Episode 78: Reward = 11.581810937479554\n",
      "Episode 79: Reward = 10.86486635853097\n",
      "Episode 80: Reward = 8.340281598463763\n",
      "Episode 81: Reward = 6.674617679727366\n",
      "Episode 82: Reward = 17.39770767093855\n",
      "Episode 83: Reward = 25.02352230145256\n",
      "Episode 84: Reward = 10.802303375534073\n",
      "Episode 85: Reward = 11.010950681003077\n",
      "Episode 86: Reward = 13.784006282996142\n",
      "Episode 87: Reward = 12.164428427233512\n",
      "Episode 88: Reward = 38.79714979951933\n",
      "Episode 89: Reward = 10.37336115316186\n",
      "Episode 90: Reward = 133.51344913130376\n",
      "Episode 91: Reward = 11.592657227496783\n",
      "Episode 92: Reward = 9.96013771841104\n",
      "Episode 93: Reward = 11.607984897094754\n",
      "Episode 94: Reward = 7.048316269806556\n",
      "Episode 95: Reward = 6.118048253490516\n",
      "Episode 96: Reward = 6.158357108973915\n",
      "Episode 97: Reward = 8.544461760027945\n",
      "Episode 98: Reward = 18.62373531314256\n",
      "Episode 99: Reward = 7.020417139448113\n",
      "Episode 100: Reward = 8.68098610889819\n",
      "Episode 101: Reward = 51.10946549121056\n",
      "Episode 102: Reward = 10.784621066823469\n",
      "Episode 103: Reward = 6.75908270659641\n",
      "Episode 104: Reward = 10.02071905359939\n",
      "Episode 105: Reward = 10.785783785136728\n",
      "Episode 106: Reward = 6.975706418514154\n",
      "Episode 107: Reward = 14.423299500449216\n",
      "Episode 108: Reward = 7.474310209423925\n",
      "Episode 109: Reward = 16.710144222076018\n",
      "Episode 110: Reward = 7.21086429388459\n",
      "Episode 111: Reward = 13.152624945313724\n",
      "Episode 112: Reward = 17.85524214461388\n",
      "Episode 113: Reward = 9.469334198541821\n",
      "Episode 114: Reward = 23.292425107245336\n",
      "Episode 115: Reward = 25.095193509398943\n",
      "Episode 116: Reward = 6.815685816433951\n",
      "Episode 117: Reward = 20.215898875937587\n",
      "Episode 118: Reward = 11.973312407918334\n",
      "Episode 119: Reward = 10.073608587209344\n",
      "Episode 120: Reward = 9.062460001717398\n",
      "Episode 121: Reward = 7.421264856335739\n",
      "Episode 122: Reward = 16.745920802364544\n",
      "Episode 123: Reward = 7.568656537830759\n",
      "Episode 124: Reward = 26.781930074639703\n",
      "Episode 125: Reward = 12.097343824742351\n",
      "Episode 126: Reward = 17.176658468683517\n",
      "Episode 127: Reward = 12.158863438290517\n",
      "Episode 128: Reward = 6.599466333907143\n",
      "Episode 129: Reward = 10.448594294994955\n",
      "Episode 130: Reward = 5.265088321742528\n",
      "Episode 131: Reward = 7.559684868512866\n",
      "Episode 132: Reward = 36.526138650910376\n",
      "Episode 133: Reward = 18.684823846636775\n",
      "Episode 134: Reward = 8.42853744145338\n",
      "Episode 135: Reward = 86.0279327263531\n",
      "Episode 136: Reward = 15.053861636820539\n",
      "Episode 137: Reward = 13.287188127592566\n",
      "Episode 138: Reward = 6.0896328496296945\n",
      "Episode 139: Reward = 6.246092485582688\n",
      "Episode 140: Reward = 10.54816472005818\n",
      "Episode 141: Reward = 11.650566713192337\n",
      "Episode 142: Reward = 22.35192382412481\n",
      "Episode 143: Reward = 23.3116440472196\n",
      "Episode 144: Reward = 19.926441828644002\n",
      "Episode 145: Reward = 9.646934191183046\n",
      "Episode 146: Reward = 32.21840066104811\n",
      "Episode 147: Reward = 13.482670461600247\n",
      "Episode 148: Reward = 9.185381043887368\n",
      "Episode 149: Reward = 6.32269117951206\n",
      "Episode 150: Reward = 6.333904283330101\n",
      "Episode 151: Reward = 7.461359646105624\n",
      "Episode 152: Reward = 11.021367227472492\n",
      "Episode 153: Reward = 8.542120015677787\n",
      "Episode 154: Reward = 10.231458751812134\n",
      "Episode 155: Reward = 11.106622716743008\n",
      "Episode 156: Reward = 11.130317817664515\n",
      "Episode 157: Reward = 8.881713669813907\n",
      "Episode 158: Reward = 5.428941264265735\n",
      "Episode 159: Reward = 43.15575622759494\n",
      "Episode 160: Reward = 9.439816088460203\n",
      "Episode 161: Reward = 10.26401004780006\n",
      "Episode 162: Reward = 16.113569260059833\n",
      "Episode 163: Reward = 14.372559452182983\n",
      "Episode 164: Reward = 9.788475416462393\n",
      "Episode 165: Reward = 21.80874542790879\n",
      "Episode 166: Reward = 17.549254939755652\n",
      "Episode 167: Reward = 7.109886926147017\n",
      "Episode 168: Reward = 4.006968099970866\n",
      "Episode 169: Reward = 8.334945791031293\n",
      "Episode 170: Reward = 104.01911572306798\n",
      "Episode 171: Reward = 7.791494615835725\n",
      "Episode 172: Reward = 7.237320324751205\n",
      "Episode 173: Reward = 36.73228302121266\n",
      "Episode 174: Reward = 4.688733154722872\n",
      "Episode 175: Reward = 6.703254044998477\n",
      "Episode 176: Reward = 6.956273740442967\n",
      "Episode 177: Reward = 14.422163104035297\n",
      "Episode 178: Reward = 6.831342986366836\n",
      "Episode 179: Reward = 9.510098190986525\n",
      "Episode 180: Reward = 8.343419483581393\n",
      "Episode 181: Reward = 8.955588234818952\n",
      "Episode 182: Reward = 16.0407444411499\n",
      "Episode 183: Reward = 10.81066607160223\n",
      "Episode 184: Reward = 17.60940869068642\n",
      "Episode 185: Reward = 8.209631308592058\n",
      "Episode 186: Reward = 16.027110499687392\n",
      "Episode 187: Reward = 5.7855032029872495\n",
      "Episode 188: Reward = 11.066637215819911\n",
      "Episode 189: Reward = 7.796126591582585\n",
      "Episode 190: Reward = 14.077552748897979\n",
      "Episode 191: Reward = 7.8371463267719745\n",
      "Episode 192: Reward = 7.7205092475819\n",
      "Episode 193: Reward = 75.68029798824611\n",
      "Episode 194: Reward = 6.844527846204859\n",
      "Episode 195: Reward = 26.544057084829575\n",
      "Episode 196: Reward = 12.623344916056755\n",
      "Episode 197: Reward = 10.999143650743083\n",
      "Episode 198: Reward = 9.391725341021825\n",
      "Episode 199: Reward = 13.029767822647697\n",
      "Episode 200: Reward = 10.402230526348925\n",
      "Average Reward over 200 Episodes: 14.746459783843028\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, RobustAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:59:05.644255Z",
     "iopub.status.busy": "2024-12-13T21:59:05.643930Z",
     "iopub.status.idle": "2024-12-13T21:59:11.512541Z",
     "shell.execute_reply": "2024-12-13T21:59:11.511712Z",
     "shell.execute_reply.started": "2024-12-13T21:59:05.644223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 39.2224931317058\n",
      "Episode 2: Reward = 41.22929859486118\n",
      "Episode 3: Reward = 40.45784215218881\n",
      "Episode 4: Reward = 40.23283605668382\n",
      "Episode 5: Reward = 40.39897994694226\n",
      "Episode 6: Reward = 40.221428874264504\n",
      "Episode 7: Reward = 41.01772176944938\n",
      "Episode 8: Reward = 40.27922336631392\n",
      "Episode 9: Reward = 39.061015672546574\n",
      "Episode 10: Reward = 40.70719901907581\n",
      "Episode 11: Reward = 40.31623703239629\n",
      "Episode 12: Reward = 38.260970694485444\n",
      "Episode 13: Reward = 39.33027439897924\n",
      "Episode 14: Reward = 40.41245361055472\n",
      "Episode 15: Reward = 40.08277081093781\n",
      "Episode 16: Reward = 41.2159869693659\n",
      "Episode 17: Reward = 41.54063980094815\n",
      "Episode 18: Reward = 39.195923863531505\n",
      "Episode 19: Reward = 39.36841631877313\n",
      "Episode 20: Reward = 40.094481008761214\n",
      "Episode 21: Reward = 40.09466668347692\n",
      "Episode 22: Reward = 40.35980564809633\n",
      "Episode 23: Reward = 40.15648743358332\n",
      "Episode 24: Reward = 40.58885837654398\n",
      "Episode 25: Reward = 40.14466295435432\n",
      "Episode 26: Reward = 40.083028072710924\n",
      "Episode 27: Reward = 41.11035175907372\n",
      "Episode 28: Reward = 38.59393821670517\n",
      "Episode 29: Reward = 40.45692197003214\n",
      "Episode 30: Reward = 40.2178112496349\n",
      "Episode 31: Reward = 40.19343825498544\n",
      "Episode 32: Reward = 39.17041140406173\n",
      "Episode 33: Reward = 40.43358106298522\n",
      "Episode 34: Reward = 41.49097229116223\n",
      "Episode 35: Reward = 38.19207977822882\n",
      "Episode 36: Reward = 39.50613258759643\n",
      "Episode 37: Reward = 40.497484472572864\n",
      "Episode 38: Reward = 41.060080467030865\n",
      "Episode 39: Reward = 39.43394917450541\n",
      "Episode 40: Reward = 40.257568156676875\n",
      "Episode 41: Reward = 39.691161136966635\n",
      "Episode 42: Reward = 40.317231636493204\n",
      "Episode 43: Reward = 39.986911834189236\n",
      "Episode 44: Reward = 40.54013704246041\n",
      "Episode 45: Reward = 39.217775230447955\n",
      "Episode 46: Reward = 38.285595850819846\n",
      "Episode 47: Reward = 40.328120967202906\n",
      "Episode 48: Reward = 38.993422828194866\n",
      "Episode 49: Reward = 39.44199291645172\n",
      "Episode 50: Reward = 40.10702102346446\n",
      "Episode 51: Reward = 39.033791739085316\n",
      "Episode 52: Reward = 41.093045793639014\n",
      "Episode 53: Reward = 39.404452976603075\n",
      "Episode 54: Reward = 40.21332281741329\n",
      "Episode 55: Reward = 39.10593397943871\n",
      "Episode 56: Reward = 39.89409145762207\n",
      "Episode 57: Reward = 40.17530514626858\n",
      "Episode 58: Reward = 41.01898443741661\n",
      "Episode 59: Reward = 40.23878099306391\n",
      "Episode 60: Reward = 39.35050680476334\n",
      "Episode 61: Reward = 40.30748980346448\n",
      "Episode 62: Reward = 40.28148038555233\n",
      "Episode 63: Reward = 41.37144713393491\n",
      "Episode 64: Reward = 41.562965086557426\n",
      "Episode 65: Reward = 39.960503800662174\n",
      "Episode 66: Reward = 39.06436294398744\n",
      "Episode 67: Reward = 41.474104060083235\n",
      "Episode 68: Reward = 39.117476753035774\n",
      "Episode 69: Reward = 39.10894364457396\n",
      "Episode 70: Reward = 40.379566908766925\n",
      "Episode 71: Reward = 39.39330730783707\n",
      "Episode 72: Reward = 38.94888622164953\n",
      "Episode 73: Reward = 39.427120823295645\n",
      "Episode 74: Reward = 39.15118233019189\n",
      "Episode 75: Reward = 39.24866495718345\n",
      "Episode 76: Reward = 39.2751192346024\n",
      "Episode 77: Reward = 38.36886384058732\n",
      "Episode 78: Reward = 40.03360836201786\n",
      "Episode 79: Reward = 40.324660409471676\n",
      "Episode 80: Reward = 40.17222197317133\n",
      "Episode 81: Reward = 39.44385399346674\n",
      "Episode 82: Reward = 40.19505924654358\n",
      "Episode 83: Reward = 39.9492001798294\n",
      "Episode 84: Reward = 40.475324554620876\n",
      "Episode 85: Reward = 41.18145347509882\n",
      "Episode 86: Reward = 39.194256003043044\n",
      "Episode 87: Reward = 39.38155404541896\n",
      "Episode 88: Reward = 39.34418011646372\n",
      "Episode 89: Reward = 40.327643669602026\n",
      "Episode 90: Reward = 40.18799391987218\n",
      "Episode 91: Reward = 40.21373624818141\n",
      "Episode 92: Reward = 40.201166511651124\n",
      "Episode 93: Reward = 41.29329901555728\n",
      "Episode 94: Reward = 40.06748749110394\n",
      "Episode 95: Reward = 39.31576513263669\n",
      "Episode 96: Reward = 38.17133021599721\n",
      "Episode 97: Reward = 40.43475229679748\n",
      "Episode 98: Reward = 40.44610359409948\n",
      "Episode 99: Reward = 39.04068575115348\n",
      "Episode 100: Reward = 40.27536116367012\n",
      "Episode 101: Reward = 40.04792702294734\n",
      "Episode 102: Reward = 40.26237740625154\n",
      "Episode 103: Reward = 39.53376643156417\n",
      "Episode 104: Reward = 40.112797957729654\n",
      "Episode 105: Reward = 39.41264611965923\n",
      "Episode 106: Reward = 40.38553634982328\n",
      "Episode 107: Reward = 38.96658590017882\n",
      "Episode 108: Reward = 40.12361959066406\n",
      "Episode 109: Reward = 40.06991714025977\n",
      "Episode 110: Reward = 39.47273876256074\n",
      "Episode 111: Reward = 40.38392531437123\n",
      "Episode 112: Reward = 39.31923553656598\n",
      "Episode 113: Reward = 40.3048738480172\n",
      "Episode 114: Reward = 40.24006038114746\n",
      "Episode 115: Reward = 41.132558491278594\n",
      "Episode 116: Reward = 39.29735500790663\n",
      "Episode 117: Reward = 40.07139494594416\n",
      "Episode 118: Reward = 39.19826872307495\n",
      "Episode 119: Reward = 40.59923911796182\n",
      "Episode 120: Reward = 39.96425689083543\n",
      "Episode 121: Reward = 39.3056692835899\n",
      "Episode 122: Reward = 41.268190090327856\n",
      "Episode 123: Reward = 39.28642980412808\n",
      "Episode 124: Reward = 39.41191363280649\n",
      "Episode 125: Reward = 39.111765905960915\n",
      "Episode 126: Reward = 39.88340337308701\n",
      "Episode 127: Reward = 41.447364140930986\n",
      "Episode 128: Reward = 38.90365357609902\n",
      "Episode 129: Reward = 40.439208596726345\n",
      "Episode 130: Reward = 38.95028290016834\n",
      "Episode 131: Reward = 40.527062136696266\n",
      "Episode 132: Reward = 39.17670860155498\n",
      "Episode 133: Reward = 40.13836076770328\n",
      "Episode 134: Reward = 41.20073477370002\n",
      "Episode 135: Reward = 40.19591725468987\n",
      "Episode 136: Reward = 38.050978812722704\n",
      "Episode 137: Reward = 39.49418454773086\n",
      "Episode 138: Reward = 40.552229565831425\n",
      "Episode 139: Reward = 39.20529940962092\n",
      "Episode 140: Reward = 40.168633167276674\n",
      "Episode 141: Reward = 41.28971242872397\n",
      "Episode 142: Reward = 39.928651441126114\n",
      "Episode 143: Reward = 40.17337387036011\n",
      "Episode 144: Reward = 39.56139891321085\n",
      "Episode 145: Reward = 39.35863036021366\n",
      "Episode 146: Reward = 40.127070480899434\n",
      "Episode 147: Reward = 39.188861118321064\n",
      "Episode 148: Reward = 40.4644760614651\n",
      "Episode 149: Reward = 41.47471394331771\n",
      "Episode 150: Reward = 41.56837548207588\n",
      "Episode 151: Reward = 40.38196722748824\n",
      "Episode 152: Reward = 39.477456623933975\n",
      "Episode 153: Reward = 41.243433321073304\n",
      "Episode 154: Reward = 41.505038595805196\n",
      "Episode 155: Reward = 38.21197635021319\n",
      "Episode 156: Reward = 39.35246121154467\n",
      "Episode 157: Reward = 39.50955472529263\n",
      "Episode 158: Reward = 41.34071116977088\n",
      "Episode 159: Reward = 41.201040704766825\n",
      "Episode 160: Reward = 38.909552588241056\n",
      "Episode 161: Reward = 40.277818989657526\n",
      "Episode 162: Reward = 39.07428774661456\n",
      "Episode 163: Reward = 39.97645205360465\n",
      "Episode 164: Reward = 40.44073023421456\n",
      "Episode 165: Reward = 40.220709550791156\n",
      "Episode 166: Reward = 40.178546191526785\n",
      "Episode 167: Reward = 38.27786231743284\n",
      "Episode 168: Reward = 40.39724419943475\n",
      "Episode 169: Reward = 41.07093416959399\n",
      "Episode 170: Reward = 40.88075937496602\n",
      "Episode 171: Reward = 41.10317039542298\n",
      "Episode 172: Reward = 40.37277720178307\n",
      "Episode 173: Reward = 40.20035394633305\n",
      "Episode 174: Reward = 40.12607025140401\n",
      "Episode 175: Reward = 41.11955813536698\n",
      "Episode 176: Reward = 39.14555410282929\n",
      "Episode 177: Reward = 39.3378516356064\n",
      "Episode 178: Reward = 40.3574145930545\n",
      "Episode 179: Reward = 40.21409526880696\n",
      "Episode 180: Reward = 41.48307547847751\n",
      "Episode 181: Reward = 41.29756023864233\n",
      "Episode 182: Reward = 38.54746471800486\n",
      "Episode 183: Reward = 40.061756680628285\n",
      "Episode 184: Reward = 40.26420859605478\n",
      "Episode 185: Reward = 38.39192708553786\n",
      "Episode 186: Reward = 38.385079848933366\n",
      "Episode 187: Reward = 39.62037256950436\n",
      "Episode 188: Reward = 40.21762889093074\n",
      "Episode 189: Reward = 39.50057165871844\n",
      "Episode 190: Reward = 38.97287610879229\n",
      "Episode 191: Reward = 40.286153356969294\n",
      "Episode 192: Reward = 40.384624568206824\n",
      "Episode 193: Reward = 41.33406550030032\n",
      "Episode 194: Reward = 41.507565395386024\n",
      "Episode 195: Reward = 39.174772457143874\n",
      "Episode 196: Reward = 39.97655965064974\n",
      "Episode 197: Reward = 40.27402789181696\n",
      "Episode 198: Reward = 40.37564334030645\n",
      "Episode 199: Reward = 40.436953030275475\n",
      "Episode 200: Reward = 41.4186263255673\n",
      "Average Reward over 200 episodes: 40.009037119857226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.009037119857226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:59:17.122777Z",
     "iopub.status.busy": "2024-12-13T21:59:17.121965Z",
     "iopub.status.idle": "2024-12-13T22:00:53.561626Z",
     "shell.execute_reply": "2024-12-13T22:00:53.560701Z",
     "shell.execute_reply.started": "2024-12-13T21:59:17.122729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 10.434742914791503\n",
      "Episode 2/200: Reward = 5.417426003477402\n",
      "Episode 3/200: Reward = 14.668772095687922\n",
      "Episode 4/200: Reward = 38.2123422633607\n",
      "Episode 5/200: Reward = 14.282820308923435\n",
      "Episode 6/200: Reward = 8.953468696901771\n",
      "Episode 7/200: Reward = 17.629842888418775\n",
      "Episode 8/200: Reward = 8.24715327926889\n",
      "Episode 9/200: Reward = 15.3304959363805\n",
      "Episode 10/200: Reward = 133.57492896370198\n",
      "Episode 11/200: Reward = 8.08644098517564\n",
      "Episode 12/200: Reward = 25.81871571647448\n",
      "Episode 13/200: Reward = 7.176300306696672\n",
      "Episode 14/200: Reward = 13.486829103478758\n",
      "Episode 15/200: Reward = 10.950812106214816\n",
      "Episode 16/200: Reward = 24.268289808574032\n",
      "Episode 17/200: Reward = 11.207309992832563\n",
      "Episode 18/200: Reward = 4.0628568264083995\n",
      "Episode 19/200: Reward = 10.721401542100223\n",
      "Episode 20/200: Reward = 9.15268438293499\n",
      "Episode 21/200: Reward = 15.409369871994203\n",
      "Episode 22/200: Reward = 12.352357959653983\n",
      "Episode 23/200: Reward = 8.117591254145553\n",
      "Episode 24/200: Reward = 9.920439372282884\n",
      "Episode 25/200: Reward = 13.347379264567108\n",
      "Episode 26/200: Reward = 10.225415923439801\n",
      "Episode 27/200: Reward = 58.959763341540864\n",
      "Episode 28/200: Reward = 8.102298089919293\n",
      "Episode 29/200: Reward = 5.452928513934436\n",
      "Episode 30/200: Reward = 11.434637416612018\n",
      "Episode 31/200: Reward = 7.155783169051557\n",
      "Episode 32/200: Reward = 9.78369122741243\n",
      "Episode 33/200: Reward = 12.198663780908984\n",
      "Episode 34/200: Reward = 8.331930854488368\n",
      "Episode 35/200: Reward = 10.007019198234957\n",
      "Episode 36/200: Reward = 41.88540178314381\n",
      "Episode 37/200: Reward = 11.58182554239045\n",
      "Episode 38/200: Reward = 16.765111898317286\n",
      "Episode 39/200: Reward = 7.371255948757386\n",
      "Episode 40/200: Reward = 7.184907595405289\n",
      "Episode 41/200: Reward = 10.327280923727876\n",
      "Episode 42/200: Reward = 8.68191082002159\n",
      "Episode 43/200: Reward = 12.042486245363687\n",
      "Episode 44/200: Reward = 12.318972401045428\n",
      "Episode 45/200: Reward = 13.828533354822596\n",
      "Episode 46/200: Reward = 13.096362831935618\n",
      "Episode 47/200: Reward = 13.111062443883736\n",
      "Episode 48/200: Reward = 7.73341579162238\n",
      "Episode 49/200: Reward = 19.451218403485857\n",
      "Episode 50/200: Reward = 12.417431944891646\n",
      "Episode 51/200: Reward = 7.89357654953152\n",
      "Episode 52/200: Reward = 10.589838875854511\n",
      "Episode 53/200: Reward = 8.344361555720049\n",
      "Episode 54/200: Reward = 8.281375013044274\n",
      "Episode 55/200: Reward = 81.25401766695188\n",
      "Episode 56/200: Reward = 6.529568934184502\n",
      "Episode 57/200: Reward = 12.108312127573496\n",
      "Episode 58/200: Reward = 15.768323407742928\n",
      "Episode 59/200: Reward = 8.602479931930977\n",
      "Episode 60/200: Reward = 15.960419143444506\n",
      "Episode 61/200: Reward = 8.749791124511308\n",
      "Episode 62/200: Reward = 39.752626713871464\n",
      "Episode 63/200: Reward = 8.46457449766868\n",
      "Episode 64/200: Reward = 44.674423971478625\n",
      "Episode 65/200: Reward = 6.993741350362681\n",
      "Episode 66/200: Reward = 9.797537708701196\n",
      "Episode 67/200: Reward = 8.49988192977527\n",
      "Episode 68/200: Reward = 25.854091119284817\n",
      "Episode 69/200: Reward = 7.884125286459066\n",
      "Episode 70/200: Reward = 7.8869836789409815\n",
      "Episode 71/200: Reward = 5.94329244549852\n",
      "Episode 72/200: Reward = 7.706373366459793\n",
      "Episode 73/200: Reward = 16.017240298306362\n",
      "Episode 74/200: Reward = 19.163728881795528\n",
      "Episode 75/200: Reward = 8.329771852462567\n",
      "Episode 76/200: Reward = 42.64997947324097\n",
      "Episode 77/200: Reward = 41.72981363078493\n",
      "Episode 78/200: Reward = 7.7450293225428775\n",
      "Episode 79/200: Reward = 66.6864619443085\n",
      "Episode 80/200: Reward = 6.49757915978448\n",
      "Episode 81/200: Reward = 9.909142004730436\n",
      "Episode 82/200: Reward = 11.745864690210745\n",
      "Episode 83/200: Reward = 25.681783385696992\n",
      "Episode 84/200: Reward = 10.616908433352634\n",
      "Episode 85/200: Reward = 10.688447766961659\n",
      "Episode 86/200: Reward = 27.30604585812352\n",
      "Episode 87/200: Reward = 26.78579352689166\n",
      "Episode 88/200: Reward = 6.209906358859901\n",
      "Episode 89/200: Reward = 13.777272364674724\n",
      "Episode 90/200: Reward = 8.981424678910454\n",
      "Episode 91/200: Reward = 16.77106390685242\n",
      "Episode 92/200: Reward = 14.92851732220065\n",
      "Episode 93/200: Reward = 16.432831557466155\n",
      "Episode 94/200: Reward = 56.24011261684414\n",
      "Episode 95/200: Reward = 6.123197999770206\n",
      "Episode 96/200: Reward = 8.118526451641573\n",
      "Episode 97/200: Reward = 24.93430260241433\n",
      "Episode 98/200: Reward = 7.058823494722083\n",
      "Episode 99/200: Reward = 67.86572771762138\n",
      "Episode 100/200: Reward = 5.82060721259579\n",
      "Episode 101/200: Reward = 25.26904392310598\n",
      "Episode 102/200: Reward = 47.67261590948908\n",
      "Episode 103/200: Reward = 41.15559418014264\n",
      "Episode 104/200: Reward = 6.770827160758983\n",
      "Episode 105/200: Reward = 7.256049108771776\n",
      "Episode 106/200: Reward = 18.947619724980107\n",
      "Episode 107/200: Reward = 42.91386069501616\n",
      "Episode 108/200: Reward = 16.51558804823407\n",
      "Episode 109/200: Reward = 16.34156579421299\n",
      "Episode 110/200: Reward = 5.204895631632926\n",
      "Episode 111/200: Reward = 8.438059521440373\n",
      "Episode 112/200: Reward = 6.5449911341562474\n",
      "Episode 113/200: Reward = 10.160861781749713\n",
      "Episode 114/200: Reward = 67.46534121811\n",
      "Episode 115/200: Reward = 15.222285590829866\n",
      "Episode 116/200: Reward = 13.310631995752919\n",
      "Episode 117/200: Reward = 13.466880990714388\n",
      "Episode 118/200: Reward = 9.769784911055206\n",
      "Episode 119/200: Reward = 5.750991729607159\n",
      "Episode 120/200: Reward = 9.877911236166407\n",
      "Episode 121/200: Reward = 12.798176615557297\n",
      "Episode 122/200: Reward = 7.8207597370129385\n",
      "Episode 123/200: Reward = 9.889736668495615\n",
      "Episode 124/200: Reward = 10.057896796805565\n",
      "Episode 125/200: Reward = 10.29861184374701\n",
      "Episode 126/200: Reward = 71.96129374519168\n",
      "Episode 127/200: Reward = 35.9419349760144\n",
      "Episode 128/200: Reward = 23.007438417792535\n",
      "Episode 129/200: Reward = 8.848530947988033\n",
      "Episode 130/200: Reward = 17.520737580751018\n",
      "Episode 131/200: Reward = 14.33344206646229\n",
      "Episode 132/200: Reward = 18.79213724051916\n",
      "Episode 133/200: Reward = 10.23899234514161\n",
      "Episode 134/200: Reward = 63.56615491104018\n",
      "Episode 135/200: Reward = 10.539254211030006\n",
      "Episode 136/200: Reward = 15.612061338470841\n",
      "Episode 137/200: Reward = 5.418147253213789\n",
      "Episode 138/200: Reward = 8.140503662147566\n",
      "Episode 139/200: Reward = 20.08616938463257\n",
      "Episode 140/200: Reward = 10.048120936176321\n",
      "Episode 141/200: Reward = 7.656245028426488\n",
      "Episode 142/200: Reward = 11.515429200349248\n",
      "Episode 143/200: Reward = 14.110981676444423\n",
      "Episode 144/200: Reward = 8.36506936639234\n",
      "Episode 145/200: Reward = 9.042831573611473\n",
      "Episode 146/200: Reward = 10.447001177547651\n",
      "Episode 147/200: Reward = 12.956638402239156\n",
      "Episode 148/200: Reward = 14.449430614225781\n",
      "Episode 149/200: Reward = 13.137813648753257\n",
      "Episode 150/200: Reward = 15.413660764370132\n",
      "Episode 151/200: Reward = 15.548032595334588\n",
      "Episode 152/200: Reward = 17.00945360552954\n",
      "Episode 153/200: Reward = 9.14164132371288\n",
      "Episode 154/200: Reward = 15.736207452074275\n",
      "Episode 155/200: Reward = 6.607071550965188\n",
      "Episode 156/200: Reward = 19.21183263115409\n",
      "Episode 157/200: Reward = 17.874396513578017\n",
      "Episode 158/200: Reward = 25.575790591788966\n",
      "Episode 159/200: Reward = 19.671571309057025\n",
      "Episode 160/200: Reward = 6.347274696893577\n",
      "Episode 161/200: Reward = 16.5803576190886\n",
      "Episode 162/200: Reward = 8.69006431493796\n",
      "Episode 163/200: Reward = 10.319144112696408\n",
      "Episode 164/200: Reward = 11.511935913647644\n",
      "Episode 165/200: Reward = 4.74599212866916\n",
      "Episode 166/200: Reward = 9.426609605311889\n",
      "Episode 167/200: Reward = 7.165752332431347\n",
      "Episode 168/200: Reward = 51.868528301424206\n",
      "Episode 169/200: Reward = 6.496512279389658\n",
      "Episode 170/200: Reward = 12.768649213991079\n",
      "Episode 171/200: Reward = 18.517270533484577\n",
      "Episode 172/200: Reward = 4.739812766630852\n",
      "Episode 173/200: Reward = 13.930017221884157\n",
      "Episode 174/200: Reward = 10.352857970809204\n",
      "Episode 175/200: Reward = 7.624394879505488\n",
      "Episode 176/200: Reward = 19.677771393669605\n",
      "Episode 177/200: Reward = 10.158129478801165\n",
      "Episode 178/200: Reward = 14.666722308271895\n",
      "Episode 179/200: Reward = 19.41043245392043\n",
      "Episode 180/200: Reward = 16.912728079115976\n",
      "Episode 181/200: Reward = 27.77700575258704\n",
      "Episode 182/200: Reward = 60.803388369705935\n",
      "Episode 183/200: Reward = 7.002747478196735\n",
      "Episode 184/200: Reward = 7.370151228913031\n",
      "Episode 185/200: Reward = 4.8985317196525635\n",
      "Episode 186/200: Reward = 5.070213746656338\n",
      "Episode 187/200: Reward = 9.54263667068285\n",
      "Episode 188/200: Reward = 6.797609796104408\n",
      "Episode 189/200: Reward = 15.167453658342364\n",
      "Episode 190/200: Reward = 8.785708333076661\n",
      "Episode 191/200: Reward = 27.106988160091973\n",
      "Episode 192/200: Reward = 12.083564842352182\n",
      "Episode 193/200: Reward = 7.804915330540557\n",
      "Episode 194/200: Reward = 6.227972166237563\n",
      "Episode 195/200: Reward = 48.695985090160875\n",
      "Episode 196/200: Reward = 10.559018948696231\n",
      "Episode 197/200: Reward = 31.86492676281576\n",
      "Episode 198/200: Reward = 5.759013988963417\n",
      "Episode 199/200: Reward = 11.58457578652522\n",
      "Episode 200/200: Reward = 14.90009751672888\n",
      "Average Reward under MAD attack: 16.953978825714188\n",
      "Final Average Reward under MAD Attack: 16.953978825714188\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:11:27.729664Z",
     "iopub.status.busy": "2024-12-13T22:11:27.729346Z",
     "iopub.status.idle": "2024-12-13T22:15:11.569359Z",
     "shell.execute_reply": "2024-12-13T22:15:11.568409Z",
     "shell.execute_reply.started": "2024-12-13T22:11:27.729636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4284193347.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/5000, TD Loss: 0.6278, Robust Loss: 0.1636\n",
      "Step 200/5000, TD Loss: 0.5392, Robust Loss: 0.3412\n",
      "Step 300/5000, TD Loss: 0.5079, Robust Loss: 0.3753\n",
      "Step 400/5000, TD Loss: 0.4827, Robust Loss: 0.7698\n",
      "Step 500/5000, TD Loss: 0.4616, Robust Loss: 0.5638\n",
      "Step 600/5000, TD Loss: 0.4881, Robust Loss: 0.9662\n",
      "Step 700/5000, TD Loss: 0.4720, Robust Loss: 0.7391\n",
      "Step 800/5000, TD Loss: 0.4840, Robust Loss: 0.8571\n",
      "Step 900/5000, TD Loss: 0.4464, Robust Loss: 0.8866\n",
      "Step 1000/5000, TD Loss: 0.4585, Robust Loss: 1.0489\n",
      "Step 1100/5000, TD Loss: 0.4712, Robust Loss: 0.8254\n",
      "Step 1200/5000, TD Loss: 0.4780, Robust Loss: 0.9241\n",
      "Step 1300/5000, TD Loss: 0.4499, Robust Loss: 0.9922\n",
      "Step 1400/5000, TD Loss: 0.4310, Robust Loss: 0.7281\n",
      "Step 1500/5000, TD Loss: 0.4719, Robust Loss: 0.9722\n",
      "Step 1600/5000, TD Loss: 0.4361, Robust Loss: 0.9781\n",
      "Step 1700/5000, TD Loss: 0.4505, Robust Loss: 0.8806\n",
      "Step 1800/5000, TD Loss: 0.4228, Robust Loss: 1.3964\n",
      "Step 1900/5000, TD Loss: 0.4204, Robust Loss: 0.7944\n",
      "Step 2000/5000, TD Loss: 0.4761, Robust Loss: 0.9264\n",
      "Step 2100/5000, TD Loss: 0.4828, Robust Loss: 0.7450\n",
      "Step 2200/5000, TD Loss: 0.4871, Robust Loss: 0.9805\n",
      "Step 2300/5000, TD Loss: 0.4537, Robust Loss: 0.8589\n",
      "Step 2400/5000, TD Loss: 0.4738, Robust Loss: 0.8882\n",
      "Step 2500/5000, TD Loss: 0.4622, Robust Loss: 0.7853\n",
      "Step 2600/5000, TD Loss: 0.4316, Robust Loss: 0.8568\n",
      "Step 2700/5000, TD Loss: 0.4621, Robust Loss: 0.7660\n",
      "Step 2800/5000, TD Loss: 0.4503, Robust Loss: 1.0404\n",
      "Step 2900/5000, TD Loss: 0.4424, Robust Loss: 1.0314\n",
      "Step 3000/5000, TD Loss: 0.4346, Robust Loss: 0.9771\n",
      "Step 3100/5000, TD Loss: 0.4482, Robust Loss: 0.7588\n",
      "Step 3200/5000, TD Loss: 0.4556, Robust Loss: 1.0873\n",
      "Step 3300/5000, TD Loss: 0.4396, Robust Loss: 0.8976\n",
      "Step 3400/5000, TD Loss: 0.4181, Robust Loss: 0.7356\n",
      "Step 3500/5000, TD Loss: 0.4538, Robust Loss: 0.8886\n",
      "Step 3600/5000, TD Loss: 0.4428, Robust Loss: 0.8168\n",
      "Step 3700/5000, TD Loss: 0.4397, Robust Loss: 0.9195\n",
      "Step 3800/5000, TD Loss: 0.4409, Robust Loss: 1.0395\n",
      "Step 3900/5000, TD Loss: 0.4581, Robust Loss: 0.6424\n",
      "Step 4000/5000, TD Loss: 0.4899, Robust Loss: 0.6358\n",
      "Step 4100/5000, TD Loss: 0.4287, Robust Loss: 0.7413\n",
      "Step 4200/5000, TD Loss: 0.4632, Robust Loss: 0.6735\n",
      "Step 4300/5000, TD Loss: 0.4470, Robust Loss: 0.8896\n",
      "Step 4400/5000, TD Loss: 0.4409, Robust Loss: 0.6880\n",
      "Step 4500/5000, TD Loss: 0.4208, Robust Loss: 1.0079\n",
      "Step 4600/5000, TD Loss: 0.4220, Robust Loss: 0.7056\n",
      "Step 4700/5000, TD Loss: 0.4301, Robust Loss: 0.7896\n",
      "Step 4800/5000, TD Loss: 0.4456, Robust Loss: 0.8674\n",
      "Step 4900/5000, TD Loss: 0.4435, Robust Loss: 0.8214\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:17:35.003403Z",
     "iopub.status.busy": "2024-12-13T22:17:35.003170Z",
     "iopub.status.idle": "2024-12-13T22:19:11.930517Z",
     "shell.execute_reply": "2024-12-13T22:19:11.929639Z",
     "shell.execute_reply.started": "2024-12-13T22:17:35.003380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 39.48946673843793\n",
      "Episode 2/200: Reward = 38.88514034896076\n",
      "Episode 3/200: Reward = 39.399382327984625\n",
      "Episode 4/200: Reward = 41.69809705750213\n",
      "Episode 5/200: Reward = 40.36334403271557\n",
      "Episode 6/200: Reward = 39.28390451162531\n",
      "Episode 7/200: Reward = 39.000457952806386\n",
      "Episode 8/200: Reward = 39.27576847154834\n",
      "Episode 9/200: Reward = 39.41900023785964\n",
      "Episode 10/200: Reward = 39.097197104467675\n",
      "Episode 11/200: Reward = 39.22610448110138\n",
      "Episode 12/200: Reward = 40.34034905099732\n",
      "Episode 13/200: Reward = 39.37402439952458\n",
      "Episode 14/200: Reward = 38.909393465164904\n",
      "Episode 15/200: Reward = 40.27047623765004\n",
      "Episode 16/200: Reward = 39.40471991610045\n",
      "Episode 17/200: Reward = 40.133139698614194\n",
      "Episode 18/200: Reward = 40.07949520524676\n",
      "Episode 19/200: Reward = 40.53360367318834\n",
      "Episode 20/200: Reward = 41.43635253200792\n",
      "Episode 21/200: Reward = 39.19371684746211\n",
      "Episode 22/200: Reward = 40.39199855596881\n",
      "Episode 23/200: Reward = 41.40508177300148\n",
      "Episode 24/200: Reward = 39.21148988943034\n",
      "Episode 25/200: Reward = 40.51368281256247\n",
      "Episode 26/200: Reward = 39.50824175669641\n",
      "Episode 27/200: Reward = 40.32611043750901\n",
      "Episode 28/200: Reward = 41.098967668765916\n",
      "Episode 29/200: Reward = 39.0340142974627\n",
      "Episode 30/200: Reward = 40.24643937831098\n",
      "Episode 31/200: Reward = 41.213326687030786\n",
      "Episode 32/200: Reward = 39.34412494847664\n",
      "Episode 33/200: Reward = 39.97863623296223\n",
      "Episode 34/200: Reward = 40.36156295386592\n",
      "Episode 35/200: Reward = 38.312484355687786\n",
      "Episode 36/200: Reward = 39.11568899555203\n",
      "Episode 37/200: Reward = 39.4241511554674\n",
      "Episode 38/200: Reward = 38.00846925970637\n",
      "Episode 39/200: Reward = 38.94166388824591\n",
      "Episode 40/200: Reward = 41.551124011979006\n",
      "Episode 41/200: Reward = 40.3044303255397\n",
      "Episode 42/200: Reward = 39.327491639232385\n",
      "Episode 43/200: Reward = 40.531257761565406\n",
      "Episode 44/200: Reward = 40.251726160385786\n",
      "Episode 45/200: Reward = 41.67745512495084\n",
      "Episode 46/200: Reward = 40.20952961010638\n",
      "Episode 47/200: Reward = 40.49435625572145\n",
      "Episode 48/200: Reward = 40.075821596272235\n",
      "Episode 49/200: Reward = 40.416697251509056\n",
      "Episode 50/200: Reward = 41.301551453673596\n",
      "Episode 51/200: Reward = 41.675154852253314\n",
      "Episode 52/200: Reward = 39.188672888902055\n",
      "Episode 53/200: Reward = 39.09331448925859\n",
      "Episode 54/200: Reward = 39.044265199695566\n",
      "Episode 55/200: Reward = 40.15554405049256\n",
      "Episode 56/200: Reward = 39.43932710862746\n",
      "Episode 57/200: Reward = 39.17367096253479\n",
      "Episode 58/200: Reward = 40.06302259580308\n",
      "Episode 59/200: Reward = 39.41551723194722\n",
      "Episode 60/200: Reward = 39.222916349390914\n",
      "Episode 61/200: Reward = 39.24711361525916\n",
      "Episode 62/200: Reward = 40.206395961090145\n",
      "Episode 63/200: Reward = 39.25745808905375\n",
      "Episode 64/200: Reward = 40.153404494441645\n",
      "Episode 65/200: Reward = 40.283913531022364\n",
      "Episode 66/200: Reward = 41.44577417768127\n",
      "Episode 67/200: Reward = 40.176465113928586\n",
      "Episode 68/200: Reward = 39.08526536862498\n",
      "Episode 69/200: Reward = 40.09128306768489\n",
      "Episode 70/200: Reward = 40.01983316454353\n",
      "Episode 71/200: Reward = 40.34164968237275\n",
      "Episode 72/200: Reward = 40.33720911245892\n",
      "Episode 73/200: Reward = 40.156414684909784\n",
      "Episode 74/200: Reward = 40.208559243946105\n",
      "Episode 75/200: Reward = 40.199589688516\n",
      "Episode 76/200: Reward = 39.211389290528324\n",
      "Episode 77/200: Reward = 40.2651918339006\n",
      "Episode 78/200: Reward = 40.449270654648764\n",
      "Episode 79/200: Reward = 40.14843875531358\n",
      "Episode 80/200: Reward = 40.599315922706225\n",
      "Episode 81/200: Reward = 39.214580214847324\n",
      "Episode 82/200: Reward = 40.01730486902611\n",
      "Episode 83/200: Reward = 40.45266740338463\n",
      "Episode 84/200: Reward = 40.15060282217569\n",
      "Episode 85/200: Reward = 39.1973221744485\n",
      "Episode 86/200: Reward = 39.466659145396356\n",
      "Episode 87/200: Reward = 41.17223293795652\n",
      "Episode 88/200: Reward = 40.151114510350304\n",
      "Episode 89/200: Reward = 40.44651769121996\n",
      "Episode 90/200: Reward = 38.44117840545568\n",
      "Episode 91/200: Reward = 39.38799012677609\n",
      "Episode 92/200: Reward = 39.568798033873016\n",
      "Episode 93/200: Reward = 40.251344584131054\n",
      "Episode 94/200: Reward = 39.36916886183075\n",
      "Episode 95/200: Reward = 40.18000656535578\n",
      "Episode 96/200: Reward = 39.05944182386807\n",
      "Episode 97/200: Reward = 39.3078844578043\n",
      "Episode 98/200: Reward = 41.627552780197306\n",
      "Episode 99/200: Reward = 40.032750944027555\n",
      "Episode 100/200: Reward = 40.33793106947302\n",
      "Episode 101/200: Reward = 40.36102634399374\n",
      "Episode 102/200: Reward = 40.15689491470588\n",
      "Episode 103/200: Reward = 38.37411749102981\n",
      "Episode 104/200: Reward = 39.149592428597884\n",
      "Episode 105/200: Reward = 39.135770655800165\n",
      "Episode 106/200: Reward = 39.52719339123735\n",
      "Episode 107/200: Reward = 40.63916083710674\n",
      "Episode 108/200: Reward = 41.53383220226645\n",
      "Episode 109/200: Reward = 41.268306860195196\n",
      "Episode 110/200: Reward = 40.49059395501099\n",
      "Episode 111/200: Reward = 40.79111464639228\n",
      "Episode 112/200: Reward = 39.41064651329418\n",
      "Episode 113/200: Reward = 40.328907199139614\n",
      "Episode 114/200: Reward = 40.251758805773875\n",
      "Episode 115/200: Reward = 39.302044321087656\n",
      "Episode 116/200: Reward = 40.272029619958964\n",
      "Episode 117/200: Reward = 39.210123017737494\n",
      "Episode 118/200: Reward = 40.575127102116966\n",
      "Episode 119/200: Reward = 39.26834657159144\n",
      "Episode 120/200: Reward = 38.53997179342995\n",
      "Episode 121/200: Reward = 38.33665512240685\n",
      "Episode 122/200: Reward = 41.16707364276837\n",
      "Episode 123/200: Reward = 38.470317421801084\n",
      "Episode 124/200: Reward = 40.68309046639324\n",
      "Episode 125/200: Reward = 38.377991907165324\n",
      "Episode 126/200: Reward = 40.49297938241135\n",
      "Episode 127/200: Reward = 40.340819562150536\n",
      "Episode 128/200: Reward = 39.27239919897591\n",
      "Episode 129/200: Reward = 40.49070796797098\n",
      "Episode 130/200: Reward = 39.27912500968481\n",
      "Episode 131/200: Reward = 39.637818130352336\n",
      "Episode 132/200: Reward = 39.28981808779821\n",
      "Episode 133/200: Reward = 39.83537697198822\n",
      "Episode 134/200: Reward = 39.230723308259854\n",
      "Episode 135/200: Reward = 41.00301816766932\n",
      "Episode 136/200: Reward = 39.44642587697127\n",
      "Episode 137/200: Reward = 39.537267495418426\n",
      "Episode 138/200: Reward = 39.98491916018247\n",
      "Episode 139/200: Reward = 40.216502764607576\n",
      "Episode 140/200: Reward = 39.227785932365904\n",
      "Episode 141/200: Reward = 40.601617018376515\n",
      "Episode 142/200: Reward = 40.09334400593996\n",
      "Episode 143/200: Reward = 38.42513363174009\n",
      "Episode 144/200: Reward = 39.27071456135086\n",
      "Episode 145/200: Reward = 40.52875709353859\n",
      "Episode 146/200: Reward = 41.66176954056665\n",
      "Episode 147/200: Reward = 39.029321410424345\n",
      "Episode 148/200: Reward = 40.47664154884993\n",
      "Episode 149/200: Reward = 40.52318035992048\n",
      "Episode 150/200: Reward = 39.24895287036614\n",
      "Episode 151/200: Reward = 40.466492880360235\n",
      "Episode 152/200: Reward = 39.23805390843645\n",
      "Episode 153/200: Reward = 40.3916291610583\n",
      "Episode 154/200: Reward = 40.10904537874451\n",
      "Episode 155/200: Reward = 40.55641589313992\n",
      "Episode 156/200: Reward = 40.218948642904564\n",
      "Episode 157/200: Reward = 40.20601119150663\n",
      "Episode 158/200: Reward = 40.36208670081483\n",
      "Episode 159/200: Reward = 40.662108623964286\n",
      "Episode 160/200: Reward = 40.557843913873285\n",
      "Episode 161/200: Reward = 40.54932118483484\n",
      "Episode 162/200: Reward = 39.6661349189643\n",
      "Episode 163/200: Reward = 39.56547536480691\n",
      "Episode 164/200: Reward = 40.134820627625636\n",
      "Episode 165/200: Reward = 40.093228820092186\n",
      "Episode 166/200: Reward = 40.150417366224175\n",
      "Episode 167/200: Reward = 39.60869668942851\n",
      "Episode 168/200: Reward = 38.502694135860274\n",
      "Episode 169/200: Reward = 40.419031611930144\n",
      "Episode 170/200: Reward = 40.36928223885484\n",
      "Episode 171/200: Reward = 40.047253717739636\n",
      "Episode 172/200: Reward = 39.286074059840324\n",
      "Episode 173/200: Reward = 41.26610033876634\n",
      "Episode 174/200: Reward = 39.387156066913974\n",
      "Episode 175/200: Reward = 39.46731555997726\n",
      "Episode 176/200: Reward = 40.49688678962955\n",
      "Episode 177/200: Reward = 40.42704638427621\n",
      "Episode 178/200: Reward = 39.56131847225995\n",
      "Episode 179/200: Reward = 39.15236592144731\n",
      "Episode 180/200: Reward = 40.16777196308526\n",
      "Episode 181/200: Reward = 40.1224710915795\n",
      "Episode 182/200: Reward = 40.07576737700252\n",
      "Episode 183/200: Reward = 39.54029401049025\n",
      "Episode 184/200: Reward = 40.47653482659498\n",
      "Episode 185/200: Reward = 40.402698452119004\n",
      "Episode 186/200: Reward = 39.40408037293059\n",
      "Episode 187/200: Reward = 39.125610112142496\n",
      "Episode 188/200: Reward = 39.36053254086342\n",
      "Episode 189/200: Reward = 40.29099914382614\n",
      "Episode 190/200: Reward = 39.34340691465575\n",
      "Episode 191/200: Reward = 40.532016411292076\n",
      "Episode 192/200: Reward = 40.74909072307437\n",
      "Episode 193/200: Reward = 41.297121125560935\n",
      "Episode 194/200: Reward = 39.34817596827428\n",
      "Episode 195/200: Reward = 40.30198610624238\n",
      "Episode 196/200: Reward = 40.14959353529734\n",
      "Episode 197/200: Reward = 40.35766282993651\n",
      "Episode 198/200: Reward = 40.54898024494845\n",
      "Episode 199/200: Reward = 40.46038614579197\n",
      "Episode 200/200: Reward = 39.24456205672453\n",
      "Average Reward under Robust Sarsa Critic-based attack: 39.94282452968665\n",
      "Final Average Reward under Robust Sarsa Attack: 39.94282452968665\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:20:47.843034Z",
     "iopub.status.busy": "2024-12-13T22:20:47.842275Z",
     "iopub.status.idle": "2024-12-13T22:20:47.852340Z",
     "shell.execute_reply": "2024-12-13T22:20:47.851412Z",
     "shell.execute_reply.started": "2024-12-13T22:20:47.843003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_state_value_attack(env, policy_net, value_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a State Value Attack using a value network.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        value_net (torch.nn.Module): The trained value network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under the state value attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute value for the perturbed state\n",
    "                value = value_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Minimize or maximize the value\n",
    "                loss = -value.mean()  # Gradient ascent to maximize adversarial effect\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                action_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(action_output, tuple):\n",
    "                    action = action_output[0]  # Extract mean for continuous actions\n",
    "                else:\n",
    "                    action = action_output\n",
    "\n",
    "                action = action.squeeze().cpu().numpy()  # Ensure the action is in NumPy format\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under State Value Attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:20:51.734095Z",
     "iopub.status.busy": "2024-12-13T22:20:51.733424Z",
     "iopub.status.idle": "2024-12-13T22:20:51.744831Z",
     "shell.execute_reply": "2024-12-13T22:20:51.743958Z",
     "shell.execute_reply.started": "2024-12-13T22:20:51.734062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_target_policy_attack(env, policy_net, target_action, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Target Policy Misclassification attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        target_action (torch.Tensor): The target action to force the policy to output.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Target Policy Misclassification attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Get policy output for the perturbed state\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    logits = policy_output  # For discrete actions\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, target_action)  # Cross-entropy loss\n",
    "                elif isinstance(env.action_space, gym.spaces.Box):\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Mean and std\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "                # Backpropagate to compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action = torch.argmax(policy_output, dim=1).item()  # Discrete action\n",
    "                else:\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()  # Continuous action\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Target Policy Misclassification attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:39:21.712024Z",
     "iopub.status.busy": "2024-12-13T22:39:21.711082Z",
     "iopub.status.idle": "2024-12-13T22:45:13.055552Z",
     "shell.execute_reply": "2024-12-13T22:45:13.054678Z",
     "shell.execute_reply.started": "2024-12-13T22:39:21.711976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 469.713006808104\n",
      "Episode 2/200: Reward = 450.1773501140388\n",
      "Episode 3/200: Reward = 447.3998494743558\n",
      "Episode 4/200: Reward = 464.7211645670482\n",
      "Episode 5/200: Reward = 468.90548264245206\n",
      "Episode 6/200: Reward = 445.68115793895026\n",
      "Episode 7/200: Reward = 471.1114744796566\n",
      "Episode 8/200: Reward = 450.76397479402056\n",
      "Episode 9/200: Reward = 444.75359193595796\n",
      "Episode 10/200: Reward = 464.94329993965795\n",
      "Episode 11/200: Reward = 470.65932766090305\n",
      "Episode 12/200: Reward = 460.36669650262223\n",
      "Episode 13/200: Reward = 451.8103824249519\n",
      "Episode 14/200: Reward = 467.08236077781004\n",
      "Episode 15/200: Reward = 460.4215329116474\n",
      "Episode 16/200: Reward = 450.3740483649145\n",
      "Episode 17/200: Reward = 481.4569747733835\n",
      "Episode 18/200: Reward = 485.57974510159295\n",
      "Episode 19/200: Reward = 446.9427707905878\n",
      "Episode 20/200: Reward = 448.4411558476614\n",
      "Episode 21/200: Reward = 446.3187168308081\n",
      "Episode 22/200: Reward = 447.08106940191595\n",
      "Episode 23/200: Reward = 444.88863222490744\n",
      "Episode 24/200: Reward = 453.19788559871574\n",
      "Episode 25/200: Reward = 454.62831422430145\n",
      "Episode 26/200: Reward = 443.7774594827742\n",
      "Episode 27/200: Reward = 470.3811080838789\n",
      "Episode 28/200: Reward = 442.74841931584643\n",
      "Episode 29/200: Reward = 445.2724328459415\n",
      "Episode 30/200: Reward = 461.7227188397587\n",
      "Episode 31/200: Reward = 447.2463261009962\n",
      "Episode 32/200: Reward = 464.1975552657243\n",
      "Episode 33/200: Reward = 465.41558689475744\n",
      "Episode 34/200: Reward = 446.9973573883092\n",
      "Episode 35/200: Reward = 469.1812132823668\n",
      "Episode 36/200: Reward = 447.6569957514297\n",
      "Episode 37/200: Reward = 449.63004969603\n",
      "Episode 38/200: Reward = 447.3163902792778\n",
      "Episode 39/200: Reward = 443.3406426588726\n",
      "Episode 40/200: Reward = 452.80261954447644\n",
      "Episode 41/200: Reward = 482.8053637192877\n",
      "Episode 42/200: Reward = 458.6228385057336\n",
      "Episode 43/200: Reward = 462.06394752548056\n",
      "Episode 44/200: Reward = 443.27266828947955\n",
      "Episode 45/200: Reward = 465.9991456711849\n",
      "Episode 46/200: Reward = 454.3570124042855\n",
      "Episode 47/200: Reward = 501.2641527061883\n",
      "Episode 48/200: Reward = 439.2229997400701\n",
      "Episode 49/200: Reward = 451.37174456919615\n",
      "Episode 50/200: Reward = 451.1093252708756\n",
      "Episode 51/200: Reward = 465.1635263884084\n",
      "Episode 52/200: Reward = 462.44372792721134\n",
      "Episode 53/200: Reward = 449.3999338472556\n",
      "Episode 54/200: Reward = 473.00910468395875\n",
      "Episode 55/200: Reward = 445.4924383100079\n",
      "Episode 56/200: Reward = 444.2768516386925\n",
      "Episode 57/200: Reward = 468.23097212227617\n",
      "Episode 58/200: Reward = 444.00891895337827\n",
      "Episode 59/200: Reward = 451.1193065417628\n",
      "Episode 60/200: Reward = 449.88271160333437\n",
      "Episode 61/200: Reward = 467.41535023859785\n",
      "Episode 62/200: Reward = 446.2398641510107\n",
      "Episode 63/200: Reward = 453.03794550821596\n",
      "Episode 64/200: Reward = 444.5881561408977\n",
      "Episode 65/200: Reward = 448.99007149361375\n",
      "Episode 66/200: Reward = 465.10964826406587\n",
      "Episode 67/200: Reward = 477.7598928867208\n",
      "Episode 68/200: Reward = 459.4857926845312\n",
      "Episode 69/200: Reward = 464.3186795923735\n",
      "Episode 70/200: Reward = 442.4737127472285\n",
      "Episode 71/200: Reward = 448.89852453881014\n",
      "Episode 72/200: Reward = 458.1332074522381\n",
      "Episode 73/200: Reward = 460.7824797937039\n",
      "Episode 74/200: Reward = 462.1317315922786\n",
      "Episode 75/200: Reward = 462.7690415380002\n",
      "Episode 76/200: Reward = 449.3974751020721\n",
      "Episode 77/200: Reward = 447.1357601784084\n",
      "Episode 78/200: Reward = 447.86194586206454\n",
      "Episode 79/200: Reward = 468.3938759990426\n",
      "Episode 80/200: Reward = 456.004188624999\n",
      "Episode 81/200: Reward = 475.4736644101958\n",
      "Episode 82/200: Reward = 469.8376230256768\n",
      "Episode 83/200: Reward = 449.04812723597547\n",
      "Episode 84/200: Reward = 445.8439281556827\n",
      "Episode 85/200: Reward = 463.39770360207433\n",
      "Episode 86/200: Reward = 468.81298308277616\n",
      "Episode 87/200: Reward = 458.6170506207187\n",
      "Episode 88/200: Reward = 445.753448687712\n",
      "Episode 89/200: Reward = 449.3539377470097\n",
      "Episode 90/200: Reward = 450.31300521314785\n",
      "Episode 91/200: Reward = 466.5308705609906\n",
      "Episode 92/200: Reward = 466.0006118194735\n",
      "Episode 93/200: Reward = 479.39580211039424\n",
      "Episode 94/200: Reward = 449.55871122310947\n",
      "Episode 95/200: Reward = 449.7081421056826\n",
      "Episode 96/200: Reward = 467.6890347312637\n",
      "Episode 97/200: Reward = 447.4700323536977\n",
      "Episode 98/200: Reward = 438.5796061580864\n",
      "Episode 99/200: Reward = 469.42782274862503\n",
      "Episode 100/200: Reward = 445.291884076148\n",
      "Episode 101/200: Reward = 464.66321872108966\n",
      "Episode 102/200: Reward = 450.1889519328338\n",
      "Episode 103/200: Reward = 447.3820063645452\n",
      "Episode 104/200: Reward = 448.43450860834724\n",
      "Episode 105/200: Reward = 464.0722218023524\n",
      "Episode 106/200: Reward = 443.19604383053866\n",
      "Episode 107/200: Reward = 451.1272892796157\n",
      "Episode 108/200: Reward = 441.8281743585594\n",
      "Episode 109/200: Reward = 446.8150012615429\n",
      "Episode 110/200: Reward = 450.4682662162143\n",
      "Episode 111/200: Reward = 468.2380157602673\n",
      "Episode 112/200: Reward = 457.4188522609188\n",
      "Episode 113/200: Reward = 441.42962408329635\n",
      "Episode 114/200: Reward = 448.3994510827366\n",
      "Episode 115/200: Reward = 466.5442424451855\n",
      "Episode 116/200: Reward = 474.58324410227175\n",
      "Episode 117/200: Reward = 445.1992806778353\n",
      "Episode 118/200: Reward = 455.1472587403661\n",
      "Episode 119/200: Reward = 449.2659865028449\n",
      "Episode 120/200: Reward = 450.24605183020816\n",
      "Episode 121/200: Reward = 468.26464177857156\n",
      "Episode 122/200: Reward = 466.72881742811853\n",
      "Episode 123/200: Reward = 466.5409814203157\n",
      "Episode 124/200: Reward = 461.99289562818353\n",
      "Episode 125/200: Reward = 453.6428385074104\n",
      "Episode 126/200: Reward = 446.9688323450234\n",
      "Episode 127/200: Reward = 444.0098493374213\n",
      "Episode 128/200: Reward = 462.9912420809936\n",
      "Episode 129/200: Reward = 464.97261009612845\n",
      "Episode 130/200: Reward = 459.98181463542625\n",
      "Episode 131/200: Reward = 464.719706124138\n",
      "Episode 132/200: Reward = 461.32352471519323\n",
      "Episode 133/200: Reward = 461.3980794672705\n",
      "Episode 134/200: Reward = 476.6266959079021\n",
      "Episode 135/200: Reward = 469.06874422304645\n",
      "Episode 136/200: Reward = 445.7387202995739\n",
      "Episode 137/200: Reward = 444.22071594386733\n",
      "Episode 138/200: Reward = 468.7483414477934\n",
      "Episode 139/200: Reward = 453.5977883959074\n",
      "Episode 140/200: Reward = 459.75725652219637\n",
      "Episode 141/200: Reward = 464.9888746398269\n",
      "Episode 142/200: Reward = 473.33223930707607\n",
      "Episode 143/200: Reward = 447.7654631066604\n",
      "Episode 144/200: Reward = 446.91384908472315\n",
      "Episode 145/200: Reward = 440.1377478210263\n",
      "Episode 146/200: Reward = 469.249266130415\n",
      "Episode 147/200: Reward = 467.1048797183214\n",
      "Episode 148/200: Reward = 449.8850826655364\n",
      "Episode 149/200: Reward = 454.63414312276194\n",
      "Episode 150/200: Reward = 467.02911736898966\n",
      "Episode 151/200: Reward = 449.14301387169405\n",
      "Episode 152/200: Reward = 468.001235030536\n",
      "Episode 153/200: Reward = 445.9600224243362\n",
      "Episode 154/200: Reward = 447.7480238914308\n",
      "Episode 155/200: Reward = 446.24082990777634\n",
      "Episode 156/200: Reward = 450.4518601636549\n",
      "Episode 157/200: Reward = 462.4825653507346\n",
      "Episode 158/200: Reward = 464.75871421715317\n",
      "Episode 159/200: Reward = 476.2501685389654\n",
      "Episode 160/200: Reward = 471.29208212413016\n",
      "Episode 161/200: Reward = 458.1080339581935\n",
      "Episode 162/200: Reward = 463.4713042342195\n",
      "Episode 163/200: Reward = 468.16284829006185\n",
      "Episode 164/200: Reward = 447.98176272411206\n",
      "Episode 165/200: Reward = 465.1170562328344\n",
      "Episode 166/200: Reward = 442.31828162472823\n",
      "Episode 167/200: Reward = 450.3487990033118\n",
      "Episode 168/200: Reward = 482.6960189964799\n",
      "Episode 169/200: Reward = 448.36657517912124\n",
      "Episode 170/200: Reward = 467.5739732544989\n",
      "Episode 171/200: Reward = 450.6520459210863\n",
      "Episode 172/200: Reward = 447.028638209406\n",
      "Episode 173/200: Reward = 466.59980542402195\n",
      "Episode 174/200: Reward = 450.2766557841418\n",
      "Episode 175/200: Reward = 468.0499131373302\n",
      "Episode 176/200: Reward = 445.8306318738934\n",
      "Episode 177/200: Reward = 443.8440871185583\n",
      "Episode 178/200: Reward = 465.41391176330944\n",
      "Episode 179/200: Reward = 457.06353936293124\n",
      "Episode 180/200: Reward = 477.1513526837275\n",
      "Episode 181/200: Reward = 453.17396709077235\n",
      "Episode 182/200: Reward = 453.09222537627744\n",
      "Episode 183/200: Reward = 445.2589124188201\n",
      "Episode 184/200: Reward = 444.5785672109746\n",
      "Episode 185/200: Reward = 450.25528474413096\n",
      "Episode 186/200: Reward = 477.4609702597671\n",
      "Episode 187/200: Reward = 466.1434534049848\n",
      "Episode 188/200: Reward = 467.3367632457232\n",
      "Episode 189/200: Reward = 447.12221083023496\n",
      "Episode 190/200: Reward = 451.5749727787846\n",
      "Episode 191/200: Reward = 447.5422268556419\n",
      "Episode 192/200: Reward = 447.0852020198363\n",
      "Episode 193/200: Reward = 447.3232049733938\n",
      "Episode 194/200: Reward = 447.8302717898267\n",
      "Episode 195/200: Reward = 438.35099999826724\n",
      "Episode 196/200: Reward = 448.9142875646263\n",
      "Episode 197/200: Reward = 450.1031446012211\n",
      "Episode 198/200: Reward = 448.49292545225154\n",
      "Episode 199/200: Reward = 447.6395838525454\n",
      "Episode 200/200: Reward = 469.1718517793745\n",
      "Average Reward under State Value Attack: 456.6953604585602\n",
      "Average Reward under State Action Value Attack: 456.6953604585602\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    value_net=VanillaAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T23:08:59.917538Z",
     "iopub.status.busy": "2024-12-13T23:08:59.917210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1613303488.py:43: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 371.1159979297294\n",
      "Episode 2/200: Reward = 375.365863770768\n",
      "Episode 3/200: Reward = 365.3831792414708\n",
      "Episode 4/200: Reward = 373.83939941524926\n",
      "Episode 5/200: Reward = 647.1185168745059\n",
      "Episode 6/200: Reward = 379.8827923023128\n",
      "Episode 7/200: Reward = 381.35557947350514\n",
      "Episode 8/200: Reward = 671.4575984465118\n",
      "Episode 9/200: Reward = 369.69331667426883\n",
      "Episode 10/200: Reward = 379.8789309746432\n",
      "Episode 11/200: Reward = 398.69874754430947\n",
      "Episode 12/200: Reward = 680.5502724012724\n",
      "Episode 13/200: Reward = 378.9238369379206\n",
      "Episode 14/200: Reward = 463.5279058967761\n",
      "Episode 15/200: Reward = 368.8632675570314\n",
      "Episode 16/200: Reward = 376.3450255731837\n",
      "Episode 17/200: Reward = 640.7411200734948\n",
      "Episode 18/200: Reward = 377.3150936119303\n",
      "Episode 19/200: Reward = 380.18713679055617\n",
      "Episode 20/200: Reward = 404.50505694818236\n",
      "Episode 21/200: Reward = 393.62639825702695\n",
      "Episode 22/200: Reward = 666.8572636552302\n",
      "Episode 23/200: Reward = 556.7632383364789\n",
      "Episode 24/200: Reward = 965.0483509308033\n",
      "Episode 25/200: Reward = 375.54063320110447\n",
      "Episode 26/200: Reward = 453.359459517602\n",
      "Episode 27/200: Reward = 593.7984275896791\n",
      "Episode 28/200: Reward = 371.54166215362307\n",
      "Episode 29/200: Reward = 366.5478613953631\n",
      "Episode 30/200: Reward = 646.3254048115846\n",
      "Episode 31/200: Reward = 819.2428305336448\n",
      "Episode 32/200: Reward = 455.5248933385405\n",
      "Episode 33/200: Reward = 377.24095283111745\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
