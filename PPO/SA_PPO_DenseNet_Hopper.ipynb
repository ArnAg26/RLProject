{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:29.039924Z",
     "iopub.status.busy": "2024-12-05T10:17:29.039557Z",
     "iopub.status.idle": "2024-12-05T10:17:30.058449Z",
     "shell.execute_reply": "2024-12-05T10:17:30.057554Z",
     "shell.execute_reply.started": "2024-12-05T10:17:29.039883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:29:45.050475Z",
     "iopub.status.busy": "2024-12-13T20:29:45.050221Z",
     "iopub.status.idle": "2024-12-13T20:29:57.104897Z",
     "shell.execute_reply": "2024-12-13T20:29:57.103830Z",
     "shell.execute_reply.started": "2024-12-13T20:29:45.050448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:31:34.808125Z",
     "iopub.status.busy": "2024-12-13T20:31:34.807807Z",
     "iopub.status.idle": "2024-12-13T20:31:38.627461Z",
     "shell.execute_reply": "2024-12-13T20:31:38.626800Z",
     "shell.execute_reply.started": "2024-12-13T20:31:34.808095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:02.758553Z",
     "iopub.status.busy": "2024-12-13T20:32:02.757781Z",
     "iopub.status.idle": "2024-12-13T20:32:02.767686Z",
     "shell.execute_reply": "2024-12-13T20:32:02.766074Z",
     "shell.execute_reply.started": "2024-12-13T20:32:02.758522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:12.712285Z",
     "iopub.status.busy": "2024-12-13T20:32:12.711500Z",
     "iopub.status.idle": "2024-12-13T20:32:12.720295Z",
     "shell.execute_reply": "2024-12-13T20:32:12.719499Z",
     "shell.execute_reply.started": "2024-12-13T20:32:12.712252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T10:00:16.822773Z",
     "iopub.status.busy": "2024-12-13T10:00:16.822060Z",
     "iopub.status.idle": "2024-12-13T10:00:16.826520Z",
     "shell.execute_reply": "2024-12-13T10:00:16.825633Z",
     "shell.execute_reply.started": "2024-12-13T10:00:16.822738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:30.917581Z",
     "iopub.status.busy": "2024-12-13T20:32:30.916832Z",
     "iopub.status.idle": "2024-12-13T20:32:30.927764Z",
     "shell.execute_reply": "2024-12-13T20:32:30.926729Z",
     "shell.execute_reply.started": "2024-12-13T20:32:30.917549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:37.433291Z",
     "iopub.status.busy": "2024-12-13T20:32:37.432506Z",
     "iopub.status.idle": "2024-12-13T20:32:37.449963Z",
     "shell.execute_reply": "2024-12-13T20:32:37.449087Z",
     "shell.execute_reply.started": "2024-12-13T20:32:37.433258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:32:44.214611Z",
     "iopub.status.busy": "2024-12-13T20:32:44.213911Z",
     "iopub.status.idle": "2024-12-13T20:32:44.226927Z",
     "shell.execute_reply": "2024-12-13T20:32:44.225957Z",
     "shell.execute_reply.started": "2024-12-13T20:32:44.214578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T18:10:25.437027Z",
     "iopub.status.busy": "2024-12-13T18:10:25.436672Z",
     "iopub.status.idle": "2024-12-13T18:10:25.466133Z",
     "shell.execute_reply": "2024-12-13T18:10:25.465385Z",
     "shell.execute_reply.started": "2024-12-13T18:10:25.436999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Import sophisticated networks\n",
    "class ValueDenseNet(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(ValueDenseNet, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "        nn.init.orthogonal_(self.output_layer.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "class CtsPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=(128, 128), activation=nn.Tanh):\n",
    "        super(CtsPolicy, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layer = nn.Linear(input_size, hidden_size)\n",
    "            nn.init.orthogonal_(layer.weight, gain=nn.init.calculate_gain(\"tanh\"))\n",
    "            self.layers.append(layer)\n",
    "            input_size = hidden_size\n",
    "\n",
    "        self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "        nn.init.orthogonal_(self.mean_layer.weight, gain=0.01)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SAPPOAgent:\n",
    "#     def __init__(self, state_dim, action_dim, discrete=True, lr=3e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#         # Actor and critic networks\n",
    "#         self.policy_net = CtsPolicy(state_dim, action_dim, hidden_sizes=(128, 128), activation=nn.Tanh).to(self.device)\n",
    "#         self.value_net = ValueDenseNet(state_dim, hidden_sizes=(128, 128), activation=nn.Tanh).to(self.device)\n",
    "\n",
    "#         self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "#         self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "#         self.gamma = gamma\n",
    "#         self.lam = lam\n",
    "#         self.eps_clip = eps_clip\n",
    "#         self.k_epochs = k_epochs\n",
    "\n",
    "#         self.sgld_steps = sgld_steps\n",
    "#         self.sgld_lr = sgld_lr\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         state = torch.tensor(state, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             mean, std = self.policy_net(state)\n",
    "#             dist = torch.distributions.Normal(mean, std)\n",
    "#             action = dist.sample()\n",
    "#             return action.cpu().numpy().squeeze(), dist.log_prob(action).sum()\n",
    "\n",
    "#     def compute_gae(self, rewards, values, dones):\n",
    "#         advantages = []\n",
    "#         advantage = 0\n",
    "#         for t in reversed(range(len(rewards))):\n",
    "#             delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "#             advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "#             advantages.insert(0, advantage)\n",
    "#         return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "#     def sgld_step(self, state, epsilon):\n",
    "#         \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "#         perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "    \n",
    "#         for _ in range(self.sgld_steps):\n",
    "#             if perturbed_state.grad is not None:\n",
    "#                 perturbed_state.grad.zero_()\n",
    "    \n",
    "#             # Compute KL divergence between original and perturbed policies\n",
    "#             with torch.no_grad():\n",
    "#                 original_logits = self.policy_net(state)\n",
    "#             perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "#             if self.policy_net.discrete:\n",
    "#                 original_policy = dist.Categorical(original_logits)\n",
    "#                 perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "#             else:\n",
    "#                 original_mean, original_std = original_logits\n",
    "#                 perturbed_mean, perturbed_std = perturbed_logits\n",
    "#                 original_policy = dist.Normal(original_mean, original_std)\n",
    "#                 perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "#             kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "    \n",
    "#             # Backpropagate KL divergence\n",
    "#             kl_div.backward()\n",
    "    \n",
    "#             # Update perturbed state using gradient and noise\n",
    "#             perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "#             perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "    \n",
    "#         return perturbed_state.detach()\n",
    "\n",
    "#     def compute_kl_regularization(self, states, actions):\n",
    "#         \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "#         if len(states) == 0:\n",
    "#             return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "#         kl_div_total = 0\n",
    "#         for state in states:\n",
    "#             perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "    \n",
    "#             with torch.no_grad():\n",
    "#                 original_logits = self.policy_net(state)\n",
    "#             perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "#             if self.policy_net.discrete:\n",
    "#                 original_policy = dist.Categorical(original_logits)\n",
    "#                 perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "#             else:\n",
    "#                 original_mean, original_std = original_logits\n",
    "#                 perturbed_mean, perturbed_std = perturbed_logits\n",
    "#                 original_policy = dist.Normal(original_mean, original_std)\n",
    "#                 perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "#             kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "#             kl_div_total += kl_div\n",
    "    \n",
    "#         return kl_div_total / len(states)\n",
    "#     def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "#         for episode in range(max_episodes):\n",
    "#             states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    \n",
    "#             # Reset the environment\n",
    "#             state, _ = env.reset()\n",
    "#             state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "#             # Rollout phase: Collect trajectories\n",
    "#             for _ in range(rollout_steps):\n",
    "#                 value = self.value_net(state).squeeze(0).detach()  # Detach the value tensor\n",
    "#                 action, log_prob = self.select_action(state.cpu().numpy())\n",
    "    \n",
    "#                 next_state, reward, done, truncated, _ = env.step(action)\n",
    "    \n",
    "#                 # Append data to lists\n",
    "#                 states.append(state.clone().detach())\n",
    "#                 actions.append(action)\n",
    "#                 rewards.append(reward)\n",
    "#                 dones.append(done or truncated)\n",
    "#                 log_probs.append(log_prob.clone().detach())\n",
    "#                 values.append(value)\n",
    "    \n",
    "#                 # Update state\n",
    "#                 state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "#                 if done or truncated:\n",
    "#                     state, _ = env.reset()\n",
    "#                     state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "#             # Add a final value estimate\n",
    "#             values.append(torch.tensor([0], device=self.device).detach())\n",
    "    \n",
    "#             # Compute advantages and returns\n",
    "#             advantages = self.compute_gae(rewards, values, dones)\n",
    "#             returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "#             # Convert lists to tensors\n",
    "#             states = torch.stack(states).to(self.device)\n",
    "#             actions = torch.tensor(\n",
    "#                 np.array(actions),\n",
    "#                 dtype=torch.float32  # Always float32 for continuous actions\n",
    "#             ).to(self.device)\n",
    "#             log_probs = torch.stack(log_probs).to(self.device)\n",
    "    \n",
    "#             # Optimization phase\n",
    "#             for _ in range(self.k_epochs):\n",
    "#                 kl_reg = self.compute_kl_regularization(states, actions)\n",
    "    \n",
    "#                 for i in range(0, rollout_steps, batch_size):\n",
    "#                     batch_states = states[i:i + batch_size]\n",
    "#                     batch_actions = actions[i:i + batch_size]\n",
    "#                     batch_log_probs = log_probs[i:i + batch_size]\n",
    "#                     batch_advantages = advantages[i:i + batch_size]\n",
    "#                     batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "#                     mean, std = self.policy_net(batch_states)\n",
    "#                     dist = torch.distributions.Normal(mean, std)\n",
    "#                     new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "#                     ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "#                     surr1 = ratio * batch_advantages\n",
    "#                     surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "#                     policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "#                     value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "#                     value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "    \n",
    "#                     # Detach kl_reg to prevent graph accumulation\n",
    "#                     kl_reg = kl_reg.detach()\n",
    "\n",
    "#                     total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg\n",
    "    \n",
    "#                     self.policy_optimizer.zero_grad()\n",
    "#                     self.value_optimizer.zero_grad()\n",
    "#                     total_loss.backward(retain_graph=False)  # No need to retain the graph here\n",
    "#                     self.policy_optimizer.step()\n",
    "#                     self.value_optimizer.step()\n",
    "    \n",
    "#             print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "     \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:33:01.312865Z",
     "iopub.status.busy": "2024-12-13T20:33:01.312531Z",
     "iopub.status.idle": "2024-12-13T20:33:01.332905Z",
     "shell.execute_reply": "2024-12-13T20:33:01.332042Z",
     "shell.execute_reply.started": "2024-12-13T20:33:01.312838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Actor and critic networks\n",
    "        self.policy_net = CtsPolicy(state_dim, action_dim).to(self.device)\n",
    "        self.value_net = ValueDenseNet(state_dim).to(self.device)\n",
    "\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        self.sgld_steps = sgld_steps\n",
    "        self.sgld_lr = sgld_lr\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        mean, std = self.policy_net(state)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return action.cpu().numpy(), dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def sgld_step(self, state, epsilon):\n",
    "        \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "        perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "\n",
    "        for _ in range(self.sgld_steps):\n",
    "            if perturbed_state.grad is not None:\n",
    "                perturbed_state.grad.zero_()\n",
    "\n",
    "            # Compute KL divergence between original and perturbed policies\n",
    "            with torch.no_grad():\n",
    "                original_mean, original_std = self.policy_net(state)\n",
    "            perturbed_mean, perturbed_std = self.policy_net(perturbed_state)\n",
    "\n",
    "            original_policy = dist.Normal(original_mean, original_std)\n",
    "            perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "\n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "\n",
    "            # Backpropagate KL divergence\n",
    "            kl_div.backward()\n",
    "\n",
    "            # Update perturbed state using gradient and noise\n",
    "            perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "            perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    def compute_kl_regularization(self, states, actions):\n",
    "        \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "        if len(states) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        kl_div_total = 0\n",
    "        for state in states:\n",
    "            perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                original_mean, original_std = self.policy_net(state)\n",
    "            perturbed_mean, perturbed_std = self.policy_net(perturbed_state)\n",
    "\n",
    "            original_policy = dist.Normal(original_mean, original_std)\n",
    "            perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "\n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "            kl_div_total += kl_div\n",
    "\n",
    "        return kl_div_total / len(states)\n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "\n",
    "            # Reset the environment\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Rollout phase: Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                value = self.value_net(state).squeeze(0).detach()\n",
    "                action, log_prob = self.select_action(state.cpu().numpy())\n",
    "\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                states.append(state.clone().detach())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob.clone().detach())\n",
    "                values.append(value)\n",
    "\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Add a final value estimate\n",
    "            values.append(torch.tensor([0], device=self.device).detach())\n",
    "\n",
    "            # Compute advantages and returns\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "            # Convert lists to tensors\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "\n",
    "            # Optimization phase\n",
    "            for _ in range(self.k_epochs):\n",
    "                kl_reg = self.compute_kl_regularization(states, actions)\n",
    "\n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i + batch_size]\n",
    "                    batch_actions = actions[i:i + batch_size]\n",
    "                    batch_log_probs = log_probs[i:i + batch_size]\n",
    "                    batch_advantages = advantages[i:i + batch_size]\n",
    "                    batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "                    mean, std = self.policy_net(batch_states)\n",
    "                    dist = torch.distributions.Normal(mean, std)\n",
    "                    new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "\n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "\n",
    "                    total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg.detach()\n",
    "\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "\n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T20:33:30.086476Z",
     "iopub.status.busy": "2024-12-13T20:33:30.085649Z",
     "iopub.status.idle": "2024-12-13T21:25:34.456941Z",
     "shell.execute_reply": "2024-12-13T21:25:34.456034Z",
     "shell.execute_reply.started": "2024-12-13T20:33:30.086442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -3.2936959266662598, Value Loss = 16.149124145507812, KL Reg = 5.190177176928046e-09\n",
      "Episode 2: Policy Loss = -8.31801986694336, Value Loss = 89.69292449951172, KL Reg = 5.093167665393139e-09\n",
      "Episode 3: Policy Loss = -7.369718551635742, Value Loss = 75.51033020019531, KL Reg = 4.908844442041982e-09\n",
      "Episode 4: Policy Loss = -4.391140937805176, Value Loss = 30.305814743041992, KL Reg = 5.617031728633037e-09\n",
      "Episode 5: Policy Loss = -6.125728607177734, Value Loss = 61.661521911621094, KL Reg = 5.258087742987527e-09\n",
      "Episode 6: Policy Loss = -3.5035226345062256, Value Loss = 21.29940414428711, KL Reg = 5.403604230735937e-09\n",
      "Episode 7: Policy Loss = -5.351285457611084, Value Loss = 35.21036911010742, KL Reg = 5.995378860035316e-09\n",
      "Episode 8: Policy Loss = -8.764406204223633, Value Loss = 108.15007781982422, KL Reg = 5.733447494549182e-09\n",
      "Episode 9: Policy Loss = -3.085789442062378, Value Loss = 20.388507843017578, KL Reg = 6.451332801304943e-09\n",
      "Episode 10: Policy Loss = -4.299745559692383, Value Loss = 26.281293869018555, KL Reg = 5.81105386032732e-09\n",
      "Episode 11: Policy Loss = -4.671839237213135, Value Loss = 29.836835861206055, KL Reg = 5.471511244081739e-09\n",
      "Episode 12: Policy Loss = -5.500026702880859, Value Loss = 40.940818786621094, KL Reg = 6.014782449881295e-09\n",
      "Episode 13: Policy Loss = -6.769908905029297, Value Loss = 56.955989837646484, KL Reg = 5.481215037406173e-09\n",
      "Episode 14: Policy Loss = -4.875415802001953, Value Loss = 31.442584991455078, KL Reg = 5.054358709344342e-09\n",
      "Episode 15: Policy Loss = -6.8178229331970215, Value Loss = 62.05345916748047, KL Reg = 5.617032616811457e-09\n",
      "Episode 16: Policy Loss = -4.13370418548584, Value Loss = 24.26702308654785, KL Reg = 5.364801047846868e-09\n",
      "Episode 17: Policy Loss = -4.6641082763671875, Value Loss = 29.337697982788086, KL Reg = 5.9177698297219195e-09\n",
      "Episode 18: Policy Loss = -5.132699966430664, Value Loss = 37.11642837524414, KL Reg = 5.849862816376117e-09\n",
      "Episode 19: Policy Loss = -5.060760498046875, Value Loss = 38.175315856933594, KL Reg = 5.18047915676334e-09\n",
      "Episode 20: Policy Loss = -3.8164849281311035, Value Loss = 21.906307220458984, KL Reg = 6.0729843376350345e-09\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    RobustAgent = SAPPOAgent(state_dim, action_dim, discrete)\n",
    "    RobustAgent.train(env, max_episodes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:58:59.470006Z",
     "iopub.status.busy": "2024-12-13T21:58:59.469289Z",
     "iopub.status.idle": "2024-12-13T21:59:02.086684Z",
     "shell.execute_reply": "2024-12-13T21:59:02.085725Z",
     "shell.execute_reply.started": "2024-12-13T21:58:59.469972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 8.305492123964871\n",
      "Episode 2: Reward = 25.747382310612124\n",
      "Episode 3: Reward = 3.2708648496659745\n",
      "Episode 4: Reward = 6.6823732225626244\n",
      "Episode 5: Reward = 10.078487054843045\n",
      "Episode 6: Reward = 9.429944475378923\n",
      "Episode 7: Reward = 9.619057926258394\n",
      "Episode 8: Reward = 9.45160716691598\n",
      "Episode 9: Reward = 14.935010016757237\n",
      "Episode 10: Reward = 6.438583452859067\n",
      "Episode 11: Reward = 6.774514376270599\n",
      "Episode 12: Reward = 8.481237161450133\n",
      "Episode 13: Reward = 12.686266890047545\n",
      "Episode 14: Reward = 6.1580337376665515\n",
      "Episode 15: Reward = 7.811738139567256\n",
      "Episode 16: Reward = 6.082923122193883\n",
      "Episode 17: Reward = 21.28172084748687\n",
      "Episode 18: Reward = 8.16732417631659\n",
      "Episode 19: Reward = 5.917934371751272\n",
      "Episode 20: Reward = 9.265311478868544\n",
      "Episode 21: Reward = 6.061407251017224\n",
      "Episode 22: Reward = 19.45414598822266\n",
      "Episode 23: Reward = 9.029031308873432\n",
      "Episode 24: Reward = 18.00142902981201\n",
      "Episode 25: Reward = 15.822545588110108\n",
      "Episode 26: Reward = 36.52550376445238\n",
      "Episode 27: Reward = 14.397145684281778\n",
      "Episode 28: Reward = 7.888907339842615\n",
      "Episode 29: Reward = 5.163227974919941\n",
      "Episode 30: Reward = 17.787624272026306\n",
      "Episode 31: Reward = 18.135466380777185\n",
      "Episode 32: Reward = 24.970184878746817\n",
      "Episode 33: Reward = 7.52559545664795\n",
      "Episode 34: Reward = 6.4645824913348875\n",
      "Episode 35: Reward = 15.862298831833353\n",
      "Episode 36: Reward = 12.868714562812135\n",
      "Episode 37: Reward = 16.842013729799103\n",
      "Episode 38: Reward = 46.132683142270224\n",
      "Episode 39: Reward = 10.21075457817568\n",
      "Episode 40: Reward = 35.127404543744355\n",
      "Episode 41: Reward = 63.11262926427967\n",
      "Episode 42: Reward = 16.49565721245691\n",
      "Episode 43: Reward = 10.620361993738998\n",
      "Episode 44: Reward = 43.11942529250384\n",
      "Episode 45: Reward = 12.86680729879124\n",
      "Episode 46: Reward = 6.9610585881324445\n",
      "Episode 47: Reward = 9.333976783153457\n",
      "Episode 48: Reward = 7.9702124740659945\n",
      "Episode 49: Reward = 7.94514962718813\n",
      "Episode 50: Reward = 8.49680781452098\n",
      "Episode 51: Reward = 8.293772218230515\n",
      "Episode 52: Reward = 7.044349468746992\n",
      "Episode 53: Reward = 10.50590585597825\n",
      "Episode 54: Reward = 14.675928876309305\n",
      "Episode 55: Reward = 25.616554556664877\n",
      "Episode 56: Reward = 5.463496553554577\n",
      "Episode 57: Reward = 7.5895980300715475\n",
      "Episode 58: Reward = 9.188998474005448\n",
      "Episode 59: Reward = 8.236467837803698\n",
      "Episode 60: Reward = 10.72798227495771\n",
      "Episode 61: Reward = 6.24730718653234\n",
      "Episode 62: Reward = 20.106442429000033\n",
      "Episode 63: Reward = 10.712602833503158\n",
      "Episode 64: Reward = 7.258798332972573\n",
      "Episode 65: Reward = 9.533296114002065\n",
      "Episode 66: Reward = 11.736311938410456\n",
      "Episode 67: Reward = 9.24950653713322\n",
      "Episode 68: Reward = 8.742750398195462\n",
      "Episode 69: Reward = 4.965315881805234\n",
      "Episode 70: Reward = 12.14780377357155\n",
      "Episode 71: Reward = 8.405252399885876\n",
      "Episode 72: Reward = 17.767398669522663\n",
      "Episode 73: Reward = 13.602850815213397\n",
      "Episode 74: Reward = 16.680568969790283\n",
      "Episode 75: Reward = 10.735206409226125\n",
      "Episode 76: Reward = 7.887441152271544\n",
      "Episode 77: Reward = 11.866639723064253\n",
      "Episode 78: Reward = 11.581810937479554\n",
      "Episode 79: Reward = 10.86486635853097\n",
      "Episode 80: Reward = 8.340281598463763\n",
      "Episode 81: Reward = 6.674617679727366\n",
      "Episode 82: Reward = 17.39770767093855\n",
      "Episode 83: Reward = 25.02352230145256\n",
      "Episode 84: Reward = 10.802303375534073\n",
      "Episode 85: Reward = 11.010950681003077\n",
      "Episode 86: Reward = 13.784006282996142\n",
      "Episode 87: Reward = 12.164428427233512\n",
      "Episode 88: Reward = 38.79714979951933\n",
      "Episode 89: Reward = 10.37336115316186\n",
      "Episode 90: Reward = 133.51344913130376\n",
      "Episode 91: Reward = 11.592657227496783\n",
      "Episode 92: Reward = 9.96013771841104\n",
      "Episode 93: Reward = 11.607984897094754\n",
      "Episode 94: Reward = 7.048316269806556\n",
      "Episode 95: Reward = 6.118048253490516\n",
      "Episode 96: Reward = 6.158357108973915\n",
      "Episode 97: Reward = 8.544461760027945\n",
      "Episode 98: Reward = 18.62373531314256\n",
      "Episode 99: Reward = 7.020417139448113\n",
      "Episode 100: Reward = 8.68098610889819\n",
      "Episode 101: Reward = 51.10946549121056\n",
      "Episode 102: Reward = 10.784621066823469\n",
      "Episode 103: Reward = 6.75908270659641\n",
      "Episode 104: Reward = 10.02071905359939\n",
      "Episode 105: Reward = 10.785783785136728\n",
      "Episode 106: Reward = 6.975706418514154\n",
      "Episode 107: Reward = 14.423299500449216\n",
      "Episode 108: Reward = 7.474310209423925\n",
      "Episode 109: Reward = 16.710144222076018\n",
      "Episode 110: Reward = 7.21086429388459\n",
      "Episode 111: Reward = 13.152624945313724\n",
      "Episode 112: Reward = 17.85524214461388\n",
      "Episode 113: Reward = 9.469334198541821\n",
      "Episode 114: Reward = 23.292425107245336\n",
      "Episode 115: Reward = 25.095193509398943\n",
      "Episode 116: Reward = 6.815685816433951\n",
      "Episode 117: Reward = 20.215898875937587\n",
      "Episode 118: Reward = 11.973312407918334\n",
      "Episode 119: Reward = 10.073608587209344\n",
      "Episode 120: Reward = 9.062460001717398\n",
      "Episode 121: Reward = 7.421264856335739\n",
      "Episode 122: Reward = 16.745920802364544\n",
      "Episode 123: Reward = 7.568656537830759\n",
      "Episode 124: Reward = 26.781930074639703\n",
      "Episode 125: Reward = 12.097343824742351\n",
      "Episode 126: Reward = 17.176658468683517\n",
      "Episode 127: Reward = 12.158863438290517\n",
      "Episode 128: Reward = 6.599466333907143\n",
      "Episode 129: Reward = 10.448594294994955\n",
      "Episode 130: Reward = 5.265088321742528\n",
      "Episode 131: Reward = 7.559684868512866\n",
      "Episode 132: Reward = 36.526138650910376\n",
      "Episode 133: Reward = 18.684823846636775\n",
      "Episode 134: Reward = 8.42853744145338\n",
      "Episode 135: Reward = 86.0279327263531\n",
      "Episode 136: Reward = 15.053861636820539\n",
      "Episode 137: Reward = 13.287188127592566\n",
      "Episode 138: Reward = 6.0896328496296945\n",
      "Episode 139: Reward = 6.246092485582688\n",
      "Episode 140: Reward = 10.54816472005818\n",
      "Episode 141: Reward = 11.650566713192337\n",
      "Episode 142: Reward = 22.35192382412481\n",
      "Episode 143: Reward = 23.3116440472196\n",
      "Episode 144: Reward = 19.926441828644002\n",
      "Episode 145: Reward = 9.646934191183046\n",
      "Episode 146: Reward = 32.21840066104811\n",
      "Episode 147: Reward = 13.482670461600247\n",
      "Episode 148: Reward = 9.185381043887368\n",
      "Episode 149: Reward = 6.32269117951206\n",
      "Episode 150: Reward = 6.333904283330101\n",
      "Episode 151: Reward = 7.461359646105624\n",
      "Episode 152: Reward = 11.021367227472492\n",
      "Episode 153: Reward = 8.542120015677787\n",
      "Episode 154: Reward = 10.231458751812134\n",
      "Episode 155: Reward = 11.106622716743008\n",
      "Episode 156: Reward = 11.130317817664515\n",
      "Episode 157: Reward = 8.881713669813907\n",
      "Episode 158: Reward = 5.428941264265735\n",
      "Episode 159: Reward = 43.15575622759494\n",
      "Episode 160: Reward = 9.439816088460203\n",
      "Episode 161: Reward = 10.26401004780006\n",
      "Episode 162: Reward = 16.113569260059833\n",
      "Episode 163: Reward = 14.372559452182983\n",
      "Episode 164: Reward = 9.788475416462393\n",
      "Episode 165: Reward = 21.80874542790879\n",
      "Episode 166: Reward = 17.549254939755652\n",
      "Episode 167: Reward = 7.109886926147017\n",
      "Episode 168: Reward = 4.006968099970866\n",
      "Episode 169: Reward = 8.334945791031293\n",
      "Episode 170: Reward = 104.01911572306798\n",
      "Episode 171: Reward = 7.791494615835725\n",
      "Episode 172: Reward = 7.237320324751205\n",
      "Episode 173: Reward = 36.73228302121266\n",
      "Episode 174: Reward = 4.688733154722872\n",
      "Episode 175: Reward = 6.703254044998477\n",
      "Episode 176: Reward = 6.956273740442967\n",
      "Episode 177: Reward = 14.422163104035297\n",
      "Episode 178: Reward = 6.831342986366836\n",
      "Episode 179: Reward = 9.510098190986525\n",
      "Episode 180: Reward = 8.343419483581393\n",
      "Episode 181: Reward = 8.955588234818952\n",
      "Episode 182: Reward = 16.0407444411499\n",
      "Episode 183: Reward = 10.81066607160223\n",
      "Episode 184: Reward = 17.60940869068642\n",
      "Episode 185: Reward = 8.209631308592058\n",
      "Episode 186: Reward = 16.027110499687392\n",
      "Episode 187: Reward = 5.7855032029872495\n",
      "Episode 188: Reward = 11.066637215819911\n",
      "Episode 189: Reward = 7.796126591582585\n",
      "Episode 190: Reward = 14.077552748897979\n",
      "Episode 191: Reward = 7.8371463267719745\n",
      "Episode 192: Reward = 7.7205092475819\n",
      "Episode 193: Reward = 75.68029798824611\n",
      "Episode 194: Reward = 6.844527846204859\n",
      "Episode 195: Reward = 26.544057084829575\n",
      "Episode 196: Reward = 12.623344916056755\n",
      "Episode 197: Reward = 10.999143650743083\n",
      "Episode 198: Reward = 9.391725341021825\n",
      "Episode 199: Reward = 13.029767822647697\n",
      "Episode 200: Reward = 10.402230526348925\n",
      "Average Reward over 200 Episodes: 14.746459783843028\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, RobustAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:59:05.644255Z",
     "iopub.status.busy": "2024-12-13T21:59:05.643930Z",
     "iopub.status.idle": "2024-12-13T21:59:11.512541Z",
     "shell.execute_reply": "2024-12-13T21:59:11.511712Z",
     "shell.execute_reply.started": "2024-12-13T21:59:05.644223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 39.2224931317058\n",
      "Episode 2: Reward = 41.22929859486118\n",
      "Episode 3: Reward = 40.45784215218881\n",
      "Episode 4: Reward = 40.23283605668382\n",
      "Episode 5: Reward = 40.39897994694226\n",
      "Episode 6: Reward = 40.221428874264504\n",
      "Episode 7: Reward = 41.01772176944938\n",
      "Episode 8: Reward = 40.27922336631392\n",
      "Episode 9: Reward = 39.061015672546574\n",
      "Episode 10: Reward = 40.70719901907581\n",
      "Episode 11: Reward = 40.31623703239629\n",
      "Episode 12: Reward = 38.260970694485444\n",
      "Episode 13: Reward = 39.33027439897924\n",
      "Episode 14: Reward = 40.41245361055472\n",
      "Episode 15: Reward = 40.08277081093781\n",
      "Episode 16: Reward = 41.2159869693659\n",
      "Episode 17: Reward = 41.54063980094815\n",
      "Episode 18: Reward = 39.195923863531505\n",
      "Episode 19: Reward = 39.36841631877313\n",
      "Episode 20: Reward = 40.094481008761214\n",
      "Episode 21: Reward = 40.09466668347692\n",
      "Episode 22: Reward = 40.35980564809633\n",
      "Episode 23: Reward = 40.15648743358332\n",
      "Episode 24: Reward = 40.58885837654398\n",
      "Episode 25: Reward = 40.14466295435432\n",
      "Episode 26: Reward = 40.083028072710924\n",
      "Episode 27: Reward = 41.11035175907372\n",
      "Episode 28: Reward = 38.59393821670517\n",
      "Episode 29: Reward = 40.45692197003214\n",
      "Episode 30: Reward = 40.2178112496349\n",
      "Episode 31: Reward = 40.19343825498544\n",
      "Episode 32: Reward = 39.17041140406173\n",
      "Episode 33: Reward = 40.43358106298522\n",
      "Episode 34: Reward = 41.49097229116223\n",
      "Episode 35: Reward = 38.19207977822882\n",
      "Episode 36: Reward = 39.50613258759643\n",
      "Episode 37: Reward = 40.497484472572864\n",
      "Episode 38: Reward = 41.060080467030865\n",
      "Episode 39: Reward = 39.43394917450541\n",
      "Episode 40: Reward = 40.257568156676875\n",
      "Episode 41: Reward = 39.691161136966635\n",
      "Episode 42: Reward = 40.317231636493204\n",
      "Episode 43: Reward = 39.986911834189236\n",
      "Episode 44: Reward = 40.54013704246041\n",
      "Episode 45: Reward = 39.217775230447955\n",
      "Episode 46: Reward = 38.285595850819846\n",
      "Episode 47: Reward = 40.328120967202906\n",
      "Episode 48: Reward = 38.993422828194866\n",
      "Episode 49: Reward = 39.44199291645172\n",
      "Episode 50: Reward = 40.10702102346446\n",
      "Episode 51: Reward = 39.033791739085316\n",
      "Episode 52: Reward = 41.093045793639014\n",
      "Episode 53: Reward = 39.404452976603075\n",
      "Episode 54: Reward = 40.21332281741329\n",
      "Episode 55: Reward = 39.10593397943871\n",
      "Episode 56: Reward = 39.89409145762207\n",
      "Episode 57: Reward = 40.17530514626858\n",
      "Episode 58: Reward = 41.01898443741661\n",
      "Episode 59: Reward = 40.23878099306391\n",
      "Episode 60: Reward = 39.35050680476334\n",
      "Episode 61: Reward = 40.30748980346448\n",
      "Episode 62: Reward = 40.28148038555233\n",
      "Episode 63: Reward = 41.37144713393491\n",
      "Episode 64: Reward = 41.562965086557426\n",
      "Episode 65: Reward = 39.960503800662174\n",
      "Episode 66: Reward = 39.06436294398744\n",
      "Episode 67: Reward = 41.474104060083235\n",
      "Episode 68: Reward = 39.117476753035774\n",
      "Episode 69: Reward = 39.10894364457396\n",
      "Episode 70: Reward = 40.379566908766925\n",
      "Episode 71: Reward = 39.39330730783707\n",
      "Episode 72: Reward = 38.94888622164953\n",
      "Episode 73: Reward = 39.427120823295645\n",
      "Episode 74: Reward = 39.15118233019189\n",
      "Episode 75: Reward = 39.24866495718345\n",
      "Episode 76: Reward = 39.2751192346024\n",
      "Episode 77: Reward = 38.36886384058732\n",
      "Episode 78: Reward = 40.03360836201786\n",
      "Episode 79: Reward = 40.324660409471676\n",
      "Episode 80: Reward = 40.17222197317133\n",
      "Episode 81: Reward = 39.44385399346674\n",
      "Episode 82: Reward = 40.19505924654358\n",
      "Episode 83: Reward = 39.9492001798294\n",
      "Episode 84: Reward = 40.475324554620876\n",
      "Episode 85: Reward = 41.18145347509882\n",
      "Episode 86: Reward = 39.194256003043044\n",
      "Episode 87: Reward = 39.38155404541896\n",
      "Episode 88: Reward = 39.34418011646372\n",
      "Episode 89: Reward = 40.327643669602026\n",
      "Episode 90: Reward = 40.18799391987218\n",
      "Episode 91: Reward = 40.21373624818141\n",
      "Episode 92: Reward = 40.201166511651124\n",
      "Episode 93: Reward = 41.29329901555728\n",
      "Episode 94: Reward = 40.06748749110394\n",
      "Episode 95: Reward = 39.31576513263669\n",
      "Episode 96: Reward = 38.17133021599721\n",
      "Episode 97: Reward = 40.43475229679748\n",
      "Episode 98: Reward = 40.44610359409948\n",
      "Episode 99: Reward = 39.04068575115348\n",
      "Episode 100: Reward = 40.27536116367012\n",
      "Episode 101: Reward = 40.04792702294734\n",
      "Episode 102: Reward = 40.26237740625154\n",
      "Episode 103: Reward = 39.53376643156417\n",
      "Episode 104: Reward = 40.112797957729654\n",
      "Episode 105: Reward = 39.41264611965923\n",
      "Episode 106: Reward = 40.38553634982328\n",
      "Episode 107: Reward = 38.96658590017882\n",
      "Episode 108: Reward = 40.12361959066406\n",
      "Episode 109: Reward = 40.06991714025977\n",
      "Episode 110: Reward = 39.47273876256074\n",
      "Episode 111: Reward = 40.38392531437123\n",
      "Episode 112: Reward = 39.31923553656598\n",
      "Episode 113: Reward = 40.3048738480172\n",
      "Episode 114: Reward = 40.24006038114746\n",
      "Episode 115: Reward = 41.132558491278594\n",
      "Episode 116: Reward = 39.29735500790663\n",
      "Episode 117: Reward = 40.07139494594416\n",
      "Episode 118: Reward = 39.19826872307495\n",
      "Episode 119: Reward = 40.59923911796182\n",
      "Episode 120: Reward = 39.96425689083543\n",
      "Episode 121: Reward = 39.3056692835899\n",
      "Episode 122: Reward = 41.268190090327856\n",
      "Episode 123: Reward = 39.28642980412808\n",
      "Episode 124: Reward = 39.41191363280649\n",
      "Episode 125: Reward = 39.111765905960915\n",
      "Episode 126: Reward = 39.88340337308701\n",
      "Episode 127: Reward = 41.447364140930986\n",
      "Episode 128: Reward = 38.90365357609902\n",
      "Episode 129: Reward = 40.439208596726345\n",
      "Episode 130: Reward = 38.95028290016834\n",
      "Episode 131: Reward = 40.527062136696266\n",
      "Episode 132: Reward = 39.17670860155498\n",
      "Episode 133: Reward = 40.13836076770328\n",
      "Episode 134: Reward = 41.20073477370002\n",
      "Episode 135: Reward = 40.19591725468987\n",
      "Episode 136: Reward = 38.050978812722704\n",
      "Episode 137: Reward = 39.49418454773086\n",
      "Episode 138: Reward = 40.552229565831425\n",
      "Episode 139: Reward = 39.20529940962092\n",
      "Episode 140: Reward = 40.168633167276674\n",
      "Episode 141: Reward = 41.28971242872397\n",
      "Episode 142: Reward = 39.928651441126114\n",
      "Episode 143: Reward = 40.17337387036011\n",
      "Episode 144: Reward = 39.56139891321085\n",
      "Episode 145: Reward = 39.35863036021366\n",
      "Episode 146: Reward = 40.127070480899434\n",
      "Episode 147: Reward = 39.188861118321064\n",
      "Episode 148: Reward = 40.4644760614651\n",
      "Episode 149: Reward = 41.47471394331771\n",
      "Episode 150: Reward = 41.56837548207588\n",
      "Episode 151: Reward = 40.38196722748824\n",
      "Episode 152: Reward = 39.477456623933975\n",
      "Episode 153: Reward = 41.243433321073304\n",
      "Episode 154: Reward = 41.505038595805196\n",
      "Episode 155: Reward = 38.21197635021319\n",
      "Episode 156: Reward = 39.35246121154467\n",
      "Episode 157: Reward = 39.50955472529263\n",
      "Episode 158: Reward = 41.34071116977088\n",
      "Episode 159: Reward = 41.201040704766825\n",
      "Episode 160: Reward = 38.909552588241056\n",
      "Episode 161: Reward = 40.277818989657526\n",
      "Episode 162: Reward = 39.07428774661456\n",
      "Episode 163: Reward = 39.97645205360465\n",
      "Episode 164: Reward = 40.44073023421456\n",
      "Episode 165: Reward = 40.220709550791156\n",
      "Episode 166: Reward = 40.178546191526785\n",
      "Episode 167: Reward = 38.27786231743284\n",
      "Episode 168: Reward = 40.39724419943475\n",
      "Episode 169: Reward = 41.07093416959399\n",
      "Episode 170: Reward = 40.88075937496602\n",
      "Episode 171: Reward = 41.10317039542298\n",
      "Episode 172: Reward = 40.37277720178307\n",
      "Episode 173: Reward = 40.20035394633305\n",
      "Episode 174: Reward = 40.12607025140401\n",
      "Episode 175: Reward = 41.11955813536698\n",
      "Episode 176: Reward = 39.14555410282929\n",
      "Episode 177: Reward = 39.3378516356064\n",
      "Episode 178: Reward = 40.3574145930545\n",
      "Episode 179: Reward = 40.21409526880696\n",
      "Episode 180: Reward = 41.48307547847751\n",
      "Episode 181: Reward = 41.29756023864233\n",
      "Episode 182: Reward = 38.54746471800486\n",
      "Episode 183: Reward = 40.061756680628285\n",
      "Episode 184: Reward = 40.26420859605478\n",
      "Episode 185: Reward = 38.39192708553786\n",
      "Episode 186: Reward = 38.385079848933366\n",
      "Episode 187: Reward = 39.62037256950436\n",
      "Episode 188: Reward = 40.21762889093074\n",
      "Episode 189: Reward = 39.50057165871844\n",
      "Episode 190: Reward = 38.97287610879229\n",
      "Episode 191: Reward = 40.286153356969294\n",
      "Episode 192: Reward = 40.384624568206824\n",
      "Episode 193: Reward = 41.33406550030032\n",
      "Episode 194: Reward = 41.507565395386024\n",
      "Episode 195: Reward = 39.174772457143874\n",
      "Episode 196: Reward = 39.97655965064974\n",
      "Episode 197: Reward = 40.27402789181696\n",
      "Episode 198: Reward = 40.37564334030645\n",
      "Episode 199: Reward = 40.436953030275475\n",
      "Episode 200: Reward = 41.4186263255673\n",
      "Average Reward over 200 episodes: 40.009037119857226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.009037119857226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T21:59:17.122777Z",
     "iopub.status.busy": "2024-12-13T21:59:17.121965Z",
     "iopub.status.idle": "2024-12-13T22:00:53.561626Z",
     "shell.execute_reply": "2024-12-13T22:00:53.560701Z",
     "shell.execute_reply.started": "2024-12-13T21:59:17.122729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 10.434742914791503\n",
      "Episode 2/200: Reward = 5.417426003477402\n",
      "Episode 3/200: Reward = 14.668772095687922\n",
      "Episode 4/200: Reward = 38.2123422633607\n",
      "Episode 5/200: Reward = 14.282820308923435\n",
      "Episode 6/200: Reward = 8.953468696901771\n",
      "Episode 7/200: Reward = 17.629842888418775\n",
      "Episode 8/200: Reward = 8.24715327926889\n",
      "Episode 9/200: Reward = 15.3304959363805\n",
      "Episode 10/200: Reward = 133.57492896370198\n",
      "Episode 11/200: Reward = 8.08644098517564\n",
      "Episode 12/200: Reward = 25.81871571647448\n",
      "Episode 13/200: Reward = 7.176300306696672\n",
      "Episode 14/200: Reward = 13.486829103478758\n",
      "Episode 15/200: Reward = 10.950812106214816\n",
      "Episode 16/200: Reward = 24.268289808574032\n",
      "Episode 17/200: Reward = 11.207309992832563\n",
      "Episode 18/200: Reward = 4.0628568264083995\n",
      "Episode 19/200: Reward = 10.721401542100223\n",
      "Episode 20/200: Reward = 9.15268438293499\n",
      "Episode 21/200: Reward = 15.409369871994203\n",
      "Episode 22/200: Reward = 12.352357959653983\n",
      "Episode 23/200: Reward = 8.117591254145553\n",
      "Episode 24/200: Reward = 9.920439372282884\n",
      "Episode 25/200: Reward = 13.347379264567108\n",
      "Episode 26/200: Reward = 10.225415923439801\n",
      "Episode 27/200: Reward = 58.959763341540864\n",
      "Episode 28/200: Reward = 8.102298089919293\n",
      "Episode 29/200: Reward = 5.452928513934436\n",
      "Episode 30/200: Reward = 11.434637416612018\n",
      "Episode 31/200: Reward = 7.155783169051557\n",
      "Episode 32/200: Reward = 9.78369122741243\n",
      "Episode 33/200: Reward = 12.198663780908984\n",
      "Episode 34/200: Reward = 8.331930854488368\n",
      "Episode 35/200: Reward = 10.007019198234957\n",
      "Episode 36/200: Reward = 41.88540178314381\n",
      "Episode 37/200: Reward = 11.58182554239045\n",
      "Episode 38/200: Reward = 16.765111898317286\n",
      "Episode 39/200: Reward = 7.371255948757386\n",
      "Episode 40/200: Reward = 7.184907595405289\n",
      "Episode 41/200: Reward = 10.327280923727876\n",
      "Episode 42/200: Reward = 8.68191082002159\n",
      "Episode 43/200: Reward = 12.042486245363687\n",
      "Episode 44/200: Reward = 12.318972401045428\n",
      "Episode 45/200: Reward = 13.828533354822596\n",
      "Episode 46/200: Reward = 13.096362831935618\n",
      "Episode 47/200: Reward = 13.111062443883736\n",
      "Episode 48/200: Reward = 7.73341579162238\n",
      "Episode 49/200: Reward = 19.451218403485857\n",
      "Episode 50/200: Reward = 12.417431944891646\n",
      "Episode 51/200: Reward = 7.89357654953152\n",
      "Episode 52/200: Reward = 10.589838875854511\n",
      "Episode 53/200: Reward = 8.344361555720049\n",
      "Episode 54/200: Reward = 8.281375013044274\n",
      "Episode 55/200: Reward = 81.25401766695188\n",
      "Episode 56/200: Reward = 6.529568934184502\n",
      "Episode 57/200: Reward = 12.108312127573496\n",
      "Episode 58/200: Reward = 15.768323407742928\n",
      "Episode 59/200: Reward = 8.602479931930977\n",
      "Episode 60/200: Reward = 15.960419143444506\n",
      "Episode 61/200: Reward = 8.749791124511308\n",
      "Episode 62/200: Reward = 39.752626713871464\n",
      "Episode 63/200: Reward = 8.46457449766868\n",
      "Episode 64/200: Reward = 44.674423971478625\n",
      "Episode 65/200: Reward = 6.993741350362681\n",
      "Episode 66/200: Reward = 9.797537708701196\n",
      "Episode 67/200: Reward = 8.49988192977527\n",
      "Episode 68/200: Reward = 25.854091119284817\n",
      "Episode 69/200: Reward = 7.884125286459066\n",
      "Episode 70/200: Reward = 7.8869836789409815\n",
      "Episode 71/200: Reward = 5.94329244549852\n",
      "Episode 72/200: Reward = 7.706373366459793\n",
      "Episode 73/200: Reward = 16.017240298306362\n",
      "Episode 74/200: Reward = 19.163728881795528\n",
      "Episode 75/200: Reward = 8.329771852462567\n",
      "Episode 76/200: Reward = 42.64997947324097\n",
      "Episode 77/200: Reward = 41.72981363078493\n",
      "Episode 78/200: Reward = 7.7450293225428775\n",
      "Episode 79/200: Reward = 66.6864619443085\n",
      "Episode 80/200: Reward = 6.49757915978448\n",
      "Episode 81/200: Reward = 9.909142004730436\n",
      "Episode 82/200: Reward = 11.745864690210745\n",
      "Episode 83/200: Reward = 25.681783385696992\n",
      "Episode 84/200: Reward = 10.616908433352634\n",
      "Episode 85/200: Reward = 10.688447766961659\n",
      "Episode 86/200: Reward = 27.30604585812352\n",
      "Episode 87/200: Reward = 26.78579352689166\n",
      "Episode 88/200: Reward = 6.209906358859901\n",
      "Episode 89/200: Reward = 13.777272364674724\n",
      "Episode 90/200: Reward = 8.981424678910454\n",
      "Episode 91/200: Reward = 16.77106390685242\n",
      "Episode 92/200: Reward = 14.92851732220065\n",
      "Episode 93/200: Reward = 16.432831557466155\n",
      "Episode 94/200: Reward = 56.24011261684414\n",
      "Episode 95/200: Reward = 6.123197999770206\n",
      "Episode 96/200: Reward = 8.118526451641573\n",
      "Episode 97/200: Reward = 24.93430260241433\n",
      "Episode 98/200: Reward = 7.058823494722083\n",
      "Episode 99/200: Reward = 67.86572771762138\n",
      "Episode 100/200: Reward = 5.82060721259579\n",
      "Episode 101/200: Reward = 25.26904392310598\n",
      "Episode 102/200: Reward = 47.67261590948908\n",
      "Episode 103/200: Reward = 41.15559418014264\n",
      "Episode 104/200: Reward = 6.770827160758983\n",
      "Episode 105/200: Reward = 7.256049108771776\n",
      "Episode 106/200: Reward = 18.947619724980107\n",
      "Episode 107/200: Reward = 42.91386069501616\n",
      "Episode 108/200: Reward = 16.51558804823407\n",
      "Episode 109/200: Reward = 16.34156579421299\n",
      "Episode 110/200: Reward = 5.204895631632926\n",
      "Episode 111/200: Reward = 8.438059521440373\n",
      "Episode 112/200: Reward = 6.5449911341562474\n",
      "Episode 113/200: Reward = 10.160861781749713\n",
      "Episode 114/200: Reward = 67.46534121811\n",
      "Episode 115/200: Reward = 15.222285590829866\n",
      "Episode 116/200: Reward = 13.310631995752919\n",
      "Episode 117/200: Reward = 13.466880990714388\n",
      "Episode 118/200: Reward = 9.769784911055206\n",
      "Episode 119/200: Reward = 5.750991729607159\n",
      "Episode 120/200: Reward = 9.877911236166407\n",
      "Episode 121/200: Reward = 12.798176615557297\n",
      "Episode 122/200: Reward = 7.8207597370129385\n",
      "Episode 123/200: Reward = 9.889736668495615\n",
      "Episode 124/200: Reward = 10.057896796805565\n",
      "Episode 125/200: Reward = 10.29861184374701\n",
      "Episode 126/200: Reward = 71.96129374519168\n",
      "Episode 127/200: Reward = 35.9419349760144\n",
      "Episode 128/200: Reward = 23.007438417792535\n",
      "Episode 129/200: Reward = 8.848530947988033\n",
      "Episode 130/200: Reward = 17.520737580751018\n",
      "Episode 131/200: Reward = 14.33344206646229\n",
      "Episode 132/200: Reward = 18.79213724051916\n",
      "Episode 133/200: Reward = 10.23899234514161\n",
      "Episode 134/200: Reward = 63.56615491104018\n",
      "Episode 135/200: Reward = 10.539254211030006\n",
      "Episode 136/200: Reward = 15.612061338470841\n",
      "Episode 137/200: Reward = 5.418147253213789\n",
      "Episode 138/200: Reward = 8.140503662147566\n",
      "Episode 139/200: Reward = 20.08616938463257\n",
      "Episode 140/200: Reward = 10.048120936176321\n",
      "Episode 141/200: Reward = 7.656245028426488\n",
      "Episode 142/200: Reward = 11.515429200349248\n",
      "Episode 143/200: Reward = 14.110981676444423\n",
      "Episode 144/200: Reward = 8.36506936639234\n",
      "Episode 145/200: Reward = 9.042831573611473\n",
      "Episode 146/200: Reward = 10.447001177547651\n",
      "Episode 147/200: Reward = 12.956638402239156\n",
      "Episode 148/200: Reward = 14.449430614225781\n",
      "Episode 149/200: Reward = 13.137813648753257\n",
      "Episode 150/200: Reward = 15.413660764370132\n",
      "Episode 151/200: Reward = 15.548032595334588\n",
      "Episode 152/200: Reward = 17.00945360552954\n",
      "Episode 153/200: Reward = 9.14164132371288\n",
      "Episode 154/200: Reward = 15.736207452074275\n",
      "Episode 155/200: Reward = 6.607071550965188\n",
      "Episode 156/200: Reward = 19.21183263115409\n",
      "Episode 157/200: Reward = 17.874396513578017\n",
      "Episode 158/200: Reward = 25.575790591788966\n",
      "Episode 159/200: Reward = 19.671571309057025\n",
      "Episode 160/200: Reward = 6.347274696893577\n",
      "Episode 161/200: Reward = 16.5803576190886\n",
      "Episode 162/200: Reward = 8.69006431493796\n",
      "Episode 163/200: Reward = 10.319144112696408\n",
      "Episode 164/200: Reward = 11.511935913647644\n",
      "Episode 165/200: Reward = 4.74599212866916\n",
      "Episode 166/200: Reward = 9.426609605311889\n",
      "Episode 167/200: Reward = 7.165752332431347\n",
      "Episode 168/200: Reward = 51.868528301424206\n",
      "Episode 169/200: Reward = 6.496512279389658\n",
      "Episode 170/200: Reward = 12.768649213991079\n",
      "Episode 171/200: Reward = 18.517270533484577\n",
      "Episode 172/200: Reward = 4.739812766630852\n",
      "Episode 173/200: Reward = 13.930017221884157\n",
      "Episode 174/200: Reward = 10.352857970809204\n",
      "Episode 175/200: Reward = 7.624394879505488\n",
      "Episode 176/200: Reward = 19.677771393669605\n",
      "Episode 177/200: Reward = 10.158129478801165\n",
      "Episode 178/200: Reward = 14.666722308271895\n",
      "Episode 179/200: Reward = 19.41043245392043\n",
      "Episode 180/200: Reward = 16.912728079115976\n",
      "Episode 181/200: Reward = 27.77700575258704\n",
      "Episode 182/200: Reward = 60.803388369705935\n",
      "Episode 183/200: Reward = 7.002747478196735\n",
      "Episode 184/200: Reward = 7.370151228913031\n",
      "Episode 185/200: Reward = 4.8985317196525635\n",
      "Episode 186/200: Reward = 5.070213746656338\n",
      "Episode 187/200: Reward = 9.54263667068285\n",
      "Episode 188/200: Reward = 6.797609796104408\n",
      "Episode 189/200: Reward = 15.167453658342364\n",
      "Episode 190/200: Reward = 8.785708333076661\n",
      "Episode 191/200: Reward = 27.106988160091973\n",
      "Episode 192/200: Reward = 12.083564842352182\n",
      "Episode 193/200: Reward = 7.804915330540557\n",
      "Episode 194/200: Reward = 6.227972166237563\n",
      "Episode 195/200: Reward = 48.695985090160875\n",
      "Episode 196/200: Reward = 10.559018948696231\n",
      "Episode 197/200: Reward = 31.86492676281576\n",
      "Episode 198/200: Reward = 5.759013988963417\n",
      "Episode 199/200: Reward = 11.58457578652522\n",
      "Episode 200/200: Reward = 14.90009751672888\n",
      "Average Reward under MAD attack: 16.953978825714188\n",
      "Final Average Reward under MAD Attack: 16.953978825714188\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:11:27.729664Z",
     "iopub.status.busy": "2024-12-13T22:11:27.729346Z",
     "iopub.status.idle": "2024-12-13T22:15:11.569359Z",
     "shell.execute_reply": "2024-12-13T22:15:11.568409Z",
     "shell.execute_reply.started": "2024-12-13T22:11:27.729636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4284193347.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/5000, TD Loss: 0.6278, Robust Loss: 0.1636\n",
      "Step 200/5000, TD Loss: 0.5392, Robust Loss: 0.3412\n",
      "Step 300/5000, TD Loss: 0.5079, Robust Loss: 0.3753\n",
      "Step 400/5000, TD Loss: 0.4827, Robust Loss: 0.7698\n",
      "Step 500/5000, TD Loss: 0.4616, Robust Loss: 0.5638\n",
      "Step 600/5000, TD Loss: 0.4881, Robust Loss: 0.9662\n",
      "Step 700/5000, TD Loss: 0.4720, Robust Loss: 0.7391\n",
      "Step 800/5000, TD Loss: 0.4840, Robust Loss: 0.8571\n",
      "Step 900/5000, TD Loss: 0.4464, Robust Loss: 0.8866\n",
      "Step 1000/5000, TD Loss: 0.4585, Robust Loss: 1.0489\n",
      "Step 1100/5000, TD Loss: 0.4712, Robust Loss: 0.8254\n",
      "Step 1200/5000, TD Loss: 0.4780, Robust Loss: 0.9241\n",
      "Step 1300/5000, TD Loss: 0.4499, Robust Loss: 0.9922\n",
      "Step 1400/5000, TD Loss: 0.4310, Robust Loss: 0.7281\n",
      "Step 1500/5000, TD Loss: 0.4719, Robust Loss: 0.9722\n",
      "Step 1600/5000, TD Loss: 0.4361, Robust Loss: 0.9781\n",
      "Step 1700/5000, TD Loss: 0.4505, Robust Loss: 0.8806\n",
      "Step 1800/5000, TD Loss: 0.4228, Robust Loss: 1.3964\n",
      "Step 1900/5000, TD Loss: 0.4204, Robust Loss: 0.7944\n",
      "Step 2000/5000, TD Loss: 0.4761, Robust Loss: 0.9264\n",
      "Step 2100/5000, TD Loss: 0.4828, Robust Loss: 0.7450\n",
      "Step 2200/5000, TD Loss: 0.4871, Robust Loss: 0.9805\n",
      "Step 2300/5000, TD Loss: 0.4537, Robust Loss: 0.8589\n",
      "Step 2400/5000, TD Loss: 0.4738, Robust Loss: 0.8882\n",
      "Step 2500/5000, TD Loss: 0.4622, Robust Loss: 0.7853\n",
      "Step 2600/5000, TD Loss: 0.4316, Robust Loss: 0.8568\n",
      "Step 2700/5000, TD Loss: 0.4621, Robust Loss: 0.7660\n",
      "Step 2800/5000, TD Loss: 0.4503, Robust Loss: 1.0404\n",
      "Step 2900/5000, TD Loss: 0.4424, Robust Loss: 1.0314\n",
      "Step 3000/5000, TD Loss: 0.4346, Robust Loss: 0.9771\n",
      "Step 3100/5000, TD Loss: 0.4482, Robust Loss: 0.7588\n",
      "Step 3200/5000, TD Loss: 0.4556, Robust Loss: 1.0873\n",
      "Step 3300/5000, TD Loss: 0.4396, Robust Loss: 0.8976\n",
      "Step 3400/5000, TD Loss: 0.4181, Robust Loss: 0.7356\n",
      "Step 3500/5000, TD Loss: 0.4538, Robust Loss: 0.8886\n",
      "Step 3600/5000, TD Loss: 0.4428, Robust Loss: 0.8168\n",
      "Step 3700/5000, TD Loss: 0.4397, Robust Loss: 0.9195\n",
      "Step 3800/5000, TD Loss: 0.4409, Robust Loss: 1.0395\n",
      "Step 3900/5000, TD Loss: 0.4581, Robust Loss: 0.6424\n",
      "Step 4000/5000, TD Loss: 0.4899, Robust Loss: 0.6358\n",
      "Step 4100/5000, TD Loss: 0.4287, Robust Loss: 0.7413\n",
      "Step 4200/5000, TD Loss: 0.4632, Robust Loss: 0.6735\n",
      "Step 4300/5000, TD Loss: 0.4470, Robust Loss: 0.8896\n",
      "Step 4400/5000, TD Loss: 0.4409, Robust Loss: 0.6880\n",
      "Step 4500/5000, TD Loss: 0.4208, Robust Loss: 1.0079\n",
      "Step 4600/5000, TD Loss: 0.4220, Robust Loss: 0.7056\n",
      "Step 4700/5000, TD Loss: 0.4301, Robust Loss: 0.7896\n",
      "Step 4800/5000, TD Loss: 0.4456, Robust Loss: 0.8674\n",
      "Step 4900/5000, TD Loss: 0.4435, Robust Loss: 0.8214\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:17:35.003403Z",
     "iopub.status.busy": "2024-12-13T22:17:35.003170Z",
     "iopub.status.idle": "2024-12-13T22:19:11.930517Z",
     "shell.execute_reply": "2024-12-13T22:19:11.929639Z",
     "shell.execute_reply.started": "2024-12-13T22:17:35.003380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 39.48946673843793\n",
      "Episode 2/200: Reward = 38.88514034896076\n",
      "Episode 3/200: Reward = 39.399382327984625\n",
      "Episode 4/200: Reward = 41.69809705750213\n",
      "Episode 5/200: Reward = 40.36334403271557\n",
      "Episode 6/200: Reward = 39.28390451162531\n",
      "Episode 7/200: Reward = 39.000457952806386\n",
      "Episode 8/200: Reward = 39.27576847154834\n",
      "Episode 9/200: Reward = 39.41900023785964\n",
      "Episode 10/200: Reward = 39.097197104467675\n",
      "Episode 11/200: Reward = 39.22610448110138\n",
      "Episode 12/200: Reward = 40.34034905099732\n",
      "Episode 13/200: Reward = 39.37402439952458\n",
      "Episode 14/200: Reward = 38.909393465164904\n",
      "Episode 15/200: Reward = 40.27047623765004\n",
      "Episode 16/200: Reward = 39.40471991610045\n",
      "Episode 17/200: Reward = 40.133139698614194\n",
      "Episode 18/200: Reward = 40.07949520524676\n",
      "Episode 19/200: Reward = 40.53360367318834\n",
      "Episode 20/200: Reward = 41.43635253200792\n",
      "Episode 21/200: Reward = 39.19371684746211\n",
      "Episode 22/200: Reward = 40.39199855596881\n",
      "Episode 23/200: Reward = 41.40508177300148\n",
      "Episode 24/200: Reward = 39.21148988943034\n",
      "Episode 25/200: Reward = 40.51368281256247\n",
      "Episode 26/200: Reward = 39.50824175669641\n",
      "Episode 27/200: Reward = 40.32611043750901\n",
      "Episode 28/200: Reward = 41.098967668765916\n",
      "Episode 29/200: Reward = 39.0340142974627\n",
      "Episode 30/200: Reward = 40.24643937831098\n",
      "Episode 31/200: Reward = 41.213326687030786\n",
      "Episode 32/200: Reward = 39.34412494847664\n",
      "Episode 33/200: Reward = 39.97863623296223\n",
      "Episode 34/200: Reward = 40.36156295386592\n",
      "Episode 35/200: Reward = 38.312484355687786\n",
      "Episode 36/200: Reward = 39.11568899555203\n",
      "Episode 37/200: Reward = 39.4241511554674\n",
      "Episode 38/200: Reward = 38.00846925970637\n",
      "Episode 39/200: Reward = 38.94166388824591\n",
      "Episode 40/200: Reward = 41.551124011979006\n",
      "Episode 41/200: Reward = 40.3044303255397\n",
      "Episode 42/200: Reward = 39.327491639232385\n",
      "Episode 43/200: Reward = 40.531257761565406\n",
      "Episode 44/200: Reward = 40.251726160385786\n",
      "Episode 45/200: Reward = 41.67745512495084\n",
      "Episode 46/200: Reward = 40.20952961010638\n",
      "Episode 47/200: Reward = 40.49435625572145\n",
      "Episode 48/200: Reward = 40.075821596272235\n",
      "Episode 49/200: Reward = 40.416697251509056\n",
      "Episode 50/200: Reward = 41.301551453673596\n",
      "Episode 51/200: Reward = 41.675154852253314\n",
      "Episode 52/200: Reward = 39.188672888902055\n",
      "Episode 53/200: Reward = 39.09331448925859\n",
      "Episode 54/200: Reward = 39.044265199695566\n",
      "Episode 55/200: Reward = 40.15554405049256\n",
      "Episode 56/200: Reward = 39.43932710862746\n",
      "Episode 57/200: Reward = 39.17367096253479\n",
      "Episode 58/200: Reward = 40.06302259580308\n",
      "Episode 59/200: Reward = 39.41551723194722\n",
      "Episode 60/200: Reward = 39.222916349390914\n",
      "Episode 61/200: Reward = 39.24711361525916\n",
      "Episode 62/200: Reward = 40.206395961090145\n",
      "Episode 63/200: Reward = 39.25745808905375\n",
      "Episode 64/200: Reward = 40.153404494441645\n",
      "Episode 65/200: Reward = 40.283913531022364\n",
      "Episode 66/200: Reward = 41.44577417768127\n",
      "Episode 67/200: Reward = 40.176465113928586\n",
      "Episode 68/200: Reward = 39.08526536862498\n",
      "Episode 69/200: Reward = 40.09128306768489\n",
      "Episode 70/200: Reward = 40.01983316454353\n",
      "Episode 71/200: Reward = 40.34164968237275\n",
      "Episode 72/200: Reward = 40.33720911245892\n",
      "Episode 73/200: Reward = 40.156414684909784\n",
      "Episode 74/200: Reward = 40.208559243946105\n",
      "Episode 75/200: Reward = 40.199589688516\n",
      "Episode 76/200: Reward = 39.211389290528324\n",
      "Episode 77/200: Reward = 40.2651918339006\n",
      "Episode 78/200: Reward = 40.449270654648764\n",
      "Episode 79/200: Reward = 40.14843875531358\n",
      "Episode 80/200: Reward = 40.599315922706225\n",
      "Episode 81/200: Reward = 39.214580214847324\n",
      "Episode 82/200: Reward = 40.01730486902611\n",
      "Episode 83/200: Reward = 40.45266740338463\n",
      "Episode 84/200: Reward = 40.15060282217569\n",
      "Episode 85/200: Reward = 39.1973221744485\n",
      "Episode 86/200: Reward = 39.466659145396356\n",
      "Episode 87/200: Reward = 41.17223293795652\n",
      "Episode 88/200: Reward = 40.151114510350304\n",
      "Episode 89/200: Reward = 40.44651769121996\n",
      "Episode 90/200: Reward = 38.44117840545568\n",
      "Episode 91/200: Reward = 39.38799012677609\n",
      "Episode 92/200: Reward = 39.568798033873016\n",
      "Episode 93/200: Reward = 40.251344584131054\n",
      "Episode 94/200: Reward = 39.36916886183075\n",
      "Episode 95/200: Reward = 40.18000656535578\n",
      "Episode 96/200: Reward = 39.05944182386807\n",
      "Episode 97/200: Reward = 39.3078844578043\n",
      "Episode 98/200: Reward = 41.627552780197306\n",
      "Episode 99/200: Reward = 40.032750944027555\n",
      "Episode 100/200: Reward = 40.33793106947302\n",
      "Episode 101/200: Reward = 40.36102634399374\n",
      "Episode 102/200: Reward = 40.15689491470588\n",
      "Episode 103/200: Reward = 38.37411749102981\n",
      "Episode 104/200: Reward = 39.149592428597884\n",
      "Episode 105/200: Reward = 39.135770655800165\n",
      "Episode 106/200: Reward = 39.52719339123735\n",
      "Episode 107/200: Reward = 40.63916083710674\n",
      "Episode 108/200: Reward = 41.53383220226645\n",
      "Episode 109/200: Reward = 41.268306860195196\n",
      "Episode 110/200: Reward = 40.49059395501099\n",
      "Episode 111/200: Reward = 40.79111464639228\n",
      "Episode 112/200: Reward = 39.41064651329418\n",
      "Episode 113/200: Reward = 40.328907199139614\n",
      "Episode 114/200: Reward = 40.251758805773875\n",
      "Episode 115/200: Reward = 39.302044321087656\n",
      "Episode 116/200: Reward = 40.272029619958964\n",
      "Episode 117/200: Reward = 39.210123017737494\n",
      "Episode 118/200: Reward = 40.575127102116966\n",
      "Episode 119/200: Reward = 39.26834657159144\n",
      "Episode 120/200: Reward = 38.53997179342995\n",
      "Episode 121/200: Reward = 38.33665512240685\n",
      "Episode 122/200: Reward = 41.16707364276837\n",
      "Episode 123/200: Reward = 38.470317421801084\n",
      "Episode 124/200: Reward = 40.68309046639324\n",
      "Episode 125/200: Reward = 38.377991907165324\n",
      "Episode 126/200: Reward = 40.49297938241135\n",
      "Episode 127/200: Reward = 40.340819562150536\n",
      "Episode 128/200: Reward = 39.27239919897591\n",
      "Episode 129/200: Reward = 40.49070796797098\n",
      "Episode 130/200: Reward = 39.27912500968481\n",
      "Episode 131/200: Reward = 39.637818130352336\n",
      "Episode 132/200: Reward = 39.28981808779821\n",
      "Episode 133/200: Reward = 39.83537697198822\n",
      "Episode 134/200: Reward = 39.230723308259854\n",
      "Episode 135/200: Reward = 41.00301816766932\n",
      "Episode 136/200: Reward = 39.44642587697127\n",
      "Episode 137/200: Reward = 39.537267495418426\n",
      "Episode 138/200: Reward = 39.98491916018247\n",
      "Episode 139/200: Reward = 40.216502764607576\n",
      "Episode 140/200: Reward = 39.227785932365904\n",
      "Episode 141/200: Reward = 40.601617018376515\n",
      "Episode 142/200: Reward = 40.09334400593996\n",
      "Episode 143/200: Reward = 38.42513363174009\n",
      "Episode 144/200: Reward = 39.27071456135086\n",
      "Episode 145/200: Reward = 40.52875709353859\n",
      "Episode 146/200: Reward = 41.66176954056665\n",
      "Episode 147/200: Reward = 39.029321410424345\n",
      "Episode 148/200: Reward = 40.47664154884993\n",
      "Episode 149/200: Reward = 40.52318035992048\n",
      "Episode 150/200: Reward = 39.24895287036614\n",
      "Episode 151/200: Reward = 40.466492880360235\n",
      "Episode 152/200: Reward = 39.23805390843645\n",
      "Episode 153/200: Reward = 40.3916291610583\n",
      "Episode 154/200: Reward = 40.10904537874451\n",
      "Episode 155/200: Reward = 40.55641589313992\n",
      "Episode 156/200: Reward = 40.218948642904564\n",
      "Episode 157/200: Reward = 40.20601119150663\n",
      "Episode 158/200: Reward = 40.36208670081483\n",
      "Episode 159/200: Reward = 40.662108623964286\n",
      "Episode 160/200: Reward = 40.557843913873285\n",
      "Episode 161/200: Reward = 40.54932118483484\n",
      "Episode 162/200: Reward = 39.6661349189643\n",
      "Episode 163/200: Reward = 39.56547536480691\n",
      "Episode 164/200: Reward = 40.134820627625636\n",
      "Episode 165/200: Reward = 40.093228820092186\n",
      "Episode 166/200: Reward = 40.150417366224175\n",
      "Episode 167/200: Reward = 39.60869668942851\n",
      "Episode 168/200: Reward = 38.502694135860274\n",
      "Episode 169/200: Reward = 40.419031611930144\n",
      "Episode 170/200: Reward = 40.36928223885484\n",
      "Episode 171/200: Reward = 40.047253717739636\n",
      "Episode 172/200: Reward = 39.286074059840324\n",
      "Episode 173/200: Reward = 41.26610033876634\n",
      "Episode 174/200: Reward = 39.387156066913974\n",
      "Episode 175/200: Reward = 39.46731555997726\n",
      "Episode 176/200: Reward = 40.49688678962955\n",
      "Episode 177/200: Reward = 40.42704638427621\n",
      "Episode 178/200: Reward = 39.56131847225995\n",
      "Episode 179/200: Reward = 39.15236592144731\n",
      "Episode 180/200: Reward = 40.16777196308526\n",
      "Episode 181/200: Reward = 40.1224710915795\n",
      "Episode 182/200: Reward = 40.07576737700252\n",
      "Episode 183/200: Reward = 39.54029401049025\n",
      "Episode 184/200: Reward = 40.47653482659498\n",
      "Episode 185/200: Reward = 40.402698452119004\n",
      "Episode 186/200: Reward = 39.40408037293059\n",
      "Episode 187/200: Reward = 39.125610112142496\n",
      "Episode 188/200: Reward = 39.36053254086342\n",
      "Episode 189/200: Reward = 40.29099914382614\n",
      "Episode 190/200: Reward = 39.34340691465575\n",
      "Episode 191/200: Reward = 40.532016411292076\n",
      "Episode 192/200: Reward = 40.74909072307437\n",
      "Episode 193/200: Reward = 41.297121125560935\n",
      "Episode 194/200: Reward = 39.34817596827428\n",
      "Episode 195/200: Reward = 40.30198610624238\n",
      "Episode 196/200: Reward = 40.14959353529734\n",
      "Episode 197/200: Reward = 40.35766282993651\n",
      "Episode 198/200: Reward = 40.54898024494845\n",
      "Episode 199/200: Reward = 40.46038614579197\n",
      "Episode 200/200: Reward = 39.24456205672453\n",
      "Average Reward under Robust Sarsa Critic-based attack: 39.94282452968665\n",
      "Final Average Reward under Robust Sarsa Attack: 39.94282452968665\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:20:47.843034Z",
     "iopub.status.busy": "2024-12-13T22:20:47.842275Z",
     "iopub.status.idle": "2024-12-13T22:20:47.852340Z",
     "shell.execute_reply": "2024-12-13T22:20:47.851412Z",
     "shell.execute_reply.started": "2024-12-13T22:20:47.843003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_state_value_attack(env, policy_net, value_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a State Value Attack using a value network.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        value_net (torch.nn.Module): The trained value network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under the state value attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute value for the perturbed state\n",
    "                value = value_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Minimize or maximize the value\n",
    "                loss = -value.mean()  # Gradient ascent to maximize adversarial effect\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                action_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(action_output, tuple):\n",
    "                    action = action_output[0]  # Extract mean for continuous actions\n",
    "                else:\n",
    "                    action = action_output\n",
    "\n",
    "                action = action.squeeze().cpu().numpy()  # Ensure the action is in NumPy format\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under State Value Attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:20:51.734095Z",
     "iopub.status.busy": "2024-12-13T22:20:51.733424Z",
     "iopub.status.idle": "2024-12-13T22:20:51.744831Z",
     "shell.execute_reply": "2024-12-13T22:20:51.743958Z",
     "shell.execute_reply.started": "2024-12-13T22:20:51.734062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_target_policy_attack(env, policy_net, target_action, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Target Policy Misclassification attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        target_action (torch.Tensor): The target action to force the policy to output.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Target Policy Misclassification attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Get policy output for the perturbed state\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    logits = policy_output  # For discrete actions\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, target_action)  # Cross-entropy loss\n",
    "                elif isinstance(env.action_space, gym.spaces.Box):\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Mean and std\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "                # Backpropagate to compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action = torch.argmax(policy_output, dim=1).item()  # Discrete action\n",
    "                else:\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()  # Continuous action\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Target Policy Misclassification attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:20:57.393195Z",
     "iopub.status.busy": "2024-12-13T22:20:57.392870Z",
     "iopub.status.idle": "2024-12-13T22:22:32.096564Z",
     "shell.execute_reply": "2024-12-13T22:22:32.095773Z",
     "shell.execute_reply.started": "2024-12-13T22:20:57.393166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 39.53260809583266\n",
      "Episode 2/200: Reward = 41.21169296067531\n",
      "Episode 3/200: Reward = 40.26438461444689\n",
      "Episode 4/200: Reward = 40.11466603377153\n",
      "Episode 5/200: Reward = 39.23989540268539\n",
      "Episode 6/200: Reward = 39.158202056593446\n",
      "Episode 7/200: Reward = 40.14646258301174\n",
      "Episode 8/200: Reward = 38.85618688325834\n",
      "Episode 9/200: Reward = 40.05670682291415\n",
      "Episode 10/200: Reward = 39.25603490676412\n",
      "Episode 11/200: Reward = 39.60938887267172\n",
      "Episode 12/200: Reward = 39.141153634082784\n",
      "Episode 13/200: Reward = 39.170966047815554\n",
      "Episode 14/200: Reward = 40.67548902909676\n",
      "Episode 15/200: Reward = 40.33815091997627\n",
      "Episode 16/200: Reward = 40.51220212706108\n",
      "Episode 17/200: Reward = 40.306312231794095\n",
      "Episode 18/200: Reward = 38.579173226762954\n",
      "Episode 19/200: Reward = 39.525388071306544\n",
      "Episode 20/200: Reward = 38.29488015910395\n",
      "Episode 21/200: Reward = 39.27354281913439\n",
      "Episode 22/200: Reward = 39.38107192474693\n",
      "Episode 23/200: Reward = 40.19868080589899\n",
      "Episode 24/200: Reward = 38.65887549578571\n",
      "Episode 25/200: Reward = 40.259036725595394\n",
      "Episode 26/200: Reward = 39.34731599334811\n",
      "Episode 27/200: Reward = 41.40983548658845\n",
      "Episode 28/200: Reward = 39.314379360712735\n",
      "Episode 29/200: Reward = 40.814369368081564\n",
      "Episode 30/200: Reward = 39.4077277019103\n",
      "Episode 31/200: Reward = 38.35262237809374\n",
      "Episode 32/200: Reward = 41.22220086669002\n",
      "Episode 33/200: Reward = 40.36349638257896\n",
      "Episode 34/200: Reward = 40.06973452427335\n",
      "Episode 35/200: Reward = 39.32654976063942\n",
      "Episode 36/200: Reward = 40.26204883590001\n",
      "Episode 37/200: Reward = 40.23107112876419\n",
      "Episode 38/200: Reward = 39.28198396319684\n",
      "Episode 39/200: Reward = 39.99657881167783\n",
      "Episode 40/200: Reward = 40.62775292408642\n",
      "Episode 41/200: Reward = 40.453640013564616\n",
      "Episode 42/200: Reward = 40.04882696378588\n",
      "Episode 43/200: Reward = 39.24648360957269\n",
      "Episode 44/200: Reward = 39.40000057262621\n",
      "Episode 45/200: Reward = 40.277525877962596\n",
      "Episode 46/200: Reward = 39.43066149340361\n",
      "Episode 47/200: Reward = 41.33177011522881\n",
      "Episode 48/200: Reward = 41.22291777467494\n",
      "Episode 49/200: Reward = 39.05342963095065\n",
      "Episode 50/200: Reward = 41.169702521199554\n",
      "Episode 51/200: Reward = 40.34835394879057\n",
      "Episode 52/200: Reward = 40.50989994138117\n",
      "Episode 53/200: Reward = 40.31560309199999\n",
      "Episode 54/200: Reward = 39.461294014921144\n",
      "Episode 55/200: Reward = 40.05678046865534\n",
      "Episode 56/200: Reward = 40.495214498682856\n",
      "Episode 57/200: Reward = 39.98943266659862\n",
      "Episode 58/200: Reward = 40.11200712307134\n",
      "Episode 59/200: Reward = 39.30530261242148\n",
      "Episode 60/200: Reward = 39.27162137651058\n",
      "Episode 61/200: Reward = 38.155974389649806\n",
      "Episode 62/200: Reward = 40.26792744550279\n",
      "Episode 63/200: Reward = 40.337642331178735\n",
      "Episode 64/200: Reward = 39.32743655731179\n",
      "Episode 65/200: Reward = 41.08368378486768\n",
      "Episode 66/200: Reward = 40.45219139075633\n",
      "Episode 67/200: Reward = 41.239659626293495\n",
      "Episode 68/200: Reward = 40.289220387419775\n",
      "Episode 69/200: Reward = 40.51991123858905\n",
      "Episode 70/200: Reward = 40.270414294441785\n",
      "Episode 71/200: Reward = 39.991553147528855\n",
      "Episode 72/200: Reward = 39.949052769342856\n",
      "Episode 73/200: Reward = 40.0362315913524\n",
      "Episode 74/200: Reward = 40.066668624409715\n",
      "Episode 75/200: Reward = 38.945754867044286\n",
      "Episode 76/200: Reward = 39.102250633029215\n",
      "Episode 77/200: Reward = 41.11582863500655\n",
      "Episode 78/200: Reward = 40.58659975803859\n",
      "Episode 79/200: Reward = 40.41082644248557\n",
      "Episode 80/200: Reward = 39.34195408070592\n",
      "Episode 81/200: Reward = 40.98556143311386\n",
      "Episode 82/200: Reward = 40.53923850037532\n",
      "Episode 83/200: Reward = 39.308426257883475\n",
      "Episode 84/200: Reward = 40.09048840191583\n",
      "Episode 85/200: Reward = 41.377635726500294\n",
      "Episode 86/200: Reward = 39.26291309281334\n",
      "Episode 87/200: Reward = 38.46207671247772\n",
      "Episode 88/200: Reward = 40.91199981702524\n",
      "Episode 89/200: Reward = 40.013485349299735\n",
      "Episode 90/200: Reward = 40.387605237897944\n",
      "Episode 91/200: Reward = 39.07620798107878\n",
      "Episode 92/200: Reward = 40.1110641654702\n",
      "Episode 93/200: Reward = 39.435039051541814\n",
      "Episode 94/200: Reward = 39.33263581275503\n",
      "Episode 95/200: Reward = 38.36540276004264\n",
      "Episode 96/200: Reward = 39.02814190237535\n",
      "Episode 97/200: Reward = 39.178302604510854\n",
      "Episode 98/200: Reward = 39.50024357316622\n",
      "Episode 99/200: Reward = 41.23238266456498\n",
      "Episode 100/200: Reward = 39.1582179452614\n",
      "Episode 101/200: Reward = 40.4081638319767\n",
      "Episode 102/200: Reward = 39.39715560374126\n",
      "Episode 103/200: Reward = 38.354221760885004\n",
      "Episode 104/200: Reward = 39.36923167510428\n",
      "Episode 105/200: Reward = 39.3533963066296\n",
      "Episode 106/200: Reward = 38.17273949639412\n",
      "Episode 107/200: Reward = 38.517495683538996\n",
      "Episode 108/200: Reward = 40.04017222095292\n",
      "Episode 109/200: Reward = 39.188796832185986\n",
      "Episode 110/200: Reward = 40.170736394523004\n",
      "Episode 111/200: Reward = 39.0440911555774\n",
      "Episode 112/200: Reward = 39.30409946875613\n",
      "Episode 113/200: Reward = 40.326138899444594\n",
      "Episode 114/200: Reward = 39.22927493531749\n",
      "Episode 115/200: Reward = 40.30576656398581\n",
      "Episode 116/200: Reward = 40.05161544077501\n",
      "Episode 117/200: Reward = 39.97514945445253\n",
      "Episode 118/200: Reward = 40.520961666634655\n",
      "Episode 119/200: Reward = 40.026638415775096\n",
      "Episode 120/200: Reward = 38.487567391560745\n",
      "Episode 121/200: Reward = 39.270642664677915\n",
      "Episode 122/200: Reward = 41.16683276088795\n",
      "Episode 123/200: Reward = 39.89215441582877\n",
      "Episode 124/200: Reward = 40.20296929645019\n",
      "Episode 125/200: Reward = 39.17284863865972\n",
      "Episode 126/200: Reward = 38.959300638849804\n",
      "Episode 127/200: Reward = 40.45658414811357\n",
      "Episode 128/200: Reward = 40.51399670446722\n",
      "Episode 129/200: Reward = 40.4313984709244\n",
      "Episode 130/200: Reward = 40.37225004580652\n",
      "Episode 131/200: Reward = 40.146448208984104\n",
      "Episode 132/200: Reward = 39.415325929976824\n",
      "Episode 133/200: Reward = 39.28583763182953\n",
      "Episode 134/200: Reward = 39.05665552334465\n",
      "Episode 135/200: Reward = 39.286745586378046\n",
      "Episode 136/200: Reward = 40.25221929577576\n",
      "Episode 137/200: Reward = 39.03639218863818\n",
      "Episode 138/200: Reward = 39.46486149411565\n",
      "Episode 139/200: Reward = 39.458600108078535\n",
      "Episode 140/200: Reward = 40.26303467374817\n",
      "Episode 141/200: Reward = 41.14854903922036\n",
      "Episode 142/200: Reward = 39.00117529932154\n",
      "Episode 143/200: Reward = 39.025470762030366\n",
      "Episode 144/200: Reward = 39.37502096169775\n",
      "Episode 145/200: Reward = 39.90682106235684\n",
      "Episode 146/200: Reward = 39.33616682404157\n",
      "Episode 147/200: Reward = 40.36862040238174\n",
      "Episode 148/200: Reward = 39.15609739029179\n",
      "Episode 149/200: Reward = 40.44013713172154\n",
      "Episode 150/200: Reward = 40.114791074179074\n",
      "Episode 151/200: Reward = 40.47282397298354\n",
      "Episode 152/200: Reward = 39.38085719134804\n",
      "Episode 153/200: Reward = 40.2664923076998\n",
      "Episode 154/200: Reward = 37.97932874964923\n",
      "Episode 155/200: Reward = 41.42765336321838\n",
      "Episode 156/200: Reward = 39.729745292152515\n",
      "Episode 157/200: Reward = 39.20365384788928\n",
      "Episode 158/200: Reward = 41.58898773044403\n",
      "Episode 159/200: Reward = 41.12709835687285\n",
      "Episode 160/200: Reward = 40.05653854003325\n",
      "Episode 161/200: Reward = 38.39026194481538\n",
      "Episode 162/200: Reward = 40.43971290688598\n",
      "Episode 163/200: Reward = 39.36672850784574\n",
      "Episode 164/200: Reward = 40.12751551122029\n",
      "Episode 165/200: Reward = 40.309728278451736\n",
      "Episode 166/200: Reward = 39.185495128225746\n",
      "Episode 167/200: Reward = 40.34015120001058\n",
      "Episode 168/200: Reward = 40.38493499712178\n",
      "Episode 169/200: Reward = 39.03986681607214\n",
      "Episode 170/200: Reward = 39.37257141068299\n",
      "Episode 171/200: Reward = 40.05612787962003\n",
      "Episode 172/200: Reward = 39.3282041719851\n",
      "Episode 173/200: Reward = 39.105578684910384\n",
      "Episode 174/200: Reward = 40.44526293702461\n",
      "Episode 175/200: Reward = 40.12768423387291\n",
      "Episode 176/200: Reward = 39.191401527935085\n",
      "Episode 177/200: Reward = 39.41550855864559\n",
      "Episode 178/200: Reward = 40.29109280714884\n",
      "Episode 179/200: Reward = 39.421892731180904\n",
      "Episode 180/200: Reward = 40.49014229657374\n",
      "Episode 181/200: Reward = 40.23892978758308\n",
      "Episode 182/200: Reward = 39.5743878521625\n",
      "Episode 183/200: Reward = 40.05020460551319\n",
      "Episode 184/200: Reward = 40.371007026408364\n",
      "Episode 185/200: Reward = 41.332638808228815\n",
      "Episode 186/200: Reward = 39.2335536276477\n",
      "Episode 187/200: Reward = 41.47081668851102\n",
      "Episode 188/200: Reward = 40.63986003122351\n",
      "Episode 189/200: Reward = 41.519411932223086\n",
      "Episode 190/200: Reward = 40.20900658631165\n",
      "Episode 191/200: Reward = 40.08488265762917\n",
      "Episode 192/200: Reward = 40.06148377485577\n",
      "Episode 193/200: Reward = 39.390802245279176\n",
      "Episode 194/200: Reward = 39.45840002083991\n",
      "Episode 195/200: Reward = 39.28944087209721\n",
      "Episode 196/200: Reward = 40.85578296712669\n",
      "Episode 197/200: Reward = 40.444141437462086\n",
      "Episode 198/200: Reward = 40.52649661636806\n",
      "Episode 199/200: Reward = 40.389867306149405\n",
      "Episode 200/200: Reward = 40.157470211434294\n",
      "Average Reward under State Value Attack: 39.8806006387346\n",
      "Average Reward under State Action Value Attack: 39.8806006387346\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    value_net=RobustAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T22:24:57.382442Z",
     "iopub.status.busy": "2024-12-13T22:24:57.382114Z",
     "iopub.status.idle": "2024-12-13T22:26:37.129388Z",
     "shell.execute_reply": "2024-12-13T22:26:37.128492Z",
     "shell.execute_reply.started": "2024-12-13T22:24:57.382411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1613303488.py:43: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 38.3616147138168\n",
      "Episode 2/200: Reward = 40.49922303707294\n",
      "Episode 3/200: Reward = 39.46538939845266\n",
      "Episode 4/200: Reward = 37.40209181156585\n",
      "Episode 5/200: Reward = 38.493583997793\n",
      "Episode 6/200: Reward = 38.21740920038773\n",
      "Episode 7/200: Reward = 38.25061637062804\n",
      "Episode 8/200: Reward = 40.06234226616439\n",
      "Episode 9/200: Reward = 40.64480758303203\n",
      "Episode 10/200: Reward = 38.355623418441304\n",
      "Episode 11/200: Reward = 38.59940858455558\n",
      "Episode 12/200: Reward = 39.290440324454025\n",
      "Episode 13/200: Reward = 38.44555439176992\n",
      "Episode 14/200: Reward = 39.26627947626379\n",
      "Episode 15/200: Reward = 40.657967966286265\n",
      "Episode 16/200: Reward = 38.63376392131351\n",
      "Episode 17/200: Reward = 39.395417330771615\n",
      "Episode 18/200: Reward = 38.31453385631184\n",
      "Episode 19/200: Reward = 38.592123299886154\n",
      "Episode 20/200: Reward = 39.42447717630096\n",
      "Episode 21/200: Reward = 38.19811996250014\n",
      "Episode 22/200: Reward = 40.32086506642841\n",
      "Episode 23/200: Reward = 38.57550752530815\n",
      "Episode 24/200: Reward = 38.324947683316154\n",
      "Episode 25/200: Reward = 39.78843169812549\n",
      "Episode 26/200: Reward = 38.45477637989025\n",
      "Episode 27/200: Reward = 40.37397256691469\n",
      "Episode 28/200: Reward = 38.276557696703755\n",
      "Episode 29/200: Reward = 39.568802980601696\n",
      "Episode 30/200: Reward = 40.0629482634922\n",
      "Episode 31/200: Reward = 39.28919272723555\n",
      "Episode 32/200: Reward = 38.17678414278879\n",
      "Episode 33/200: Reward = 38.56193417340834\n",
      "Episode 34/200: Reward = 38.341908122386556\n",
      "Episode 35/200: Reward = 38.54835762248924\n",
      "Episode 36/200: Reward = 38.28942359123327\n",
      "Episode 37/200: Reward = 38.22243809987499\n",
      "Episode 38/200: Reward = 38.30065360348645\n",
      "Episode 39/200: Reward = 40.48881169619141\n",
      "Episode 40/200: Reward = 40.05876593558222\n",
      "Episode 41/200: Reward = 39.442705035290224\n",
      "Episode 42/200: Reward = 39.35132297650003\n",
      "Episode 43/200: Reward = 38.08014964244606\n",
      "Episode 44/200: Reward = 38.2809383268226\n",
      "Episode 45/200: Reward = 38.203179261510655\n",
      "Episode 46/200: Reward = 38.975148269742846\n",
      "Episode 47/200: Reward = 38.351083764144775\n",
      "Episode 48/200: Reward = 38.32695522513085\n",
      "Episode 49/200: Reward = 38.423827227202466\n",
      "Episode 50/200: Reward = 38.27457200844188\n",
      "Episode 51/200: Reward = 38.296293912634994\n",
      "Episode 52/200: Reward = 38.02104879077978\n",
      "Episode 53/200: Reward = 40.27526717946475\n",
      "Episode 54/200: Reward = 38.47891357448448\n",
      "Episode 55/200: Reward = 38.576398629486924\n",
      "Episode 56/200: Reward = 39.61104233583315\n",
      "Episode 57/200: Reward = 38.229010998532296\n",
      "Episode 58/200: Reward = 38.119328179531415\n",
      "Episode 59/200: Reward = 38.44647645626495\n",
      "Episode 60/200: Reward = 38.42872471851908\n",
      "Episode 61/200: Reward = 37.140701781201834\n",
      "Episode 62/200: Reward = 37.936493999676365\n",
      "Episode 63/200: Reward = 39.53026024599069\n",
      "Episode 64/200: Reward = 39.17784616014786\n",
      "Episode 65/200: Reward = 39.38557242690624\n",
      "Episode 66/200: Reward = 39.08602235788142\n",
      "Episode 67/200: Reward = 38.465171726419705\n",
      "Episode 68/200: Reward = 39.17889610210468\n",
      "Episode 69/200: Reward = 37.62966168411818\n",
      "Episode 70/200: Reward = 38.44888235718103\n",
      "Episode 71/200: Reward = 40.47036194308167\n",
      "Episode 72/200: Reward = 38.57390323162252\n",
      "Episode 73/200: Reward = 37.29100580709827\n",
      "Episode 74/200: Reward = 38.99388029156823\n",
      "Episode 75/200: Reward = 39.41051047141869\n",
      "Episode 76/200: Reward = 38.536721780766484\n",
      "Episode 77/200: Reward = 39.49946239598498\n",
      "Episode 78/200: Reward = 39.034828306140746\n",
      "Episode 79/200: Reward = 37.42682895713311\n",
      "Episode 80/200: Reward = 38.09355431979055\n",
      "Episode 81/200: Reward = 40.37759075353397\n",
      "Episode 82/200: Reward = 39.22872041358113\n",
      "Episode 83/200: Reward = 38.227705105794676\n",
      "Episode 84/200: Reward = 38.24355727069622\n",
      "Episode 85/200: Reward = 38.34778196834399\n",
      "Episode 86/200: Reward = 38.04809325034385\n",
      "Episode 87/200: Reward = 38.41997397147347\n",
      "Episode 88/200: Reward = 40.331292751003865\n",
      "Episode 89/200: Reward = 39.27877232659282\n",
      "Episode 90/200: Reward = 39.3212842345105\n",
      "Episode 91/200: Reward = 38.9689520929467\n",
      "Episode 92/200: Reward = 38.1989421347263\n",
      "Episode 93/200: Reward = 39.51533912058527\n",
      "Episode 94/200: Reward = 38.05358623457884\n",
      "Episode 95/200: Reward = 39.547794910185544\n",
      "Episode 96/200: Reward = 38.28438305889351\n",
      "Episode 97/200: Reward = 39.23715620507396\n",
      "Episode 98/200: Reward = 38.21348247732783\n",
      "Episode 99/200: Reward = 38.232602149348146\n",
      "Episode 100/200: Reward = 38.395028717907444\n",
      "Episode 101/200: Reward = 40.316707795058306\n",
      "Episode 102/200: Reward = 39.08876157145085\n",
      "Episode 103/200: Reward = 38.66378146287869\n",
      "Episode 104/200: Reward = 39.49476005213353\n",
      "Episode 105/200: Reward = 38.32775666913244\n",
      "Episode 106/200: Reward = 39.44589598429374\n",
      "Episode 107/200: Reward = 39.37376720549475\n",
      "Episode 108/200: Reward = 39.40309525387173\n",
      "Episode 109/200: Reward = 40.25692614551798\n",
      "Episode 110/200: Reward = 39.17516953186915\n",
      "Episode 111/200: Reward = 38.56150717979102\n",
      "Episode 112/200: Reward = 39.729353422616946\n",
      "Episode 113/200: Reward = 38.29019258857329\n",
      "Episode 114/200: Reward = 38.37136069437449\n",
      "Episode 115/200: Reward = 38.07956754776561\n",
      "Episode 116/200: Reward = 38.54788541973489\n",
      "Episode 117/200: Reward = 38.54642541653002\n",
      "Episode 118/200: Reward = 38.92536109469211\n",
      "Episode 119/200: Reward = 39.19524380910616\n",
      "Episode 120/200: Reward = 39.567284639071076\n",
      "Episode 121/200: Reward = 38.13913732476226\n",
      "Episode 122/200: Reward = 38.28530263117567\n",
      "Episode 123/200: Reward = 39.43186155450781\n",
      "Episode 124/200: Reward = 38.52659974795537\n",
      "Episode 125/200: Reward = 39.63533962640998\n",
      "Episode 126/200: Reward = 39.40549055558795\n",
      "Episode 127/200: Reward = 39.373662815218154\n",
      "Episode 128/200: Reward = 38.57927163259385\n",
      "Episode 129/200: Reward = 39.00994023340423\n",
      "Episode 130/200: Reward = 39.62012390332474\n",
      "Episode 131/200: Reward = 38.485012365339074\n",
      "Episode 132/200: Reward = 39.226304900821404\n",
      "Episode 133/200: Reward = 39.197248256428416\n",
      "Episode 134/200: Reward = 38.24126900427057\n",
      "Episode 135/200: Reward = 39.33568354672644\n",
      "Episode 136/200: Reward = 38.07089909413387\n",
      "Episode 137/200: Reward = 39.24468735818316\n",
      "Episode 138/200: Reward = 37.201524877080324\n",
      "Episode 139/200: Reward = 40.00732567324463\n",
      "Episode 140/200: Reward = 39.31256176887961\n",
      "Episode 141/200: Reward = 38.33318553401228\n",
      "Episode 142/200: Reward = 38.42171522179648\n",
      "Episode 143/200: Reward = 38.52107809932673\n",
      "Episode 144/200: Reward = 38.083375366578444\n",
      "Episode 145/200: Reward = 39.39537979507637\n",
      "Episode 146/200: Reward = 39.45021328968265\n",
      "Episode 147/200: Reward = 37.35937797140488\n",
      "Episode 148/200: Reward = 39.28189233082189\n",
      "Episode 149/200: Reward = 38.58994849923133\n",
      "Episode 150/200: Reward = 39.18366610057181\n",
      "Episode 151/200: Reward = 39.0316873986162\n",
      "Episode 152/200: Reward = 39.45804435065445\n",
      "Episode 153/200: Reward = 38.19038425400323\n",
      "Episode 154/200: Reward = 38.43224314679133\n",
      "Episode 155/200: Reward = 39.1683876445528\n",
      "Episode 156/200: Reward = 38.19370071262146\n",
      "Episode 157/200: Reward = 39.08916475163034\n",
      "Episode 158/200: Reward = 38.288333873603314\n",
      "Episode 159/200: Reward = 38.33120071412032\n",
      "Episode 160/200: Reward = 39.55023590993865\n",
      "Episode 161/200: Reward = 39.05306964388171\n",
      "Episode 162/200: Reward = 39.22340677490747\n",
      "Episode 163/200: Reward = 39.030635257325414\n",
      "Episode 164/200: Reward = 39.62520740123288\n",
      "Episode 165/200: Reward = 38.28477512125186\n",
      "Episode 166/200: Reward = 39.550385480675104\n",
      "Episode 167/200: Reward = 39.30400162912798\n",
      "Episode 168/200: Reward = 39.13241924541071\n",
      "Episode 169/200: Reward = 39.42132242081255\n",
      "Episode 170/200: Reward = 40.13492198305585\n",
      "Episode 171/200: Reward = 39.319954237224785\n",
      "Episode 172/200: Reward = 38.12642639005151\n",
      "Episode 173/200: Reward = 39.510048722566374\n",
      "Episode 174/200: Reward = 38.55983786772312\n",
      "Episode 175/200: Reward = 37.415723970359075\n",
      "Episode 176/200: Reward = 39.021345312309535\n",
      "Episode 177/200: Reward = 38.98632311563214\n",
      "Episode 178/200: Reward = 39.133130136051754\n",
      "Episode 179/200: Reward = 38.60301788018254\n",
      "Episode 180/200: Reward = 39.66241975464198\n",
      "Episode 181/200: Reward = 40.29001657962827\n",
      "Episode 182/200: Reward = 40.31380315148739\n",
      "Episode 183/200: Reward = 39.35499167495527\n",
      "Episode 184/200: Reward = 38.59491666378122\n",
      "Episode 185/200: Reward = 39.19103944197473\n",
      "Episode 186/200: Reward = 38.28401111933266\n",
      "Episode 187/200: Reward = 39.3218828203778\n",
      "Episode 188/200: Reward = 39.04600284286876\n",
      "Episode 189/200: Reward = 38.40878400530424\n",
      "Episode 190/200: Reward = 39.1186990050463\n",
      "Episode 191/200: Reward = 39.09632129558971\n",
      "Episode 192/200: Reward = 39.12686400865747\n",
      "Episode 193/200: Reward = 40.04387530665769\n",
      "Episode 194/200: Reward = 38.508705506376145\n",
      "Episode 195/200: Reward = 38.596884279159866\n",
      "Episode 196/200: Reward = 39.07273814571371\n",
      "Episode 197/200: Reward = 38.97968163750328\n",
      "Episode 198/200: Reward = 39.441464755707905\n",
      "Episode 199/200: Reward = 39.17977938348359\n",
      "Episode 200/200: Reward = 38.52910228480387\n",
      "Average Reward under Target Policy Misclassification attack: 38.90106030620773\n",
      "Average Reward under Target Policy Misclassification Attack: 38.90106030620773\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(RobustAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(RobustAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
