{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-06T05:37:11.977779Z",
     "iopub.status.busy": "2024-12-06T05:37:11.976747Z",
     "iopub.status.idle": "2024-12-06T05:37:12.401103Z",
     "shell.execute_reply": "2024-12-06T05:37:12.400321Z",
     "shell.execute_reply.started": "2024-12-06T05:37:11.977729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:29:28.861320Z",
     "iopub.status.busy": "2024-12-06T08:29:28.860434Z",
     "iopub.status.idle": "2024-12-06T08:29:41.137567Z",
     "shell.execute_reply": "2024-12-06T08:29:41.136322Z",
     "shell.execute_reply.started": "2024-12-06T08:29:28.861280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:30:54.048557Z",
     "iopub.status.busy": "2024-12-06T08:30:54.047870Z",
     "iopub.status.idle": "2024-12-06T08:30:57.675563Z",
     "shell.execute_reply": "2024-12-06T08:30:57.674880Z",
     "shell.execute_reply.started": "2024-12-06T08:30:54.048508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:03.543911Z",
     "iopub.status.busy": "2024-12-06T12:46:03.543562Z",
     "iopub.status.idle": "2024-12-06T12:46:03.553876Z",
     "shell.execute_reply": "2024-12-06T12:46:03.552918Z",
     "shell.execute_reply.started": "2024-12-06T12:46:03.543881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_state_value_attack(env, policy_net, value_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a State Value Attack using a value network.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        value_net (torch.nn.Module): The trained value network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under the state value attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute value for the perturbed state\n",
    "                value = value_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Minimize or maximize the value\n",
    "                loss = -value.mean()  # Gradient ascent to maximize adversarial effect\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                action_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(action_output, tuple):\n",
    "                    action = action_output[0]  # Extract mean for continuous actions\n",
    "                else:\n",
    "                    action = action_output\n",
    "\n",
    "                action = action.squeeze().cpu().numpy()  # Ensure the action is in NumPy format\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under State Value Attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:11.252972Z",
     "iopub.status.busy": "2024-12-06T12:46:11.252193Z",
     "iopub.status.idle": "2024-12-06T12:46:11.264259Z",
     "shell.execute_reply": "2024-12-06T12:46:11.263325Z",
     "shell.execute_reply.started": "2024-12-06T12:46:11.252937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_target_policy_attack(env, policy_net, target_action, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Target Policy Misclassification attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        target_action (torch.Tensor): The target action to force the policy to output.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Target Policy Misclassification attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Get policy output for the perturbed state\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    logits = policy_output  # For discrete actions\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, target_action)  # Cross-entropy loss\n",
    "                elif isinstance(env.action_space, gym.spaces.Box):\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Mean and std\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "                # Backpropagate to compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action = torch.argmax(policy_output, dim=1).item()  # Discrete action\n",
    "                else:\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()  # Continuous action\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Target Policy Misclassification attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:16.924799Z",
     "iopub.status.busy": "2024-12-06T12:46:16.923874Z",
     "iopub.status.idle": "2024-12-06T12:46:16.934473Z",
     "shell.execute_reply": "2024-12-06T12:46:16.933715Z",
     "shell.execute_reply.started": "2024-12-06T12:46:16.924750Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:21.638824Z",
     "iopub.status.busy": "2024-12-06T12:46:21.637955Z",
     "iopub.status.idle": "2024-12-06T12:46:21.646800Z",
     "shell.execute_reply": "2024-12-06T12:46:21.645977Z",
     "shell.execute_reply.started": "2024-12-06T12:46:21.638787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:26:01.262622Z",
     "iopub.status.busy": "2024-12-06T06:26:01.262211Z",
     "iopub.status.idle": "2024-12-06T06:26:01.267101Z",
     "shell.execute_reply": "2024-12-06T06:26:01.266175Z",
     "shell.execute_reply.started": "2024-12-06T06:26:01.262587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:42.203975Z",
     "iopub.status.busy": "2024-12-06T12:46:42.203648Z",
     "iopub.status.idle": "2024-12-06T12:46:42.214077Z",
     "shell.execute_reply": "2024-12-06T12:46:42.213222Z",
     "shell.execute_reply.started": "2024-12-06T12:46:42.203945Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:47.988214Z",
     "iopub.status.busy": "2024-12-06T12:46:47.987910Z",
     "iopub.status.idle": "2024-12-06T12:46:48.004934Z",
     "shell.execute_reply": "2024-12-06T12:46:48.004061Z",
     "shell.execute_reply.started": "2024-12-06T12:46:47.988188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:46:52.845746Z",
     "iopub.status.busy": "2024-12-06T12:46:52.845000Z",
     "iopub.status.idle": "2024-12-06T12:46:52.857999Z",
     "shell.execute_reply": "2024-12-06T12:46:52.856992Z",
     "shell.execute_reply.started": "2024-12-06T12:46:52.845709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:31:46.102865Z",
     "iopub.status.busy": "2024-12-06T08:31:46.102340Z",
     "iopub.status.idle": "2024-12-06T08:31:46.221087Z",
     "shell.execute_reply": "2024-12-06T08:31:46.220227Z",
     "shell.execute_reply.started": "2024-12-06T08:31:46.102829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, hidden_sizes=(64, 64)):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.discrete = discrete\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = state_dim\n",
    "\n",
    "        for hidden_dim in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        if self.discrete:\n",
    "            self.output = nn.Linear(input_dim, action_dim)\n",
    "        else:\n",
    "            self.mean = nn.Linear(input_dim, action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = torch.tanh(layer(x))\n",
    "        if self.discrete:\n",
    "            logits = self.output(x)\n",
    "            return torch.softmax(logits, dim=-1)\n",
    "        else:\n",
    "            mean = self.mean(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            return mean, std\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(64, 64)):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = state_dim\n",
    "\n",
    "        for hidden_dim in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.output = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = torch.tanh(layer(x))\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "class SAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, lr=4e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Actor and critic networks\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, discrete).to(self.device)\n",
    "        self.value_net = ValueNetwork(state_dim).to(self.device)\n",
    "\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        self.sgld_steps = sgld_steps\n",
    "        self.sgld_lr = sgld_lr\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if self.policy_net.discrete:\n",
    "            probs = self.policy_net(state)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            return action.item(), dist.log_prob(action)\n",
    "        else:\n",
    "            mean, std = self.policy_net(state)\n",
    "            dist = torch.distributions.Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            return action.cpu().numpy(), dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def sgld_step(self, state, epsilon):\n",
    "        \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "        perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "    \n",
    "        for _ in range(self.sgld_steps):\n",
    "            if perturbed_state.grad is not None:\n",
    "                perturbed_state.grad.zero_()\n",
    "    \n",
    "            # Compute KL divergence between original and perturbed policies\n",
    "            with torch.no_grad():\n",
    "                original_logits = self.policy_net(state)\n",
    "            perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "            if self.policy_net.discrete:\n",
    "                original_policy = dist.Categorical(original_logits)\n",
    "                perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "            else:\n",
    "                original_mean, original_std = original_logits\n",
    "                perturbed_mean, perturbed_std = perturbed_logits\n",
    "                original_policy = dist.Normal(original_mean, original_std)\n",
    "                perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "    \n",
    "            # Backpropagate KL divergence\n",
    "            kl_div.backward()\n",
    "    \n",
    "            # Update perturbed state using gradient and noise\n",
    "            perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "            perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "    \n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    def compute_kl_regularization(self, states, actions):\n",
    "        \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "        if len(states) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        kl_div_total = 0\n",
    "        for state in states:\n",
    "            perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                original_logits = self.policy_net(state)\n",
    "            perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "            if self.policy_net.discrete:\n",
    "                original_policy = dist.Categorical(original_logits)\n",
    "                perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "            else:\n",
    "                original_mean, original_std = original_logits\n",
    "                perturbed_mean, perturbed_std = perturbed_logits\n",
    "                original_policy = dist.Normal(original_mean, original_std)\n",
    "                perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "            kl_div_total += kl_div\n",
    "    \n",
    "        return kl_div_total / len(states)\n",
    "    \n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    \n",
    "            # Reset the environment\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "            # Rollout phase: Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                value = self.value_net(state).squeeze(0).detach()  # Detach the value tensor\n",
    "                action, log_prob = self.select_action(state.cpu().numpy())\n",
    "    \n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Append data to lists\n",
    "                states.append(state.clone().detach())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob.clone().detach())\n",
    "                values.append(value)\n",
    "    \n",
    "                # Update state\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "            # Add a final value estimate\n",
    "            values.append(torch.tensor([0], device=self.device).detach())\n",
    "    \n",
    "            # Compute advantages and returns\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "            # Convert lists to tensors\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(\n",
    "                np.array(actions),\n",
    "                dtype=torch.float32 if not self.policy_net.discrete else torch.long\n",
    "            ).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "    \n",
    "            # Optimization phase\n",
    "            for _ in range(self.k_epochs):\n",
    "                kl_reg = self.compute_kl_regularization(states, actions)\n",
    "    \n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i + batch_size]\n",
    "                    batch_actions = actions[i:i + batch_size]\n",
    "                    batch_log_probs = log_probs[i:i + batch_size]\n",
    "                    batch_advantages = advantages[i:i + batch_size]\n",
    "                    batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "                    if self.policy_net.discrete:\n",
    "                        action_probs = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Categorical(action_probs)\n",
    "                        new_log_probs = dist.log_prob(batch_actions)\n",
    "                    else:\n",
    "                        mean, std = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Normal(mean, std)\n",
    "                        new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "\n",
    "                    # Detach kl_reg to prevent graph accumulation\n",
    "                    kl_reg = kl_reg.detach()\n",
    "    \n",
    "                    total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg\n",
    "    \n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    total_loss.backward(retain_graph=False)  # No need to retain the graph here\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "    \n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n",
    "    \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T08:31:52.155683Z",
     "iopub.status.busy": "2024-12-06T08:31:52.154918Z",
     "iopub.status.idle": "2024-12-06T12:40:53.714120Z",
     "shell.execute_reply": "2024-12-06T12:40:53.713146Z",
     "shell.execute_reply.started": "2024-12-06T08:31:52.155647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = 0.6777732968330383, Value Loss = 6.672414779663086, KL Reg = 3.5286213915242115e-06\n",
      "Episode 2: Policy Loss = -0.6063567399978638, Value Loss = 7.908211708068848, KL Reg = 3.9623555494472384e-06\n",
      "Episode 3: Policy Loss = -0.8411083221435547, Value Loss = 6.700573444366455, KL Reg = 4.636903668142622e-06\n",
      "Episode 4: Policy Loss = 0.05862126499414444, Value Loss = 9.219980239868164, KL Reg = 5.3730004765384365e-06\n",
      "Episode 5: Policy Loss = 1.3558109998703003, Value Loss = 27.874187469482422, KL Reg = 6.476532234955812e-06\n",
      "Episode 6: Policy Loss = 2.3461716175079346, Value Loss = 39.928497314453125, KL Reg = 7.937432201288175e-06\n",
      "Episode 7: Policy Loss = -0.34775999188423157, Value Loss = 72.41099548339844, KL Reg = 8.599373359174933e-06\n",
      "Episode 8: Policy Loss = 3.537834882736206, Value Loss = 31.979164123535156, KL Reg = 1.002785393211525e-05\n",
      "Episode 9: Policy Loss = 2.8317785263061523, Value Loss = 45.44618606567383, KL Reg = 1.2716084711428266e-05\n",
      "Episode 10: Policy Loss = 1.5930187702178955, Value Loss = 76.86249542236328, KL Reg = 1.4450642993324436e-05\n",
      "Episode 11: Policy Loss = -17.907556533813477, Value Loss = 421.8785400390625, KL Reg = 1.5705149053246714e-05\n",
      "Episode 12: Policy Loss = -9.153806686401367, Value Loss = 176.31051635742188, KL Reg = 1.6381234672735445e-05\n",
      "Episode 13: Policy Loss = -0.7280039191246033, Value Loss = 129.69912719726562, KL Reg = 1.803689156076871e-05\n",
      "Episode 14: Policy Loss = 1.5074541568756104, Value Loss = 165.70932006835938, KL Reg = 2.0095236322958954e-05\n",
      "Episode 15: Policy Loss = 0.3667945861816406, Value Loss = 142.81057739257812, KL Reg = 2.2463036657427438e-05\n",
      "Episode 16: Policy Loss = 0.2034769058227539, Value Loss = 149.07290649414062, KL Reg = 2.4740373191889375e-05\n",
      "Episode 17: Policy Loss = -0.6694386005401611, Value Loss = 179.29763793945312, KL Reg = 2.508883881091606e-05\n",
      "Episode 18: Policy Loss = 5.679443359375, Value Loss = 101.12794494628906, KL Reg = 2.8433067200239748e-05\n",
      "Episode 19: Policy Loss = -9.119795799255371, Value Loss = 606.594970703125, KL Reg = 2.9240361982374452e-05\n",
      "Episode 20: Policy Loss = 4.350350379943848, Value Loss = 484.800537109375, KL Reg = 2.9923594411229715e-05\n",
      "Episode 21: Policy Loss = -0.39951372146606445, Value Loss = 613.3912353515625, KL Reg = 3.323099736007862e-05\n",
      "Episode 22: Policy Loss = 14.186182975769043, Value Loss = 557.844482421875, KL Reg = 3.4692602639552206e-05\n",
      "Episode 23: Policy Loss = 7.905609130859375, Value Loss = 326.63323974609375, KL Reg = 3.749625466298312e-05\n",
      "Episode 24: Policy Loss = 10.752996444702148, Value Loss = 427.82421875, KL Reg = 4.0293172787642106e-05\n",
      "Episode 25: Policy Loss = 11.898197174072266, Value Loss = 381.4997863769531, KL Reg = 4.138877920922823e-05\n",
      "Episode 26: Policy Loss = -3.4518675804138184, Value Loss = 912.927978515625, KL Reg = 4.270061253919266e-05\n",
      "Episode 27: Policy Loss = 8.899168968200684, Value Loss = 416.4952087402344, KL Reg = 4.8017376684583724e-05\n",
      "Episode 28: Policy Loss = -7.811315536499023, Value Loss = 647.0883178710938, KL Reg = 4.217109380988404e-05\n",
      "Episode 29: Policy Loss = -0.336942195892334, Value Loss = 914.4500732421875, KL Reg = 4.2932337237289175e-05\n",
      "Episode 30: Policy Loss = 18.70648193359375, Value Loss = 904.5438842773438, KL Reg = 4.597138104145415e-05\n",
      "Episode 31: Policy Loss = 9.909834861755371, Value Loss = 544.4332275390625, KL Reg = 4.379413076094352e-05\n",
      "Episode 32: Policy Loss = 14.47896957397461, Value Loss = 847.8314208984375, KL Reg = 4.840577094000764e-05\n",
      "Episode 33: Policy Loss = -1.0960607528686523, Value Loss = 880.47802734375, KL Reg = 5.226183566264808e-05\n",
      "Episode 34: Policy Loss = -0.09319686889648438, Value Loss = 550.478759765625, KL Reg = 5.0155100325355306e-05\n",
      "Episode 35: Policy Loss = 11.456897735595703, Value Loss = 678.3353271484375, KL Reg = 5.707070158678107e-05\n",
      "Episode 36: Policy Loss = 23.434139251708984, Value Loss = 1198.50927734375, KL Reg = 5.0191902118967846e-05\n",
      "Episode 37: Policy Loss = 18.206634521484375, Value Loss = 650.2852783203125, KL Reg = 5.3078794735483825e-05\n",
      "Episode 38: Policy Loss = 22.09394645690918, Value Loss = 1248.0699462890625, KL Reg = 5.768402843386866e-05\n",
      "Episode 39: Policy Loss = 3.99853515625, Value Loss = 1100.49072265625, KL Reg = 5.7428402215009555e-05\n",
      "Episode 40: Policy Loss = 5.1943135261535645, Value Loss = 610.462890625, KL Reg = 6.143462815089151e-05\n",
      "Episode 41: Policy Loss = 25.73589515686035, Value Loss = 1311.23095703125, KL Reg = 6.976113945711404e-05\n",
      "Episode 42: Policy Loss = -0.32715511322021484, Value Loss = 647.751953125, KL Reg = 6.786550511606038e-05\n",
      "Episode 43: Policy Loss = 18.51498794555664, Value Loss = 1119.5440673828125, KL Reg = 6.806790770497173e-05\n",
      "Episode 44: Policy Loss = 12.250843048095703, Value Loss = 702.7897338867188, KL Reg = 6.957068399060518e-05\n",
      "Episode 45: Policy Loss = 22.179288864135742, Value Loss = 1367.50537109375, KL Reg = 7.486913091270253e-05\n",
      "Episode 46: Policy Loss = 27.001100540161133, Value Loss = 1450.5712890625, KL Reg = 7.5998053944204e-05\n",
      "Episode 47: Policy Loss = 4.697323799133301, Value Loss = 709.2755126953125, KL Reg = 8.57835475471802e-05\n",
      "Episode 48: Policy Loss = 3.5550551414489746, Value Loss = 744.2655029296875, KL Reg = 8.539696864318103e-05\n",
      "Episode 49: Policy Loss = 2.3088226318359375, Value Loss = 728.8917236328125, KL Reg = 9.992741252062842e-05\n",
      "Episode 50: Policy Loss = 23.793434143066406, Value Loss = 1688.493408203125, KL Reg = 9.522836626274511e-05\n",
      "Episode 51: Policy Loss = 5.622803688049316, Value Loss = 777.0545043945312, KL Reg = 9.907027560984716e-05\n",
      "Episode 52: Policy Loss = 4.826170921325684, Value Loss = 730.0382080078125, KL Reg = 0.00011875166092067957\n",
      "Episode 53: Policy Loss = 16.664093017578125, Value Loss = 965.0621948242188, KL Reg = 0.0001069059144356288\n",
      "Episode 54: Policy Loss = 2.3867921829223633, Value Loss = 888.5385131835938, KL Reg = 0.00012414816592354327\n",
      "Episode 55: Policy Loss = 23.607574462890625, Value Loss = 1667.718994140625, KL Reg = 0.00011333648581057787\n",
      "Episode 56: Policy Loss = 25.53340721130371, Value Loss = 1657.7694091796875, KL Reg = 0.00011546549649210647\n",
      "Episode 57: Policy Loss = 21.090574264526367, Value Loss = 968.4852294921875, KL Reg = 0.0001222259015776217\n",
      "Episode 58: Policy Loss = 29.656795501708984, Value Loss = 1793.2716064453125, KL Reg = 0.0001292334054596722\n",
      "Episode 59: Policy Loss = 30.0056095123291, Value Loss = 1861.65283203125, KL Reg = 0.00012232967128511518\n",
      "Episode 60: Policy Loss = 32.461517333984375, Value Loss = 1922.6668701171875, KL Reg = 0.0001293384557357058\n",
      "Episode 61: Policy Loss = 13.740655899047852, Value Loss = 1009.9105224609375, KL Reg = 0.00013818006846122444\n",
      "Episode 62: Policy Loss = 22.03940200805664, Value Loss = 1829.1055908203125, KL Reg = 0.00014067832671571523\n",
      "Episode 63: Policy Loss = 19.56901741027832, Value Loss = 1169.32958984375, KL Reg = 0.0001402320631314069\n",
      "Episode 64: Policy Loss = 2.87636137008667, Value Loss = 961.60498046875, KL Reg = 0.00014630748773925006\n",
      "Episode 65: Policy Loss = 9.648176193237305, Value Loss = 562.861572265625, KL Reg = 0.00015555888239759952\n",
      "Episode 66: Policy Loss = 22.015750885009766, Value Loss = 1235.2874755859375, KL Reg = 0.0001363164046779275\n",
      "Episode 67: Policy Loss = 17.378009796142578, Value Loss = 1082.433349609375, KL Reg = 0.00013259434490464628\n",
      "Episode 68: Policy Loss = 22.26970672607422, Value Loss = 457.7964782714844, KL Reg = 0.00013898855831939727\n",
      "Episode 69: Policy Loss = 20.61141014099121, Value Loss = 1222.806640625, KL Reg = 0.00013262122229207307\n",
      "Episode 70: Policy Loss = 7.3852219581604, Value Loss = 1026.9559326171875, KL Reg = 0.00012403170694597065\n",
      "Episode 71: Policy Loss = 29.12413215637207, Value Loss = 1767.438232421875, KL Reg = 0.00013221568951848894\n",
      "Episode 72: Policy Loss = 20.417997360229492, Value Loss = 1241.228759765625, KL Reg = 0.00013235841470304877\n",
      "Episode 73: Policy Loss = 19.217144012451172, Value Loss = 1216.32373046875, KL Reg = 0.00013165429118089378\n",
      "Episode 74: Policy Loss = 20.250688552856445, Value Loss = 1286.0751953125, KL Reg = 0.00013618526281788945\n",
      "Episode 75: Policy Loss = 24.3013973236084, Value Loss = 1869.2662353515625, KL Reg = 0.00013763914466835558\n",
      "Episode 76: Policy Loss = 9.066633224487305, Value Loss = 986.49658203125, KL Reg = 0.00016780270379967988\n",
      "Episode 77: Policy Loss = 4.460693836212158, Value Loss = 1031.9810791015625, KL Reg = 0.0001600364048499614\n",
      "Episode 78: Policy Loss = 31.480579376220703, Value Loss = 2118.611572265625, KL Reg = 0.00016842028708197176\n",
      "Episode 79: Policy Loss = 4.047419548034668, Value Loss = 961.0020751953125, KL Reg = 0.0001895003078971058\n",
      "Episode 80: Policy Loss = 16.507099151611328, Value Loss = 1131.8197021484375, KL Reg = 0.00019719266856554896\n",
      "Episode 81: Policy Loss = 26.071653366088867, Value Loss = 1683.55224609375, KL Reg = 0.00019211455946788192\n",
      "Episode 82: Policy Loss = 7.867403030395508, Value Loss = 1001.2107543945312, KL Reg = 0.00019483875075820833\n",
      "Episode 83: Policy Loss = 16.766712188720703, Value Loss = 1154.0234375, KL Reg = 0.00019338619313202798\n",
      "Episode 84: Policy Loss = 36.596275329589844, Value Loss = 2111.5732421875, KL Reg = 0.00021204660879448056\n",
      "Episode 85: Policy Loss = 31.557716369628906, Value Loss = 2122.384033203125, KL Reg = 0.0002178862487198785\n",
      "Episode 86: Policy Loss = 35.68231201171875, Value Loss = 2299.5244140625, KL Reg = 0.00021494309476111084\n",
      "Episode 87: Policy Loss = 22.631603240966797, Value Loss = 1398.839599609375, KL Reg = 0.00022252689814195037\n",
      "Episode 88: Policy Loss = 20.86971664428711, Value Loss = 1318.59130859375, KL Reg = 0.00021692139853257686\n",
      "Episode 89: Policy Loss = 21.68468475341797, Value Loss = 1347.1968994140625, KL Reg = 0.00020581134594976902\n",
      "Episode 90: Policy Loss = 28.34797477722168, Value Loss = 1633.56787109375, KL Reg = 0.00021443754667416215\n",
      "Episode 91: Policy Loss = 24.367599487304688, Value Loss = 1448.0704345703125, KL Reg = 0.00020635568944271654\n",
      "Episode 92: Policy Loss = 12.505571365356445, Value Loss = 1014.7616577148438, KL Reg = 0.00020002899691462517\n",
      "Episode 93: Policy Loss = 16.330942153930664, Value Loss = 1137.79150390625, KL Reg = 0.00019685193547047675\n",
      "Episode 94: Policy Loss = 31.34489631652832, Value Loss = 2039.82177734375, KL Reg = 0.0002138106501661241\n",
      "Episode 95: Policy Loss = 22.344297409057617, Value Loss = 1462.8861083984375, KL Reg = 0.0001897138135973364\n",
      "Episode 96: Policy Loss = 19.008739471435547, Value Loss = 1152.3333740234375, KL Reg = 0.00020146722090430558\n",
      "Episode 97: Policy Loss = 13.87155818939209, Value Loss = 1571.424072265625, KL Reg = 0.00022680395341012627\n",
      "Episode 98: Policy Loss = 15.664544105529785, Value Loss = 1171.9483642578125, KL Reg = 0.00024277671764139086\n",
      "Episode 99: Policy Loss = 26.212574005126953, Value Loss = 1511.245849609375, KL Reg = 0.00028189129079692066\n",
      "Episode 100: Policy Loss = 16.541698455810547, Value Loss = 1444.110595703125, KL Reg = 0.00028309650951996446\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    RobustAgent = SAPPOAgent(state_dim, action_dim, discrete)\n",
    "    RobustAgent.train(env, max_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:47:00.855945Z",
     "iopub.status.busy": "2024-12-06T12:47:00.855610Z",
     "iopub.status.idle": "2024-12-06T12:47:24.569902Z",
     "shell.execute_reply": "2024-12-06T12:47:24.569098Z",
     "shell.execute_reply.started": "2024-12-06T12:47:00.855915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 339.401879035104\n",
      "Episode 2: Reward = 285.1441981154519\n",
      "Episode 3: Reward = 324.313798867123\n",
      "Episode 4: Reward = 290.17250325770885\n",
      "Episode 5: Reward = 417.4087682719066\n",
      "Episode 6: Reward = 264.02180846051357\n",
      "Episode 7: Reward = 294.980749784987\n",
      "Episode 8: Reward = 354.016271654072\n",
      "Episode 9: Reward = 117.62726944169935\n",
      "Episode 10: Reward = 343.71253172566236\n",
      "Episode 11: Reward = 341.938019086542\n",
      "Episode 12: Reward = 305.9787617849171\n",
      "Episode 13: Reward = 330.08123306850064\n",
      "Episode 14: Reward = 89.38297983490634\n",
      "Episode 15: Reward = 239.43243267121798\n",
      "Episode 16: Reward = 275.75938962657887\n",
      "Episode 17: Reward = 238.66013541181374\n",
      "Episode 18: Reward = 191.11645484634258\n",
      "Episode 19: Reward = 218.0120458839302\n",
      "Episode 20: Reward = 331.42242725501075\n",
      "Episode 21: Reward = 163.97806757820987\n",
      "Episode 22: Reward = 399.0547938482045\n",
      "Episode 23: Reward = 265.8970989304789\n",
      "Episode 24: Reward = 294.14903509341417\n",
      "Episode 25: Reward = 419.6688285010828\n",
      "Episode 26: Reward = 411.9556824549326\n",
      "Episode 27: Reward = 309.28040259319664\n",
      "Episode 28: Reward = 324.64977304711806\n",
      "Episode 29: Reward = 276.0905143500562\n",
      "Episode 30: Reward = 255.24603773909922\n",
      "Episode 31: Reward = 268.9768873376224\n",
      "Episode 32: Reward = 438.6097010507179\n",
      "Episode 33: Reward = 298.6877509082806\n",
      "Episode 34: Reward = 379.38709312441733\n",
      "Episode 35: Reward = 320.9367491045225\n",
      "Episode 36: Reward = 256.1658570788593\n",
      "Episode 37: Reward = 342.18159475983305\n",
      "Episode 38: Reward = 294.62665425971994\n",
      "Episode 39: Reward = 328.9218425104692\n",
      "Episode 40: Reward = 374.1508034546117\n",
      "Episode 41: Reward = 294.8561615236876\n",
      "Episode 42: Reward = 318.5515772184553\n",
      "Episode 43: Reward = 262.0392937229405\n",
      "Episode 44: Reward = 356.2963563755073\n",
      "Episode 45: Reward = 241.51219593174784\n",
      "Episode 46: Reward = 438.77747859963955\n",
      "Episode 47: Reward = 299.1604045691584\n",
      "Episode 48: Reward = 331.99895772999287\n",
      "Episode 49: Reward = 113.39040641849458\n",
      "Episode 50: Reward = 243.95422447497626\n",
      "Episode 51: Reward = 305.9324451166875\n",
      "Episode 52: Reward = 307.2438845233154\n",
      "Episode 53: Reward = 307.05733645519564\n",
      "Episode 54: Reward = 357.1712910478931\n",
      "Episode 55: Reward = 251.97142012739224\n",
      "Episode 56: Reward = 276.5973527380391\n",
      "Episode 57: Reward = 348.14692823912065\n",
      "Episode 58: Reward = 239.18673901631254\n",
      "Episode 59: Reward = 372.0214283311019\n",
      "Episode 60: Reward = 307.0699514256313\n",
      "Episode 61: Reward = 287.5104047004281\n",
      "Episode 62: Reward = 240.44930011756043\n",
      "Episode 63: Reward = 250.26316624036315\n",
      "Episode 64: Reward = 304.00702713511885\n",
      "Episode 65: Reward = 176.29711129019418\n",
      "Episode 66: Reward = 355.20301850106773\n",
      "Episode 67: Reward = 338.46819017639774\n",
      "Episode 68: Reward = 272.54725382053493\n",
      "Episode 69: Reward = 326.99233952339785\n",
      "Episode 70: Reward = 270.290129496905\n",
      "Episode 71: Reward = 356.8303448373241\n",
      "Episode 72: Reward = 415.83335345902685\n",
      "Episode 73: Reward = 298.05101085150477\n",
      "Episode 74: Reward = 251.49472594073157\n",
      "Episode 75: Reward = 312.32128874700965\n",
      "Episode 76: Reward = 292.9790527389982\n",
      "Episode 77: Reward = 334.9364638751646\n",
      "Episode 78: Reward = 279.67366748847866\n",
      "Episode 79: Reward = 245.62593501691646\n",
      "Episode 80: Reward = 245.89588006465678\n",
      "Episode 81: Reward = 294.0397776761726\n",
      "Episode 82: Reward = 358.6240323004675\n",
      "Episode 83: Reward = 245.73568927478848\n",
      "Episode 84: Reward = 381.81357301269026\n",
      "Episode 85: Reward = 335.15463263275996\n",
      "Episode 86: Reward = 325.4661313499231\n",
      "Episode 87: Reward = 354.06121706085565\n",
      "Episode 88: Reward = 265.8784192334209\n",
      "Episode 89: Reward = 267.15285164445123\n",
      "Episode 90: Reward = 317.777078851508\n",
      "Episode 91: Reward = 273.209448546485\n",
      "Episode 92: Reward = 265.4719253748167\n",
      "Episode 93: Reward = 304.7678013570683\n",
      "Episode 94: Reward = 234.17676614200292\n",
      "Episode 95: Reward = 374.9681390512481\n",
      "Episode 96: Reward = 320.48070816387946\n",
      "Episode 97: Reward = 291.27552162781757\n",
      "Episode 98: Reward = 415.07722805797687\n",
      "Episode 99: Reward = 206.65013717657268\n",
      "Episode 100: Reward = 276.6142177853759\n",
      "Episode 101: Reward = 210.49924727175792\n",
      "Episode 102: Reward = 247.85802081270177\n",
      "Episode 103: Reward = 335.738282446287\n",
      "Episode 104: Reward = 309.4799228768262\n",
      "Episode 105: Reward = 308.44957606861277\n",
      "Episode 106: Reward = 319.544918153816\n",
      "Episode 107: Reward = 228.45820729406037\n",
      "Episode 108: Reward = 164.35234244011056\n",
      "Episode 109: Reward = 344.76415082460136\n",
      "Episode 110: Reward = 328.0880703405294\n",
      "Episode 111: Reward = 307.4859140032482\n",
      "Episode 112: Reward = 355.3058407137509\n",
      "Episode 113: Reward = 292.34750310157926\n",
      "Episode 114: Reward = 369.59393010559376\n",
      "Episode 115: Reward = 286.04539818734816\n",
      "Episode 116: Reward = 149.63255560842217\n",
      "Episode 117: Reward = 239.56320129208618\n",
      "Episode 118: Reward = 311.42699250688344\n",
      "Episode 119: Reward = 396.5499520154468\n",
      "Episode 120: Reward = 335.8132454265389\n",
      "Episode 121: Reward = 301.63445500310655\n",
      "Episode 122: Reward = 101.60832021405547\n",
      "Episode 123: Reward = 281.95091168865366\n",
      "Episode 124: Reward = 357.80560940045683\n",
      "Episode 125: Reward = 30.170422372874835\n",
      "Episode 126: Reward = 324.7999858334797\n",
      "Episode 127: Reward = 258.2374847962034\n",
      "Episode 128: Reward = 305.6258005988957\n",
      "Episode 129: Reward = 394.22415294981397\n",
      "Episode 130: Reward = 306.04836541001833\n",
      "Episode 131: Reward = 275.8506422423918\n",
      "Episode 132: Reward = 100.64929687792112\n",
      "Episode 133: Reward = 326.6949532103078\n",
      "Episode 134: Reward = 262.57836309794794\n",
      "Episode 135: Reward = 269.1894672600088\n",
      "Episode 136: Reward = 224.0954983691596\n",
      "Episode 137: Reward = 312.44341891184234\n",
      "Episode 138: Reward = 337.7796755204395\n",
      "Episode 139: Reward = 301.7981169113931\n",
      "Episode 140: Reward = 350.7542942505093\n",
      "Episode 141: Reward = 204.77210555448661\n",
      "Episode 142: Reward = 304.1434742010226\n",
      "Episode 143: Reward = 341.9074854224066\n",
      "Episode 144: Reward = 310.9349182392553\n",
      "Episode 145: Reward = 248.93290266385546\n",
      "Episode 146: Reward = 151.54326137756703\n",
      "Episode 147: Reward = 417.82793688457707\n",
      "Episode 148: Reward = 383.45996652959946\n",
      "Episode 149: Reward = 393.29433552128614\n",
      "Episode 150: Reward = 326.17595802537096\n",
      "Episode 151: Reward = 260.57358859358396\n",
      "Episode 152: Reward = 471.4029935627067\n",
      "Episode 153: Reward = 370.4015308708974\n",
      "Episode 154: Reward = 299.82231667588854\n",
      "Episode 155: Reward = 77.23675542150085\n",
      "Episode 156: Reward = 428.5359178930605\n",
      "Episode 157: Reward = 275.55472339587527\n",
      "Episode 158: Reward = 319.43561487536834\n",
      "Episode 159: Reward = 160.9748588626693\n",
      "Episode 160: Reward = 484.3486769641402\n",
      "Episode 161: Reward = 450.6962083920167\n",
      "Episode 162: Reward = 256.4775755534097\n",
      "Episode 163: Reward = 362.2339538397455\n",
      "Episode 164: Reward = 307.5478530452672\n",
      "Episode 165: Reward = 288.6945811512365\n",
      "Episode 166: Reward = 273.8071291270045\n",
      "Episode 167: Reward = 216.19843429484354\n",
      "Episode 168: Reward = 327.1288722292156\n",
      "Episode 169: Reward = 297.8270468021791\n",
      "Episode 170: Reward = 306.7551854766162\n",
      "Episode 171: Reward = 317.8052566731329\n",
      "Episode 172: Reward = 279.70573727907663\n",
      "Episode 173: Reward = 341.4427018815512\n",
      "Episode 174: Reward = 402.4297888841595\n",
      "Episode 175: Reward = 333.61471151018367\n",
      "Episode 176: Reward = 315.88079707565964\n",
      "Episode 177: Reward = 303.80404310725373\n",
      "Episode 178: Reward = 312.524934263643\n",
      "Episode 179: Reward = 102.15554221562986\n",
      "Episode 180: Reward = 214.5907000365973\n",
      "Episode 181: Reward = 292.70283380639916\n",
      "Episode 182: Reward = 459.82603003513475\n",
      "Episode 183: Reward = 306.6273406187159\n",
      "Episode 184: Reward = 304.55629584188796\n",
      "Episode 185: Reward = 241.94448031470816\n",
      "Episode 186: Reward = 383.87830216085433\n",
      "Episode 187: Reward = 258.89903215956883\n",
      "Episode 188: Reward = 333.04933027549106\n",
      "Episode 189: Reward = 231.65008700261905\n",
      "Episode 190: Reward = 366.3330817753868\n",
      "Episode 191: Reward = 347.4967899315225\n",
      "Episode 192: Reward = 372.51287347172354\n",
      "Episode 193: Reward = 89.05797496675157\n",
      "Episode 194: Reward = 316.1871003928217\n",
      "Episode 195: Reward = 350.06690348850765\n",
      "Episode 196: Reward = 298.71502442749744\n",
      "Episode 197: Reward = 382.27848252399997\n",
      "Episode 198: Reward = 269.548837375552\n",
      "Episode 199: Reward = 340.2147752112664\n",
      "Episode 200: Reward = 365.55185645522613\n",
      "Average Reward over 200 Episodes: 298.8541696637751\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, RobustAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:48:34.653449Z",
     "iopub.status.busy": "2024-12-06T12:48:34.652662Z",
     "iopub.status.idle": "2024-12-06T12:48:52.501603Z",
     "shell.execute_reply": "2024-12-06T12:48:52.500812Z",
     "shell.execute_reply.started": "2024-12-06T12:48:34.653404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 350.973517500949\n",
      "Episode 2: Reward = 343.1103180235472\n",
      "Episode 3: Reward = 365.6246840596118\n",
      "Episode 4: Reward = 337.9951893348273\n",
      "Episode 5: Reward = 361.85833884253896\n",
      "Episode 6: Reward = 341.37784120165446\n",
      "Episode 7: Reward = 341.94729580442134\n",
      "Episode 8: Reward = 360.2953576968117\n",
      "Episode 9: Reward = 327.6110628830733\n",
      "Episode 10: Reward = 377.8520711065494\n",
      "Episode 11: Reward = 381.85974077557574\n",
      "Episode 12: Reward = 359.8215496553415\n",
      "Episode 13: Reward = 340.95530586149965\n",
      "Episode 14: Reward = 357.93363049947845\n",
      "Episode 15: Reward = 346.13525455946285\n",
      "Episode 16: Reward = 352.27469499799037\n",
      "Episode 17: Reward = 350.00224145503745\n",
      "Episode 18: Reward = 331.53317100859493\n",
      "Episode 19: Reward = 369.7353718325735\n",
      "Episode 20: Reward = 381.63530152768305\n",
      "Episode 21: Reward = 344.7138894802823\n",
      "Episode 22: Reward = 343.6928298357322\n",
      "Episode 23: Reward = 358.463327873662\n",
      "Episode 24: Reward = 380.3329209039133\n",
      "Episode 25: Reward = 368.0239199043304\n",
      "Episode 26: Reward = 391.4192012521282\n",
      "Episode 27: Reward = 370.9858278675228\n",
      "Episode 28: Reward = 338.892919255084\n",
      "Episode 29: Reward = 367.536246983472\n",
      "Episode 30: Reward = 351.2453171465972\n",
      "Episode 31: Reward = 360.1816238750443\n",
      "Episode 32: Reward = 378.04023422997994\n",
      "Episode 33: Reward = 368.3325901337732\n",
      "Episode 34: Reward = 368.7109205593478\n",
      "Episode 35: Reward = 330.4059837509798\n",
      "Episode 36: Reward = 349.6485817932138\n",
      "Episode 37: Reward = 339.2650596482284\n",
      "Episode 38: Reward = 335.2418723591796\n",
      "Episode 39: Reward = 364.06321974838903\n",
      "Episode 40: Reward = 329.7588115969187\n",
      "Episode 41: Reward = 355.66022079802445\n",
      "Episode 42: Reward = 355.99830164213586\n",
      "Episode 43: Reward = 367.50864181858964\n",
      "Episode 44: Reward = 332.4163513697782\n",
      "Episode 45: Reward = 328.5697043616723\n",
      "Episode 46: Reward = 362.8176793210464\n",
      "Episode 47: Reward = 371.58076362096\n",
      "Episode 48: Reward = 350.8495704154692\n",
      "Episode 49: Reward = 346.32226083295086\n",
      "Episode 50: Reward = 387.3936906907182\n",
      "Episode 51: Reward = 361.211104012214\n",
      "Episode 52: Reward = 360.3405325123937\n",
      "Episode 53: Reward = 331.7279979144171\n",
      "Episode 54: Reward = 344.92481995101673\n",
      "Episode 55: Reward = 373.7810699195156\n",
      "Episode 56: Reward = 374.47516320792965\n",
      "Episode 57: Reward = 370.0817923136593\n",
      "Episode 58: Reward = 366.0799217352182\n",
      "Episode 59: Reward = 374.013509295748\n",
      "Episode 60: Reward = 336.9335435739299\n",
      "Episode 61: Reward = 331.25502576964544\n",
      "Episode 62: Reward = 370.7699292205031\n",
      "Episode 63: Reward = 331.46653480102646\n",
      "Episode 64: Reward = 356.3534353011042\n",
      "Episode 65: Reward = 332.60368746265885\n",
      "Episode 66: Reward = 353.05654417636606\n",
      "Episode 67: Reward = 369.11893542810543\n",
      "Episode 68: Reward = 359.3913456718639\n",
      "Episode 69: Reward = 375.8133586971058\n",
      "Episode 70: Reward = 364.0599489945144\n",
      "Episode 71: Reward = 376.2249856078625\n",
      "Episode 72: Reward = 371.7354995088592\n",
      "Episode 73: Reward = 349.01455452025726\n",
      "Episode 74: Reward = 348.8167478501256\n",
      "Episode 75: Reward = 334.45804804742016\n",
      "Episode 76: Reward = 332.3039898086829\n",
      "Episode 77: Reward = 333.4854038675512\n",
      "Episode 78: Reward = 330.8752745433809\n",
      "Episode 79: Reward = 360.3461387747929\n",
      "Episode 80: Reward = 341.50445243361094\n",
      "Episode 81: Reward = 350.2346128420196\n",
      "Episode 82: Reward = 356.83761594219754\n",
      "Episode 83: Reward = 343.8174290464521\n",
      "Episode 84: Reward = 378.3225016406636\n",
      "Episode 85: Reward = 338.9377942532345\n",
      "Episode 86: Reward = 342.16461942870137\n",
      "Episode 87: Reward = 368.82459397149154\n",
      "Episode 88: Reward = 373.05444753748594\n",
      "Episode 89: Reward = 373.49348104359194\n",
      "Episode 90: Reward = 359.56588796815663\n",
      "Episode 91: Reward = 339.8572238416461\n",
      "Episode 92: Reward = 348.91917363232983\n",
      "Episode 93: Reward = 370.82931788529606\n",
      "Episode 94: Reward = 335.14123790895036\n",
      "Episode 95: Reward = 372.151426081893\n",
      "Episode 96: Reward = 366.9141566832167\n",
      "Episode 97: Reward = 338.2794926743585\n",
      "Episode 98: Reward = 367.4613035943239\n",
      "Episode 99: Reward = 370.2765981675223\n",
      "Episode 100: Reward = 356.5616580169088\n",
      "Episode 101: Reward = 360.86298324896325\n",
      "Episode 102: Reward = 371.12615766451233\n",
      "Episode 103: Reward = 364.2609777742823\n",
      "Episode 104: Reward = 335.11239518582744\n",
      "Episode 105: Reward = 334.11593236242715\n",
      "Episode 106: Reward = 387.2676837188239\n",
      "Episode 107: Reward = 358.65168878804525\n",
      "Episode 108: Reward = 344.72572321744013\n",
      "Episode 109: Reward = 368.0880668610597\n",
      "Episode 110: Reward = 364.9332626414143\n",
      "Episode 111: Reward = 355.3020950630419\n",
      "Episode 112: Reward = 383.030453546469\n",
      "Episode 113: Reward = 374.02442716752313\n",
      "Episode 114: Reward = 364.3957278411589\n",
      "Episode 115: Reward = 360.8252211630382\n",
      "Episode 116: Reward = 362.22112418009044\n",
      "Episode 117: Reward = 354.2457441463811\n",
      "Episode 118: Reward = 373.977198786531\n",
      "Episode 119: Reward = 350.1149219997582\n",
      "Episode 120: Reward = 360.2615894391931\n",
      "Episode 121: Reward = 382.54930933101997\n",
      "Episode 122: Reward = 371.8463792344792\n",
      "Episode 123: Reward = 351.2747198140874\n",
      "Episode 124: Reward = 345.44742205545157\n",
      "Episode 125: Reward = 371.11509349961955\n",
      "Episode 126: Reward = 370.61755935564685\n",
      "Episode 127: Reward = 352.8364190144776\n",
      "Episode 128: Reward = 370.67437063515206\n",
      "Episode 129: Reward = 349.66418116788157\n",
      "Episode 130: Reward = 332.3484052555248\n",
      "Episode 131: Reward = 370.241964901406\n",
      "Episode 132: Reward = 338.3452096529433\n",
      "Episode 133: Reward = 379.56585269806266\n",
      "Episode 134: Reward = 367.9439791864819\n",
      "Episode 135: Reward = 354.0118529731356\n",
      "Episode 136: Reward = 346.5875039069101\n",
      "Episode 137: Reward = 365.673932463646\n",
      "Episode 138: Reward = 361.45894184244486\n",
      "Episode 139: Reward = 380.7225828955785\n",
      "Episode 140: Reward = 354.284825420967\n",
      "Episode 141: Reward = 356.64765611755274\n",
      "Episode 142: Reward = 333.32965022667037\n",
      "Episode 143: Reward = 324.65003588156014\n",
      "Episode 144: Reward = 386.9197663547387\n",
      "Episode 145: Reward = 329.48849576545746\n",
      "Episode 146: Reward = 359.7542933692491\n",
      "Episode 147: Reward = 364.1103020800116\n",
      "Episode 148: Reward = 386.4080363874076\n",
      "Episode 149: Reward = 353.4281855208375\n",
      "Episode 150: Reward = 354.6599240485073\n",
      "Episode 151: Reward = 364.1570645049333\n",
      "Episode 152: Reward = 340.3188748037113\n",
      "Episode 153: Reward = 365.85186575556946\n",
      "Episode 154: Reward = 324.51174131481554\n",
      "Episode 155: Reward = 376.2642235683087\n",
      "Episode 156: Reward = 344.8060917970484\n",
      "Episode 157: Reward = 349.2795372703706\n",
      "Episode 158: Reward = 350.32185109051983\n",
      "Episode 159: Reward = 340.512724320829\n",
      "Episode 160: Reward = 358.2771201036733\n",
      "Episode 161: Reward = 357.8713510766519\n",
      "Episode 162: Reward = 326.5063228411967\n",
      "Episode 163: Reward = 372.966969369833\n",
      "Episode 164: Reward = 359.99604003205525\n",
      "Episode 165: Reward = 329.38484484400124\n",
      "Episode 166: Reward = 351.5798844222737\n",
      "Episode 167: Reward = 369.87425234604484\n",
      "Episode 168: Reward = 347.0629171134809\n",
      "Episode 169: Reward = 365.9638909186812\n",
      "Episode 170: Reward = 335.994627208027\n",
      "Episode 171: Reward = 338.53311671521385\n",
      "Episode 172: Reward = 342.45116731536973\n",
      "Episode 173: Reward = 351.26258460139053\n",
      "Episode 174: Reward = 375.70618104688083\n",
      "Episode 175: Reward = 346.0319447760374\n",
      "Episode 176: Reward = 338.35287241256987\n",
      "Episode 177: Reward = 339.7771347435838\n",
      "Episode 178: Reward = 359.4588973787254\n",
      "Episode 179: Reward = 366.6310578510134\n",
      "Episode 180: Reward = 373.1113791922594\n",
      "Episode 181: Reward = 340.75179040363116\n",
      "Episode 182: Reward = 351.7158531662273\n",
      "Episode 183: Reward = 357.1875100324919\n",
      "Episode 184: Reward = 363.32300762495265\n",
      "Episode 185: Reward = 343.1404403125346\n",
      "Episode 186: Reward = 377.1065508547842\n",
      "Episode 187: Reward = 351.9584229741093\n",
      "Episode 188: Reward = 357.3115379756177\n",
      "Episode 189: Reward = 365.53543201876494\n",
      "Episode 190: Reward = 354.39611852241825\n",
      "Episode 191: Reward = 353.16986523400806\n",
      "Episode 192: Reward = 341.99877799665995\n",
      "Episode 193: Reward = 353.66158526204305\n",
      "Episode 194: Reward = 346.82383467156353\n",
      "Episode 195: Reward = 373.7789124491655\n",
      "Episode 196: Reward = 339.6064089891076\n",
      "Episode 197: Reward = 365.5867879642364\n",
      "Episode 198: Reward = 351.43954777272717\n",
      "Episode 199: Reward = 352.04694617289135\n",
      "Episode 200: Reward = 354.0076804057466\n",
      "Average Reward over 200 episodes: 355.9899402808497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "355.9899402808497"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T12:49:58.538998Z",
     "iopub.status.busy": "2024-12-06T12:49:58.538291Z",
     "iopub.status.idle": "2024-12-06T13:03:23.707450Z",
     "shell.execute_reply": "2024-12-06T13:03:23.706587Z",
     "shell.execute_reply.started": "2024-12-06T12:49:58.538964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 333.4317832219636\n",
      "Episode 2/200: Reward = 403.72559388025246\n",
      "Episode 3/200: Reward = 288.42368709131983\n",
      "Episode 4/200: Reward = 267.04737567181104\n",
      "Episode 5/200: Reward = 263.5538508974242\n",
      "Episode 6/200: Reward = 250.55200998067826\n",
      "Episode 7/200: Reward = 358.8898825815961\n",
      "Episode 8/200: Reward = 300.79142638991283\n",
      "Episode 9/200: Reward = 234.8793379217863\n",
      "Episode 10/200: Reward = 265.1251243401611\n",
      "Episode 11/200: Reward = 291.61135121612506\n",
      "Episode 12/200: Reward = 476.16353905745456\n",
      "Episode 13/200: Reward = 322.82956552572693\n",
      "Episode 14/200: Reward = 334.260675523603\n",
      "Episode 15/200: Reward = 257.6875999735629\n",
      "Episode 16/200: Reward = 352.9523528646386\n",
      "Episode 17/200: Reward = 428.13693973063306\n",
      "Episode 18/200: Reward = 278.9556502046077\n",
      "Episode 19/200: Reward = 289.0489655864593\n",
      "Episode 20/200: Reward = 398.71319382004367\n",
      "Episode 21/200: Reward = 385.33349676335024\n",
      "Episode 22/200: Reward = 419.10361637893345\n",
      "Episode 23/200: Reward = 324.0180714886811\n",
      "Episode 24/200: Reward = 260.3558991452891\n",
      "Episode 25/200: Reward = 247.98621691004604\n",
      "Episode 26/200: Reward = 103.48256926069597\n",
      "Episode 27/200: Reward = 280.47544480925495\n",
      "Episode 28/200: Reward = 257.423439987817\n",
      "Episode 29/200: Reward = 294.08039168264906\n",
      "Episode 30/200: Reward = 322.67756852502833\n",
      "Episode 31/200: Reward = 295.65474553602985\n",
      "Episode 32/200: Reward = 30.958443372298248\n",
      "Episode 33/200: Reward = 335.6355386085308\n",
      "Episode 34/200: Reward = 256.1520803570666\n",
      "Episode 35/200: Reward = 295.5030171727758\n",
      "Episode 36/200: Reward = 328.469539480947\n",
      "Episode 37/200: Reward = 218.73463600385497\n",
      "Episode 38/200: Reward = 334.110565822499\n",
      "Episode 39/200: Reward = 419.75906298801215\n",
      "Episode 40/200: Reward = 209.04571475121276\n",
      "Episode 41/200: Reward = 277.0873003745535\n",
      "Episode 42/200: Reward = 266.8731927010562\n",
      "Episode 43/200: Reward = 228.9560624485864\n",
      "Episode 44/200: Reward = 455.7195458249184\n",
      "Episode 45/200: Reward = 383.40757917271696\n",
      "Episode 46/200: Reward = 320.8232822006541\n",
      "Episode 47/200: Reward = 391.0273586852522\n",
      "Episode 48/200: Reward = 301.1536551022839\n",
      "Episode 49/200: Reward = 248.72074177824726\n",
      "Episode 50/200: Reward = 392.59462690330184\n",
      "Episode 51/200: Reward = 291.2096349053607\n",
      "Episode 52/200: Reward = 292.64989880800937\n",
      "Episode 53/200: Reward = 216.28278130117033\n",
      "Episode 54/200: Reward = 378.7936260926773\n",
      "Episode 55/200: Reward = 471.6859408013238\n",
      "Episode 56/200: Reward = 273.4560581032944\n",
      "Episode 57/200: Reward = 280.92241063025034\n",
      "Episode 58/200: Reward = 292.35019701509754\n",
      "Episode 59/200: Reward = 389.4768356859446\n",
      "Episode 60/200: Reward = 327.63779284382406\n",
      "Episode 61/200: Reward = 226.5977292438541\n",
      "Episode 62/200: Reward = 334.3364441764541\n",
      "Episode 63/200: Reward = 271.90445263997594\n",
      "Episode 64/200: Reward = 251.99627617660317\n",
      "Episode 65/200: Reward = 329.511889544282\n",
      "Episode 66/200: Reward = 330.5761749888802\n",
      "Episode 67/200: Reward = 297.73581512826064\n",
      "Episode 68/200: Reward = 394.8183606512036\n",
      "Episode 69/200: Reward = 268.7385931925118\n",
      "Episode 70/200: Reward = 326.9903127706233\n",
      "Episode 71/200: Reward = 294.97134358304896\n",
      "Episode 72/200: Reward = 305.48575563584683\n",
      "Episode 73/200: Reward = 346.23080806519\n",
      "Episode 74/200: Reward = 321.7339725546645\n",
      "Episode 75/200: Reward = 366.89224966680666\n",
      "Episode 76/200: Reward = 265.53418691801875\n",
      "Episode 77/200: Reward = 288.5989755282435\n",
      "Episode 78/200: Reward = 335.12101734723154\n",
      "Episode 79/200: Reward = 315.2030836692649\n",
      "Episode 80/200: Reward = 137.72628655836817\n",
      "Episode 81/200: Reward = 543.4188708086375\n",
      "Episode 82/200: Reward = 278.18559823262643\n",
      "Episode 83/200: Reward = 436.86467823865763\n",
      "Episode 84/200: Reward = 308.27945943500157\n",
      "Episode 85/200: Reward = 261.4520654228926\n",
      "Episode 86/200: Reward = 373.0562622017582\n",
      "Episode 87/200: Reward = 334.42957064891885\n",
      "Episode 88/200: Reward = 307.29958170190156\n",
      "Episode 89/200: Reward = 212.3634988144173\n",
      "Episode 90/200: Reward = 251.44273435855587\n",
      "Episode 91/200: Reward = 290.60721667317654\n",
      "Episode 92/200: Reward = 384.6684951902945\n",
      "Episode 93/200: Reward = 408.0717638882478\n",
      "Episode 94/200: Reward = 247.19978500432373\n",
      "Episode 95/200: Reward = 250.08002233390968\n",
      "Episode 96/200: Reward = 250.2048171507857\n",
      "Episode 97/200: Reward = 257.5131668982532\n",
      "Episode 98/200: Reward = 394.8689964652561\n",
      "Episode 99/200: Reward = 326.11204846325893\n",
      "Episode 100/200: Reward = 298.1424344467028\n",
      "Episode 101/200: Reward = 307.7091821786307\n",
      "Episode 102/200: Reward = 271.8008709043885\n",
      "Episode 103/200: Reward = 329.8157638858055\n",
      "Episode 104/200: Reward = 243.18970256310303\n",
      "Episode 105/200: Reward = 152.5459637586499\n",
      "Episode 106/200: Reward = 323.4205828057038\n",
      "Episode 107/200: Reward = 318.8722337024035\n",
      "Episode 108/200: Reward = 274.45643523984813\n",
      "Episode 109/200: Reward = 331.69193955064475\n",
      "Episode 110/200: Reward = 238.43229638387436\n",
      "Episode 111/200: Reward = 315.23743787462615\n",
      "Episode 112/200: Reward = 309.01983343684276\n",
      "Episode 113/200: Reward = 301.6172803244792\n",
      "Episode 114/200: Reward = 259.4705744430631\n",
      "Episode 115/200: Reward = 301.7823674047293\n",
      "Episode 116/200: Reward = 375.440388508711\n",
      "Episode 117/200: Reward = 290.9370412341579\n",
      "Episode 118/200: Reward = 214.66184472967592\n",
      "Episode 119/200: Reward = 373.5807941559985\n",
      "Episode 120/200: Reward = 287.5662685443666\n",
      "Episode 121/200: Reward = 371.2267745659886\n",
      "Episode 122/200: Reward = 467.6700159744365\n",
      "Episode 123/200: Reward = 337.9230319233517\n",
      "Episode 124/200: Reward = 335.3044333035432\n",
      "Episode 125/200: Reward = 256.75318873999925\n",
      "Episode 126/200: Reward = 352.55678288737477\n",
      "Episode 127/200: Reward = 350.21373778300756\n",
      "Episode 128/200: Reward = 306.61591410638613\n",
      "Episode 129/200: Reward = 298.2380835541847\n",
      "Episode 130/200: Reward = 274.0549956935566\n",
      "Episode 131/200: Reward = 350.5384079471311\n",
      "Episode 132/200: Reward = 259.7509930764163\n",
      "Episode 133/200: Reward = 321.66963685563167\n",
      "Episode 134/200: Reward = 251.44455752793974\n",
      "Episode 135/200: Reward = 328.3124097162589\n",
      "Episode 136/200: Reward = 312.03507008877995\n",
      "Episode 137/200: Reward = 362.3072684851349\n",
      "Episode 138/200: Reward = 339.0342530147805\n",
      "Episode 139/200: Reward = 382.6042136775072\n",
      "Episode 140/200: Reward = 353.1963894202012\n",
      "Episode 141/200: Reward = 348.21461326849646\n",
      "Episode 142/200: Reward = 250.94758953600788\n",
      "Episode 143/200: Reward = 352.59745131320005\n",
      "Episode 144/200: Reward = 378.4061827888345\n",
      "Episode 145/200: Reward = 390.6320999013154\n",
      "Episode 146/200: Reward = 265.8932198397953\n",
      "Episode 147/200: Reward = 306.30742697630563\n",
      "Episode 148/200: Reward = 280.1117834523887\n",
      "Episode 149/200: Reward = 484.11701620244634\n",
      "Episode 150/200: Reward = 140.8836316649908\n",
      "Episode 151/200: Reward = 288.0739872624111\n",
      "Episode 152/200: Reward = 314.36727994031946\n",
      "Episode 153/200: Reward = 223.32348929588034\n",
      "Episode 154/200: Reward = 272.4439788218972\n",
      "Episode 155/200: Reward = 275.6803488368759\n",
      "Episode 156/200: Reward = 298.0399624588526\n",
      "Episode 157/200: Reward = 312.814521330181\n",
      "Episode 158/200: Reward = 312.751965773244\n",
      "Episode 159/200: Reward = 419.5368100487346\n",
      "Episode 160/200: Reward = 271.45279406786113\n",
      "Episode 161/200: Reward = 278.4517437023683\n",
      "Episode 162/200: Reward = 336.7130695178552\n",
      "Episode 163/200: Reward = 326.138474597434\n",
      "Episode 164/200: Reward = 276.7639903065855\n",
      "Episode 165/200: Reward = 268.9082017026038\n",
      "Episode 166/200: Reward = 293.81203282157077\n",
      "Episode 167/200: Reward = 367.8212549505593\n",
      "Episode 168/200: Reward = 270.05608947798015\n",
      "Episode 169/200: Reward = 261.92069941542456\n",
      "Episode 170/200: Reward = 307.21121408926194\n",
      "Episode 171/200: Reward = 289.7753659657826\n",
      "Episode 172/200: Reward = 341.27517374864925\n",
      "Episode 173/200: Reward = 301.4064383027537\n",
      "Episode 174/200: Reward = 343.36318563619045\n",
      "Episode 175/200: Reward = 290.0440364641735\n",
      "Episode 176/200: Reward = 298.7732134964434\n",
      "Episode 177/200: Reward = 305.35797209501874\n",
      "Episode 178/200: Reward = 315.63356787746744\n",
      "Episode 179/200: Reward = 289.542523051814\n",
      "Episode 180/200: Reward = 283.0987451698228\n",
      "Episode 181/200: Reward = 360.4517816620338\n",
      "Episode 182/200: Reward = 230.40248344516456\n",
      "Episode 183/200: Reward = 238.63408728369131\n",
      "Episode 184/200: Reward = 301.32218853198265\n",
      "Episode 185/200: Reward = 260.83800938959513\n",
      "Episode 186/200: Reward = 398.6699900101163\n",
      "Episode 187/200: Reward = 325.7374151418011\n",
      "Episode 188/200: Reward = 299.64089617572535\n",
      "Episode 189/200: Reward = 266.41090871501626\n",
      "Episode 190/200: Reward = 249.5863454078269\n",
      "Episode 191/200: Reward = 371.44728533063306\n",
      "Episode 192/200: Reward = 322.19131507234806\n",
      "Episode 193/200: Reward = 347.6410002991484\n",
      "Episode 194/200: Reward = 233.73111989396804\n",
      "Episode 195/200: Reward = 251.7517024690329\n",
      "Episode 196/200: Reward = 314.8695097142694\n",
      "Episode 197/200: Reward = 371.5234557125146\n",
      "Episode 198/200: Reward = 362.6652118403738\n",
      "Episode 199/200: Reward = 317.56222774389903\n",
      "Episode 200/200: Reward = 368.0576695787597\n",
      "Average Reward under MAD attack: 308.9001499443959\n",
      "Final Average Reward under MAD Attack: 308.9001499443959\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T13:03:38.485775Z",
     "iopub.status.busy": "2024-12-06T13:03:38.485379Z",
     "iopub.status.idle": "2024-12-06T13:07:50.310391Z",
     "shell.execute_reply": "2024-12-06T13:07:50.309400Z",
     "shell.execute_reply.started": "2024-12-06T13:03:38.485741Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/4284193347.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000, TD Loss: 6.1999, Robust Loss: 0.0000\n",
      "Step 100/5000, TD Loss: 5.7912, Robust Loss: 0.0328\n",
      "Step 200/5000, TD Loss: 4.3061, Robust Loss: 0.1697\n",
      "Step 300/5000, TD Loss: 4.2290, Robust Loss: 0.3166\n",
      "Step 400/5000, TD Loss: 3.8098, Robust Loss: 0.6019\n",
      "Step 500/5000, TD Loss: 4.0872, Robust Loss: 0.6546\n",
      "Step 600/5000, TD Loss: 2.8455, Robust Loss: 0.4701\n",
      "Step 700/5000, TD Loss: 3.9531, Robust Loss: 0.8368\n",
      "Step 800/5000, TD Loss: 3.5651, Robust Loss: 1.1834\n",
      "Step 900/5000, TD Loss: 2.5904, Robust Loss: 1.3390\n",
      "Step 1000/5000, TD Loss: 2.2741, Robust Loss: 1.3409\n",
      "Step 1100/5000, TD Loss: 1.8981, Robust Loss: 1.1337\n",
      "Step 1200/5000, TD Loss: 1.8008, Robust Loss: 2.0916\n",
      "Step 1300/5000, TD Loss: 1.9789, Robust Loss: 1.6612\n",
      "Step 1400/5000, TD Loss: 1.5431, Robust Loss: 2.7907\n",
      "Step 1500/5000, TD Loss: 1.7528, Robust Loss: 3.3115\n",
      "Step 1600/5000, TD Loss: 1.5100, Robust Loss: 2.4014\n",
      "Step 1700/5000, TD Loss: 1.4106, Robust Loss: 3.7988\n",
      "Step 1800/5000, TD Loss: 1.2044, Robust Loss: 2.3466\n",
      "Step 1900/5000, TD Loss: 1.1338, Robust Loss: 1.6903\n",
      "Step 2000/5000, TD Loss: 0.9785, Robust Loss: 2.2345\n",
      "Step 2100/5000, TD Loss: 0.9106, Robust Loss: 3.2297\n",
      "Step 2200/5000, TD Loss: 0.8314, Robust Loss: 3.6213\n",
      "Step 2300/5000, TD Loss: 1.4696, Robust Loss: 3.3309\n",
      "Step 2500/5000, TD Loss: 0.9656, Robust Loss: 2.1337\n",
      "Step 2600/5000, TD Loss: 0.5507, Robust Loss: 1.6066\n",
      "Step 2700/5000, TD Loss: 0.8416, Robust Loss: 3.4712\n",
      "Step 2800/5000, TD Loss: 0.6397, Robust Loss: 2.2959\n",
      "Step 2900/5000, TD Loss: 0.8879, Robust Loss: 2.3488\n",
      "Step 3000/5000, TD Loss: 0.5472, Robust Loss: 3.1590\n",
      "Step 3100/5000, TD Loss: 0.7277, Robust Loss: 2.9558\n",
      "Step 3200/5000, TD Loss: 0.7481, Robust Loss: 3.2069\n",
      "Step 3300/5000, TD Loss: 0.6643, Robust Loss: 2.5599\n",
      "Step 3400/5000, TD Loss: 0.9123, Robust Loss: 2.2399\n",
      "Step 3500/5000, TD Loss: 0.5774, Robust Loss: 1.6282\n",
      "Step 3600/5000, TD Loss: 0.9482, Robust Loss: 2.4616\n",
      "Step 3700/5000, TD Loss: 0.8112, Robust Loss: 1.7389\n",
      "Step 3800/5000, TD Loss: 0.7528, Robust Loss: 2.2287\n",
      "Step 3900/5000, TD Loss: 0.7463, Robust Loss: 1.2439\n",
      "Step 4000/5000, TD Loss: 0.7182, Robust Loss: 5.0387\n",
      "Step 4100/5000, TD Loss: 0.5662, Robust Loss: 2.4107\n",
      "Step 4200/5000, TD Loss: 0.7066, Robust Loss: 3.5649\n",
      "Step 4300/5000, TD Loss: 0.3828, Robust Loss: 4.6384\n",
      "Step 4400/5000, TD Loss: 0.7239, Robust Loss: 2.2927\n",
      "Step 4500/5000, TD Loss: 0.6370, Robust Loss: 2.6286\n",
      "Step 4600/5000, TD Loss: 0.4276, Robust Loss: 1.5506\n",
      "Step 4700/5000, TD Loss: 0.4480, Robust Loss: 2.7203\n",
      "Step 4800/5000, TD Loss: 0.6350, Robust Loss: 2.5794\n",
      "Step 4900/5000, TD Loss: 0.7812, Robust Loss: 3.3193\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T13:16:06.537317Z",
     "iopub.status.busy": "2024-12-06T13:16:06.536669Z",
     "iopub.status.idle": "2024-12-06T13:21:32.161548Z",
     "shell.execute_reply": "2024-12-06T13:21:32.160742Z",
     "shell.execute_reply.started": "2024-12-06T13:16:06.537284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 392.57314719148195\n",
      "Episode 2/200: Reward = 394.56969375991673\n",
      "Episode 3/200: Reward = 397.552650830144\n",
      "Episode 4/200: Reward = 401.9052033200802\n",
      "Episode 5/200: Reward = 402.5532820185515\n",
      "Episode 6/200: Reward = 397.3567511914547\n",
      "Episode 7/200: Reward = 395.0661711613391\n",
      "Episode 8/200: Reward = 399.29057381809673\n",
      "Episode 9/200: Reward = 399.15690640470507\n",
      "Episode 10/200: Reward = 398.32890138943304\n",
      "Episode 11/200: Reward = 404.8116518958388\n",
      "Episode 12/200: Reward = 397.8385510080094\n",
      "Episode 13/200: Reward = 395.25767934767987\n",
      "Episode 14/200: Reward = 402.4979507719529\n",
      "Episode 15/200: Reward = 395.07916141022474\n",
      "Episode 16/200: Reward = 400.4118734032521\n",
      "Episode 17/200: Reward = 402.14073895004503\n",
      "Episode 18/200: Reward = 402.4961786763197\n",
      "Episode 19/200: Reward = 398.10382000686013\n",
      "Episode 20/200: Reward = 396.2763902745013\n",
      "Episode 21/200: Reward = 389.89947151280154\n",
      "Episode 22/200: Reward = 395.2551953706645\n",
      "Episode 23/200: Reward = 402.9197562410079\n",
      "Episode 24/200: Reward = 395.98516703922667\n",
      "Episode 25/200: Reward = 396.50250176082\n",
      "Episode 26/200: Reward = 401.099686445051\n",
      "Episode 27/200: Reward = 399.7129456012981\n",
      "Episode 28/200: Reward = 374.37793024207224\n",
      "Episode 29/200: Reward = 402.8325991536946\n",
      "Episode 30/200: Reward = 399.8614087563847\n",
      "Episode 31/200: Reward = 391.5018190397138\n",
      "Episode 32/200: Reward = 396.8682210465144\n",
      "Episode 33/200: Reward = 392.49210001048925\n",
      "Episode 34/200: Reward = 394.27234554416117\n",
      "Episode 35/200: Reward = 401.6491357730973\n",
      "Episode 36/200: Reward = 398.30771083361276\n",
      "Episode 37/200: Reward = 403.7126711716137\n",
      "Episode 38/200: Reward = 392.466956870464\n",
      "Episode 39/200: Reward = 395.9815201762719\n",
      "Episode 40/200: Reward = 399.12609205431113\n",
      "Episode 41/200: Reward = 393.97766911139615\n",
      "Episode 42/200: Reward = 399.2218719643355\n",
      "Episode 43/200: Reward = 394.99591959784\n",
      "Episode 44/200: Reward = 397.1095827948944\n",
      "Episode 45/200: Reward = 395.8685291650401\n",
      "Episode 46/200: Reward = 394.51758287416897\n",
      "Episode 47/200: Reward = 397.025161095461\n",
      "Episode 48/200: Reward = 400.3074421916224\n",
      "Episode 49/200: Reward = 390.25537844609903\n",
      "Episode 50/200: Reward = 397.8071761143904\n",
      "Episode 51/200: Reward = 404.9637256851894\n",
      "Episode 52/200: Reward = 402.5578410532161\n",
      "Episode 53/200: Reward = 394.2334358247325\n",
      "Episode 54/200: Reward = 394.9676776695254\n",
      "Episode 55/200: Reward = 398.1469183435266\n",
      "Episode 56/200: Reward = 397.30015658433575\n",
      "Episode 57/200: Reward = 399.96435739094085\n",
      "Episode 58/200: Reward = 397.9353902700469\n",
      "Episode 59/200: Reward = 395.5946344758867\n",
      "Episode 60/200: Reward = 403.1641467955163\n",
      "Episode 61/200: Reward = 398.5450033842447\n",
      "Episode 62/200: Reward = 393.3863533255865\n",
      "Episode 63/200: Reward = 397.1906749120628\n",
      "Episode 64/200: Reward = 395.3596959127128\n",
      "Episode 65/200: Reward = 392.6341208637091\n",
      "Episode 66/200: Reward = 395.4886775267121\n",
      "Episode 67/200: Reward = 391.47844998162526\n",
      "Episode 68/200: Reward = 401.99972393138995\n",
      "Episode 69/200: Reward = 399.421584407598\n",
      "Episode 70/200: Reward = 389.62285561224525\n",
      "Episode 71/200: Reward = 391.6669467786161\n",
      "Episode 72/200: Reward = 400.4577597807552\n",
      "Episode 73/200: Reward = 393.5164685308675\n",
      "Episode 74/200: Reward = 397.3146720135745\n",
      "Episode 75/200: Reward = 403.23268372386684\n",
      "Episode 76/200: Reward = 394.7425868254713\n",
      "Episode 77/200: Reward = 401.4269077609359\n",
      "Episode 78/200: Reward = 399.1806367984689\n",
      "Episode 79/200: Reward = 390.777195605072\n",
      "Episode 80/200: Reward = 397.89835538967594\n",
      "Episode 81/200: Reward = 392.91222076851705\n",
      "Episode 82/200: Reward = 399.7655951927742\n",
      "Episode 83/200: Reward = 405.27047520476475\n",
      "Episode 84/200: Reward = 397.8026656517623\n",
      "Episode 85/200: Reward = 399.986691460388\n",
      "Episode 86/200: Reward = 400.56749931377755\n",
      "Episode 87/200: Reward = 398.7701264664409\n",
      "Episode 88/200: Reward = 404.7583608822232\n",
      "Episode 89/200: Reward = 396.5495240094204\n",
      "Episode 90/200: Reward = 403.9907642647928\n",
      "Episode 91/200: Reward = 404.39873387142643\n",
      "Episode 92/200: Reward = 389.85948760551\n",
      "Episode 93/200: Reward = 393.17562499322247\n",
      "Episode 94/200: Reward = 398.403783883851\n",
      "Episode 95/200: Reward = 398.12838474629666\n",
      "Episode 96/200: Reward = 399.4740589231738\n",
      "Episode 97/200: Reward = 399.1587676357829\n",
      "Episode 98/200: Reward = 394.6669385491482\n",
      "Episode 99/200: Reward = 397.50655827031835\n",
      "Episode 100/200: Reward = 396.01907376868985\n",
      "Episode 101/200: Reward = 398.10162799259604\n",
      "Episode 102/200: Reward = 400.95982697195205\n",
      "Episode 103/200: Reward = 396.4883956337369\n",
      "Episode 104/200: Reward = 406.3732818245134\n",
      "Episode 105/200: Reward = 400.1431254498004\n",
      "Episode 106/200: Reward = 397.4540886786308\n",
      "Episode 107/200: Reward = 405.1911321937182\n",
      "Episode 108/200: Reward = 388.2727479337987\n",
      "Episode 109/200: Reward = 400.09743927500307\n",
      "Episode 110/200: Reward = 396.2244480842421\n",
      "Episode 111/200: Reward = 400.44093583762367\n",
      "Episode 112/200: Reward = 398.25707217443403\n",
      "Episode 113/200: Reward = 396.96912338579887\n",
      "Episode 114/200: Reward = 392.90597249621527\n",
      "Episode 115/200: Reward = 393.7257248254045\n",
      "Episode 116/200: Reward = 402.05590276314706\n",
      "Episode 117/200: Reward = 397.50122904815925\n",
      "Episode 118/200: Reward = 397.8164671578685\n",
      "Episode 119/200: Reward = 394.88660547748304\n",
      "Episode 120/200: Reward = 395.1891774690515\n",
      "Episode 121/200: Reward = 394.3030970276629\n",
      "Episode 122/200: Reward = 395.05229814449984\n",
      "Episode 123/200: Reward = 397.2045698775555\n",
      "Episode 124/200: Reward = 395.5779940884241\n",
      "Episode 125/200: Reward = 392.11267465487714\n",
      "Episode 126/200: Reward = 404.2668446244137\n",
      "Episode 127/200: Reward = 390.8076325685115\n",
      "Episode 128/200: Reward = 405.9589997911672\n",
      "Episode 129/200: Reward = 400.3186249741189\n",
      "Episode 130/200: Reward = 395.2195748816487\n",
      "Episode 131/200: Reward = 397.41811319587606\n",
      "Episode 132/200: Reward = 399.45338262514986\n",
      "Episode 133/200: Reward = 398.995547592591\n",
      "Episode 134/200: Reward = 393.1473045549848\n",
      "Episode 135/200: Reward = 399.600222114267\n",
      "Episode 136/200: Reward = 396.7376970329323\n",
      "Episode 137/200: Reward = 391.0146035737764\n",
      "Episode 138/200: Reward = 399.42577353503384\n",
      "Episode 139/200: Reward = 404.7724775284927\n",
      "Episode 140/200: Reward = 404.37771935401383\n",
      "Episode 141/200: Reward = 392.2373342663158\n",
      "Episode 142/200: Reward = 400.7287639159035\n",
      "Episode 143/200: Reward = 400.5353572452231\n",
      "Episode 144/200: Reward = 394.5103646891264\n",
      "Episode 145/200: Reward = 393.32600999528523\n",
      "Episode 146/200: Reward = 392.8763204341329\n",
      "Episode 147/200: Reward = 395.38802199927414\n",
      "Episode 148/200: Reward = 398.6134357587142\n",
      "Episode 149/200: Reward = 395.9281303728432\n",
      "Episode 150/200: Reward = 392.4594972196234\n",
      "Episode 151/200: Reward = 403.66180180051987\n",
      "Episode 152/200: Reward = 396.57427034438354\n",
      "Episode 153/200: Reward = 399.5592401795827\n",
      "Episode 154/200: Reward = 396.7528978828522\n",
      "Episode 155/200: Reward = 391.6301509473208\n",
      "Episode 156/200: Reward = 399.3050298670817\n",
      "Episode 157/200: Reward = 399.0325759144426\n",
      "Episode 158/200: Reward = 389.4118148726167\n",
      "Episode 159/200: Reward = 394.72266539293605\n",
      "Episode 160/200: Reward = 402.12569138043\n",
      "Episode 161/200: Reward = 402.2760282076883\n",
      "Episode 162/200: Reward = 392.1329150663395\n",
      "Episode 163/200: Reward = 398.3214316370283\n",
      "Episode 164/200: Reward = 389.1361229071684\n",
      "Episode 165/200: Reward = 396.03706559150635\n",
      "Episode 166/200: Reward = 393.97247212293865\n",
      "Episode 167/200: Reward = 396.6403674771935\n",
      "Episode 168/200: Reward = 395.5310222219968\n",
      "Episode 169/200: Reward = 395.3351558777179\n",
      "Episode 170/200: Reward = 396.77726736083434\n",
      "Episode 171/200: Reward = 402.5137855427568\n",
      "Episode 172/200: Reward = 398.951398678236\n",
      "Episode 173/200: Reward = 393.8282841702808\n",
      "Episode 174/200: Reward = 393.6591375113727\n",
      "Episode 175/200: Reward = 400.31599466662607\n",
      "Episode 176/200: Reward = 399.5401470170893\n",
      "Episode 177/200: Reward = 402.49882244899817\n",
      "Episode 178/200: Reward = 398.5931279504989\n",
      "Episode 179/200: Reward = 401.24148763739896\n",
      "Episode 180/200: Reward = 398.93295119240844\n",
      "Episode 181/200: Reward = 401.4338315707071\n",
      "Episode 182/200: Reward = 395.5452058453253\n",
      "Episode 183/200: Reward = 404.72890249264333\n",
      "Episode 184/200: Reward = 398.9719705621255\n",
      "Episode 185/200: Reward = 396.529496311688\n",
      "Episode 186/200: Reward = 397.7272244869653\n",
      "Episode 187/200: Reward = 381.5155810973573\n",
      "Episode 188/200: Reward = 401.1556814658713\n",
      "Episode 189/200: Reward = 393.8343991856987\n",
      "Episode 190/200: Reward = 388.5311131735987\n",
      "Episode 191/200: Reward = 400.62522979413654\n",
      "Episode 192/200: Reward = 394.270092267295\n",
      "Episode 193/200: Reward = 389.5047300399823\n",
      "Episode 194/200: Reward = 397.72178937505635\n",
      "Episode 195/200: Reward = 391.0485459276744\n",
      "Episode 196/200: Reward = 396.3046394200674\n",
      "Episode 197/200: Reward = 403.75582063543027\n",
      "Episode 198/200: Reward = 406.4931687654551\n",
      "Episode 199/200: Reward = 399.44752836409003\n",
      "Episode 200/200: Reward = 398.5468888661923\n",
      "Average Reward under Robust Sarsa Critic-based attack: 397.36319736637\n",
      "Final Average Reward under Robust Sarsa Attack: 397.36319736637\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T13:24:17.036496Z",
     "iopub.status.busy": "2024-12-06T13:24:17.036131Z",
     "iopub.status.idle": "2024-12-06T13:29:49.354461Z",
     "shell.execute_reply": "2024-12-06T13:29:49.353565Z",
     "shell.execute_reply.started": "2024-12-06T13:24:17.036465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 392.8101105062745\n",
      "Episode 2/200: Reward = 395.9151603105879\n",
      "Episode 3/200: Reward = 399.6415264926808\n",
      "Episode 4/200: Reward = 390.43101910805177\n",
      "Episode 5/200: Reward = 388.3201617314316\n",
      "Episode 6/200: Reward = 394.643071016279\n",
      "Episode 7/200: Reward = 392.0375836825289\n",
      "Episode 8/200: Reward = 393.6124980141872\n",
      "Episode 9/200: Reward = 398.22328245121986\n",
      "Episode 10/200: Reward = 386.01921784239323\n",
      "Episode 11/200: Reward = 390.1047314061136\n",
      "Episode 12/200: Reward = 389.47213913677234\n",
      "Episode 13/200: Reward = 393.45955097224777\n",
      "Episode 14/200: Reward = 389.642621341879\n",
      "Episode 15/200: Reward = 394.81551779512046\n",
      "Episode 16/200: Reward = 390.5571606330244\n",
      "Episode 17/200: Reward = 394.38081536009156\n",
      "Episode 18/200: Reward = 399.59204045204973\n",
      "Episode 19/200: Reward = 394.6123719171847\n",
      "Episode 20/200: Reward = 394.47400830444974\n",
      "Episode 21/200: Reward = 390.0487251945646\n",
      "Episode 22/200: Reward = 395.57920541996776\n",
      "Episode 23/200: Reward = 395.7023536688362\n",
      "Episode 24/200: Reward = 396.6327993490635\n",
      "Episode 25/200: Reward = 401.64481276913364\n",
      "Episode 26/200: Reward = 394.028770112133\n",
      "Episode 27/200: Reward = 396.2948286635003\n",
      "Episode 28/200: Reward = 400.6086165606069\n",
      "Episode 29/200: Reward = 394.25124180890225\n",
      "Episode 30/200: Reward = 391.3594195183996\n",
      "Episode 31/200: Reward = 397.05279861266115\n",
      "Episode 32/200: Reward = 395.8309205471905\n",
      "Episode 33/200: Reward = 397.56408245770234\n",
      "Episode 34/200: Reward = 389.0802276538982\n",
      "Episode 35/200: Reward = 397.5985673838598\n",
      "Episode 36/200: Reward = 391.0781815206639\n",
      "Episode 37/200: Reward = 397.5673290400192\n",
      "Episode 38/200: Reward = 393.9370453519423\n",
      "Episode 39/200: Reward = 389.55983144767225\n",
      "Episode 40/200: Reward = 393.71359284029995\n",
      "Episode 41/200: Reward = 390.7736454076778\n",
      "Episode 42/200: Reward = 388.87186625052135\n",
      "Episode 43/200: Reward = 391.7483098913691\n",
      "Episode 44/200: Reward = 385.64000145313355\n",
      "Episode 45/200: Reward = 399.20109544732895\n",
      "Episode 46/200: Reward = 400.733812188931\n",
      "Episode 47/200: Reward = 383.86714412680914\n",
      "Episode 48/200: Reward = 399.86453162432605\n",
      "Episode 49/200: Reward = 388.9949177832495\n",
      "Episode 50/200: Reward = 387.05181568682065\n",
      "Episode 51/200: Reward = 397.0626191199676\n",
      "Episode 52/200: Reward = 398.5019879599008\n",
      "Episode 53/200: Reward = 387.944723718633\n",
      "Episode 54/200: Reward = 398.1301940537633\n",
      "Episode 55/200: Reward = 387.9520166479774\n",
      "Episode 56/200: Reward = 388.33135324463217\n",
      "Episode 57/200: Reward = 392.46769247018153\n",
      "Episode 58/200: Reward = 390.3336312678254\n",
      "Episode 59/200: Reward = 395.2822997322735\n",
      "Episode 60/200: Reward = 395.9266218410993\n",
      "Episode 61/200: Reward = 392.6978572922322\n",
      "Episode 62/200: Reward = 389.0446623464217\n",
      "Episode 63/200: Reward = 395.23344694301727\n",
      "Episode 64/200: Reward = 392.458235657903\n",
      "Episode 65/200: Reward = 390.4468638665804\n",
      "Episode 66/200: Reward = 390.94140153800214\n",
      "Episode 67/200: Reward = 389.0522207675222\n",
      "Episode 68/200: Reward = 396.05307514212836\n",
      "Episode 69/200: Reward = 386.6081228052574\n",
      "Episode 70/200: Reward = 395.4090608219063\n",
      "Episode 71/200: Reward = 390.7255603375875\n",
      "Episode 72/200: Reward = 394.12843479885925\n",
      "Episode 73/200: Reward = 392.8424702120345\n",
      "Episode 74/200: Reward = 393.9865686363517\n",
      "Episode 75/200: Reward = 394.37668481103634\n",
      "Episode 76/200: Reward = 386.6986339466417\n",
      "Episode 77/200: Reward = 393.4210144521874\n",
      "Episode 78/200: Reward = 395.0123485647665\n",
      "Episode 79/200: Reward = 394.2494686843706\n",
      "Episode 80/200: Reward = 393.7527724784468\n",
      "Episode 81/200: Reward = 391.0659330236874\n",
      "Episode 82/200: Reward = 397.9057020738732\n",
      "Episode 83/200: Reward = 389.9062724472256\n",
      "Episode 84/200: Reward = 386.9517782950734\n",
      "Episode 85/200: Reward = 394.4906060745364\n",
      "Episode 86/200: Reward = 395.2319741684897\n",
      "Episode 87/200: Reward = 390.38495826041907\n",
      "Episode 88/200: Reward = 397.47645474465855\n",
      "Episode 89/200: Reward = 390.8288686496001\n",
      "Episode 90/200: Reward = 390.59884556056323\n",
      "Episode 91/200: Reward = 394.6258369687291\n",
      "Episode 92/200: Reward = 395.2654030622455\n",
      "Episode 93/200: Reward = 393.8199998154124\n",
      "Episode 94/200: Reward = 393.32880644467383\n",
      "Episode 95/200: Reward = 389.6679689147641\n",
      "Episode 96/200: Reward = 398.02464079350244\n",
      "Episode 97/200: Reward = 400.80043986339575\n",
      "Episode 98/200: Reward = 387.87082387620535\n",
      "Episode 99/200: Reward = 390.218762353333\n",
      "Episode 100/200: Reward = 391.3722655994595\n",
      "Episode 101/200: Reward = 390.76082797553835\n",
      "Episode 102/200: Reward = 393.00699548279374\n",
      "Episode 103/200: Reward = 391.6584098955429\n",
      "Episode 104/200: Reward = 392.21808157094233\n",
      "Episode 105/200: Reward = 392.265908731181\n",
      "Episode 106/200: Reward = 398.8518851395414\n",
      "Episode 107/200: Reward = 401.2844985877381\n",
      "Episode 108/200: Reward = 395.28583318869266\n",
      "Episode 109/200: Reward = 387.85368794994343\n",
      "Episode 110/200: Reward = 391.48746228808994\n",
      "Episode 111/200: Reward = 385.9554719609712\n",
      "Episode 112/200: Reward = 393.28237527876263\n",
      "Episode 113/200: Reward = 394.9213644593787\n",
      "Episode 114/200: Reward = 388.72884613908496\n",
      "Episode 115/200: Reward = 392.54461690295057\n",
      "Episode 116/200: Reward = 394.9174049125448\n",
      "Episode 117/200: Reward = 388.70605581300975\n",
      "Episode 118/200: Reward = 390.31604514685955\n",
      "Episode 119/200: Reward = 391.3798241969683\n",
      "Episode 120/200: Reward = 391.4384573959582\n",
      "Episode 121/200: Reward = 394.0215248217178\n",
      "Episode 122/200: Reward = 394.0428313074463\n",
      "Episode 123/200: Reward = 395.1158363048449\n",
      "Episode 124/200: Reward = 394.53475440623976\n",
      "Episode 125/200: Reward = 387.50480827056475\n",
      "Episode 126/200: Reward = 391.306640586779\n",
      "Episode 127/200: Reward = 397.51012171413254\n",
      "Episode 128/200: Reward = 393.9993562296285\n",
      "Episode 129/200: Reward = 397.1332501848069\n",
      "Episode 130/200: Reward = 399.41566577855895\n",
      "Episode 131/200: Reward = 394.53026447481733\n",
      "Episode 132/200: Reward = 400.64163296372607\n",
      "Episode 133/200: Reward = 384.27583724178595\n",
      "Episode 134/200: Reward = 390.8657603248593\n",
      "Episode 135/200: Reward = 399.6781668191336\n",
      "Episode 136/200: Reward = 391.04505068990085\n",
      "Episode 137/200: Reward = 398.32681615025285\n",
      "Episode 138/200: Reward = 391.55058934199445\n",
      "Episode 139/200: Reward = 399.2250311940245\n",
      "Episode 140/200: Reward = 396.86720284492236\n",
      "Episode 141/200: Reward = 385.6144533707695\n",
      "Episode 142/200: Reward = 395.65237325330696\n",
      "Episode 143/200: Reward = 397.2026784887814\n",
      "Episode 144/200: Reward = 390.1673688255273\n",
      "Episode 145/200: Reward = 391.8913604813819\n",
      "Episode 146/200: Reward = 395.1213137856105\n",
      "Episode 147/200: Reward = 390.6145717345542\n",
      "Episode 148/200: Reward = 389.5292473155075\n",
      "Episode 149/200: Reward = 392.67812460012226\n",
      "Episode 150/200: Reward = 393.7501461424328\n",
      "Episode 151/200: Reward = 391.42616974690634\n",
      "Episode 152/200: Reward = 395.4852322256658\n",
      "Episode 153/200: Reward = 391.80147239852664\n",
      "Episode 154/200: Reward = 392.82018876050626\n",
      "Episode 155/200: Reward = 397.7978156743105\n",
      "Episode 156/200: Reward = 393.6315962994577\n",
      "Episode 157/200: Reward = 395.8251879087094\n",
      "Episode 158/200: Reward = 391.5704414670369\n",
      "Episode 159/200: Reward = 398.1309211822624\n",
      "Episode 160/200: Reward = 390.21803842773954\n",
      "Episode 161/200: Reward = 394.0892613800242\n",
      "Episode 162/200: Reward = 395.94634596430586\n",
      "Episode 163/200: Reward = 387.7341613692458\n",
      "Episode 164/200: Reward = 391.23133763646877\n",
      "Episode 165/200: Reward = 390.6940430314868\n",
      "Episode 166/200: Reward = 393.3520879856102\n",
      "Episode 167/200: Reward = 390.40658844939003\n",
      "Episode 168/200: Reward = 386.8919382979847\n",
      "Episode 169/200: Reward = 389.0843438740523\n",
      "Episode 170/200: Reward = 394.56447634157854\n",
      "Episode 171/200: Reward = 390.63475890846416\n",
      "Episode 172/200: Reward = 394.0196611490893\n",
      "Episode 173/200: Reward = 389.9674751876306\n",
      "Episode 174/200: Reward = 394.3401205367349\n",
      "Episode 175/200: Reward = 391.4250848470904\n",
      "Episode 176/200: Reward = 391.98415162714645\n",
      "Episode 177/200: Reward = 394.0413622681748\n",
      "Episode 178/200: Reward = 395.4608995445308\n",
      "Episode 179/200: Reward = 395.5595793285139\n",
      "Episode 180/200: Reward = 396.8130862720188\n",
      "Episode 181/200: Reward = 393.61190713715087\n",
      "Episode 182/200: Reward = 388.54496959919413\n",
      "Episode 183/200: Reward = 399.4104501028903\n",
      "Episode 184/200: Reward = 396.8913305261646\n",
      "Episode 185/200: Reward = 391.9870613120979\n",
      "Episode 186/200: Reward = 396.709562161644\n",
      "Episode 187/200: Reward = 390.16789070418804\n",
      "Episode 188/200: Reward = 395.5825608385363\n",
      "Episode 189/200: Reward = 392.0332040134561\n",
      "Episode 190/200: Reward = 396.95149096939934\n",
      "Episode 191/200: Reward = 392.74611611849116\n",
      "Episode 192/200: Reward = 391.47236752639316\n",
      "Episode 193/200: Reward = 392.86793387477667\n",
      "Episode 194/200: Reward = 382.99479130643374\n",
      "Episode 195/200: Reward = 393.49833953654803\n",
      "Episode 196/200: Reward = 389.1786186783877\n",
      "Episode 197/200: Reward = 393.21946833789343\n",
      "Episode 198/200: Reward = 385.7992016627969\n",
      "Episode 199/200: Reward = 394.1542874302193\n",
      "Episode 200/200: Reward = 389.0610329962986\n",
      "Average Reward under State Value Attack: 393.06030354496664\n",
      "Average Reward under State Action Value Attack: 393.06030354496664\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    value_net=RobustAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Walker2d-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
