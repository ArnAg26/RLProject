{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:29.039924Z",
     "iopub.status.busy": "2024-12-05T10:17:29.039557Z",
     "iopub.status.idle": "2024-12-05T10:17:30.058449Z",
     "shell.execute_reply": "2024-12-05T10:17:30.057554Z",
     "shell.execute_reply.started": "2024-12-05T10:17:29.039883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:30.060609Z",
     "iopub.status.busy": "2024-12-05T10:17:30.060138Z",
     "iopub.status.idle": "2024-12-05T10:17:42.434827Z",
     "shell.execute_reply": "2024-12-05T10:17:42.433751Z",
     "shell.execute_reply.started": "2024-12-05T10:17:30.060572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:42.436860Z",
     "iopub.status.busy": "2024-12-05T10:17:42.436428Z",
     "iopub.status.idle": "2024-12-05T10:17:46.172343Z",
     "shell.execute_reply": "2024-12-05T10:17:46.171672Z",
     "shell.execute_reply.started": "2024-12-05T10:17:42.436818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:11.868944Z",
     "iopub.status.busy": "2024-12-05T10:23:11.868237Z",
     "iopub.status.idle": "2024-12-05T10:23:11.877448Z",
     "shell.execute_reply": "2024-12-05T10:23:11.876585Z",
     "shell.execute_reply.started": "2024-12-05T10:23:11.868913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:34.009932Z",
     "iopub.status.busy": "2024-12-05T10:23:34.009621Z",
     "iopub.status.idle": "2024-12-05T10:23:34.018932Z",
     "shell.execute_reply": "2024-12-05T10:23:34.018060Z",
     "shell.execute_reply.started": "2024-12-05T10:23:34.009903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:34.021771Z",
     "iopub.status.busy": "2024-12-05T10:23:34.021480Z",
     "iopub.status.idle": "2024-12-05T10:23:34.033970Z",
     "shell.execute_reply": "2024-12-05T10:23:34.033266Z",
     "shell.execute_reply.started": "2024-12-05T10:23:34.021745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:59.151305Z",
     "iopub.status.busy": "2024-12-05T10:23:59.151037Z",
     "iopub.status.idle": "2024-12-05T10:23:59.161904Z",
     "shell.execute_reply": "2024-12-05T10:23:59.160864Z",
     "shell.execute_reply.started": "2024-12-05T10:23:59.151280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:36:51.121466Z",
     "iopub.status.busy": "2024-12-05T10:36:51.121047Z",
     "iopub.status.idle": "2024-12-05T10:36:51.137624Z",
     "shell.execute_reply": "2024-12-05T10:36:51.136717Z",
     "shell.execute_reply.started": "2024-12-05T10:36:51.121412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:36:51.139138Z",
     "iopub.status.busy": "2024-12-05T10:36:51.138850Z",
     "iopub.status.idle": "2024-12-05T10:36:51.153219Z",
     "shell.execute_reply": "2024-12-05T10:36:51.152380Z",
     "shell.execute_reply.started": "2024-12-05T10:36:51.139113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:49:36.304848Z",
     "iopub.status.busy": "2024-12-05T10:49:36.304547Z",
     "iopub.status.idle": "2024-12-05T10:49:36.334422Z",
     "shell.execute_reply": "2024-12-05T10:49:36.333490Z",
     "shell.execute_reply.started": "2024-12-05T10:49:36.304821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, hidden_sizes=(64, 64)):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.discrete = discrete\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = state_dim\n",
    "\n",
    "        for hidden_dim in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        if self.discrete:\n",
    "            self.output = nn.Linear(input_dim, action_dim)\n",
    "        else:\n",
    "            self.mean = nn.Linear(input_dim, action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = torch.tanh(layer(x))\n",
    "        if self.discrete:\n",
    "            logits = self.output(x)\n",
    "            return torch.softmax(logits, dim=-1)\n",
    "        else:\n",
    "            mean = self.mean(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            return mean, std\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(64, 64)):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        input_dim = state_dim\n",
    "\n",
    "        for hidden_dim in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.output = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = torch.tanh(layer(x))\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "class SAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, lr=3e-4, gamma=0.99, lam=0.95, eps_clip=0.2, k_epochs=4, sgld_steps=10, sgld_lr=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Actor and critic networks\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, discrete).to(self.device)\n",
    "        self.value_net = ValueNetwork(state_dim).to(self.device)\n",
    "\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "\n",
    "        self.sgld_steps = sgld_steps\n",
    "        self.sgld_lr = sgld_lr\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if self.policy_net.discrete:\n",
    "            probs = self.policy_net(state)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            return action.item(), dist.log_prob(action)\n",
    "        else:\n",
    "            mean, std = self.policy_net(state)\n",
    "            dist = torch.distributions.Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "            return action.cpu().numpy(), dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def sgld_step(self, state, epsilon):\n",
    "        \"\"\"Perform Stochastic Gradient Langevin Dynamics (SGLD) to generate perturbed states.\"\"\"\n",
    "        perturbed_state = state.clone().detach().to(self.device).requires_grad_(True)\n",
    "    \n",
    "        for _ in range(self.sgld_steps):\n",
    "            if perturbed_state.grad is not None:\n",
    "                perturbed_state.grad.zero_()\n",
    "    \n",
    "            # Compute KL divergence between original and perturbed policies\n",
    "            with torch.no_grad():\n",
    "                original_logits = self.policy_net(state)\n",
    "            perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "            if self.policy_net.discrete:\n",
    "                original_policy = dist.Categorical(original_logits)\n",
    "                perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "            else:\n",
    "                original_mean, original_std = original_logits\n",
    "                perturbed_mean, perturbed_std = perturbed_logits\n",
    "                original_policy = dist.Normal(original_mean, original_std)\n",
    "                perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "    \n",
    "            # Backpropagate KL divergence\n",
    "            kl_div.backward()\n",
    "    \n",
    "            # Update perturbed state using gradient and noise\n",
    "            perturbed_state = perturbed_state + epsilon * perturbed_state.grad + torch.randn_like(perturbed_state) * epsilon\n",
    "            perturbed_state = perturbed_state.detach().clone().requires_grad_(True)\n",
    "    \n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    def compute_kl_regularization(self, states, actions):\n",
    "        \"\"\"Compute the KL divergence regularization across all states.\"\"\"\n",
    "        if len(states) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "    \n",
    "        kl_div_total = 0\n",
    "        for state in states:\n",
    "            perturbed_state = self.sgld_step(state, self.sgld_lr)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                original_logits = self.policy_net(state)\n",
    "            perturbed_logits = self.policy_net(perturbed_state)\n",
    "    \n",
    "            if self.policy_net.discrete:\n",
    "                original_policy = dist.Categorical(original_logits)\n",
    "                perturbed_policy = dist.Categorical(perturbed_logits)\n",
    "            else:\n",
    "                original_mean, original_std = original_logits\n",
    "                perturbed_mean, perturbed_std = perturbed_logits\n",
    "                original_policy = dist.Normal(original_mean, original_std)\n",
    "                perturbed_policy = dist.Normal(perturbed_mean, perturbed_std)\n",
    "    \n",
    "            kl_div = dist.kl.kl_divergence(original_policy, perturbed_policy).mean()\n",
    "            kl_div_total += kl_div\n",
    "    \n",
    "        return kl_div_total / len(states)\n",
    "    \n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    \n",
    "            # Reset the environment\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "            # Rollout phase: Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                value = self.value_net(state).squeeze(0).detach()  # Detach the value tensor\n",
    "                action, log_prob = self.select_action(state.cpu().numpy())\n",
    "    \n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Append data to lists\n",
    "                states.append(state.clone().detach())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob.clone().detach())\n",
    "                values.append(value)\n",
    "    \n",
    "                # Update state\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "    \n",
    "            # Add a final value estimate\n",
    "            values.append(torch.tensor([0], device=self.device).detach())\n",
    "    \n",
    "            # Compute advantages and returns\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "\n",
    "            # Convert lists to tensors\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(\n",
    "                np.array(actions),\n",
    "                dtype=torch.float32 if not self.policy_net.discrete else torch.long\n",
    "            ).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "    \n",
    "            # Optimization phase\n",
    "            for _ in range(self.k_epochs):\n",
    "                kl_reg = self.compute_kl_regularization(states, actions)\n",
    "    \n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i + batch_size]\n",
    "                    batch_actions = actions[i:i + batch_size]\n",
    "                    batch_log_probs = log_probs[i:i + batch_size]\n",
    "                    batch_advantages = advantages[i:i + batch_size]\n",
    "                    batch_returns = returns[i:i + batch_size]\n",
    "\n",
    "                    if self.policy_net.discrete:\n",
    "                        action_probs = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Categorical(action_probs)\n",
    "                        new_log_probs = dist.log_prob(batch_actions)\n",
    "                    else:\n",
    "                        mean, std = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Normal(mean, std)\n",
    "                        new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "\n",
    "                    # Detach kl_reg to prevent graph accumulation\n",
    "                    kl_reg = kl_reg.detach()\n",
    "    \n",
    "                    total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_reg\n",
    "    \n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    total_loss.backward(retain_graph=False)  # No need to retain the graph here\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "    \n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}, KL Reg = {kl_reg.item()}\")\n",
    "    \n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:49:36.336552Z",
     "iopub.status.busy": "2024-12-05T10:49:36.335657Z",
     "iopub.status.idle": "2024-12-05T15:01:21.908446Z",
     "shell.execute_reply": "2024-12-05T15:01:21.907370Z",
     "shell.execute_reply.started": "2024-12-05T10:49:36.336508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -8.539791107177734, Value Loss = 91.50924682617188, KL Reg = 1.0124143955181353e-05\n",
      "Episode 2: Policy Loss = -14.921133041381836, Value Loss = 221.65118408203125, KL Reg = 1.2055054867232684e-05\n",
      "Episode 3: Policy Loss = -4.782430648803711, Value Loss = 120.10716247558594, KL Reg = 1.5467019693460315e-05\n",
      "Episode 4: Policy Loss = -6.200763702392578, Value Loss = 64.98123168945312, KL Reg = 2.2710195480613038e-05\n",
      "Episode 5: Policy Loss = -9.923398971557617, Value Loss = 157.38397216796875, KL Reg = 3.756040314328857e-05\n",
      "Episode 6: Policy Loss = -10.554612159729004, Value Loss = 216.71499633789062, KL Reg = 5.482680717250332e-05\n",
      "Episode 7: Policy Loss = -10.816160202026367, Value Loss = 239.9801025390625, KL Reg = 7.194499630713835e-05\n",
      "Episode 8: Policy Loss = -8.576520919799805, Value Loss = 209.21029663085938, KL Reg = 9.797522943699732e-05\n",
      "Episode 9: Policy Loss = -5.71106481552124, Value Loss = 173.89205932617188, KL Reg = 0.0001289803913095966\n",
      "Episode 10: Policy Loss = -12.812941551208496, Value Loss = 266.5433654785156, KL Reg = 0.00015061003796290606\n",
      "Episode 11: Policy Loss = -2.8076770305633545, Value Loss = 231.26776123046875, KL Reg = 0.0001816509902710095\n",
      "Episode 12: Policy Loss = -10.489791870117188, Value Loss = 573.7066650390625, KL Reg = 0.0001898663176689297\n",
      "Episode 13: Policy Loss = -12.164995193481445, Value Loss = 589.9742431640625, KL Reg = 0.00021378588280640543\n",
      "Episode 14: Policy Loss = -13.248479843139648, Value Loss = 379.1156311035156, KL Reg = 0.0002116763062076643\n",
      "Episode 15: Policy Loss = -14.317716598510742, Value Loss = 499.4757080078125, KL Reg = 0.00024035133537836373\n",
      "Episode 16: Policy Loss = -12.438724517822266, Value Loss = 460.1611328125, KL Reg = 0.000274428486591205\n",
      "Episode 17: Policy Loss = -5.799797534942627, Value Loss = 297.166259765625, KL Reg = 0.0003383452130947262\n",
      "Episode 18: Policy Loss = -5.94990873336792, Value Loss = 295.5150451660156, KL Reg = 0.00037552075809799135\n",
      "Episode 19: Policy Loss = -11.739875793457031, Value Loss = 713.5875854492188, KL Reg = 0.00044451782014220953\n",
      "Episode 20: Policy Loss = -0.47879791259765625, Value Loss = 701.6289672851562, KL Reg = 0.00044759453157894313\n",
      "Episode 21: Policy Loss = 2.213884115219116, Value Loss = 488.9283447265625, KL Reg = 0.0004612741759046912\n",
      "Episode 22: Policy Loss = 4.343314170837402, Value Loss = 739.248046875, KL Reg = 0.00045996354310773313\n",
      "Episode 23: Policy Loss = 12.29129695892334, Value Loss = 649.2523193359375, KL Reg = 0.0005113395163789392\n",
      "Episode 24: Policy Loss = 0.6936101913452148, Value Loss = 882.4385375976562, KL Reg = 0.0005614898400381207\n",
      "Episode 25: Policy Loss = 11.710406303405762, Value Loss = 684.4908447265625, KL Reg = 0.00070335587952286\n",
      "Episode 26: Policy Loss = 5.247302055358887, Value Loss = 586.07568359375, KL Reg = 0.0007044323137961328\n",
      "Episode 27: Policy Loss = -5.698000431060791, Value Loss = 610.7299194335938, KL Reg = 0.0006763850105926394\n",
      "Episode 28: Policy Loss = 14.250818252563477, Value Loss = 786.2076416015625, KL Reg = 0.0007395363645628095\n",
      "Episode 29: Policy Loss = -4.655186176300049, Value Loss = 818.4962768554688, KL Reg = 0.0006696382770314813\n",
      "Episode 30: Policy Loss = -1.2687902450561523, Value Loss = 501.128662109375, KL Reg = 0.0007696251850575209\n",
      "Episode 31: Policy Loss = 12.46805191040039, Value Loss = 912.8328247070312, KL Reg = 0.000777280714828521\n",
      "Episode 32: Policy Loss = -10.163238525390625, Value Loss = 373.62469482421875, KL Reg = 0.0007649861508980393\n",
      "Episode 33: Policy Loss = 10.978716850280762, Value Loss = 700.9910888671875, KL Reg = 0.0007566884160041809\n",
      "Episode 34: Policy Loss = -9.965599060058594, Value Loss = 446.97259521484375, KL Reg = 0.0008472622139379382\n",
      "Episode 35: Policy Loss = 8.786140441894531, Value Loss = 665.8465576171875, KL Reg = 0.0009159791516140103\n",
      "Episode 36: Policy Loss = 11.652616500854492, Value Loss = 733.0987548828125, KL Reg = 0.0008399419602937996\n",
      "Episode 37: Policy Loss = 7.286022186279297, Value Loss = 869.4002075195312, KL Reg = 0.0008569994242861867\n",
      "Episode 38: Policy Loss = -4.962062358856201, Value Loss = 174.7106475830078, KL Reg = 0.0009145142394118011\n",
      "Episode 39: Policy Loss = 11.735840797424316, Value Loss = 819.856689453125, KL Reg = 0.001021173084154725\n",
      "Episode 40: Policy Loss = 17.078779220581055, Value Loss = 964.9102783203125, KL Reg = 0.0009804838337004185\n",
      "Episode 41: Policy Loss = 13.781349182128906, Value Loss = 935.5406494140625, KL Reg = 0.001036743400618434\n",
      "Episode 42: Policy Loss = 10.553094863891602, Value Loss = 884.4703979492188, KL Reg = 0.0011483316775411367\n",
      "Episode 43: Policy Loss = 3.1157283782958984, Value Loss = 867.44482421875, KL Reg = 0.0012756676878780127\n",
      "Episode 44: Policy Loss = 9.284795761108398, Value Loss = 932.34912109375, KL Reg = 0.0012348276795819402\n",
      "Episode 45: Policy Loss = 9.307263374328613, Value Loss = 1153.776611328125, KL Reg = 0.0012588530080392957\n",
      "Episode 46: Policy Loss = 9.336772918701172, Value Loss = 912.3176879882812, KL Reg = 0.0013474503066390753\n",
      "Episode 47: Policy Loss = 2.926954507827759, Value Loss = 915.063720703125, KL Reg = 0.0012593864230439067\n",
      "Episode 48: Policy Loss = 13.200201034545898, Value Loss = 1064.2320556640625, KL Reg = 0.001351115177385509\n",
      "Episode 49: Policy Loss = 22.01775550842285, Value Loss = 1372.95361328125, KL Reg = 0.0014206587802618742\n",
      "Episode 50: Policy Loss = 9.36373519897461, Value Loss = 1181.948974609375, KL Reg = 0.0014957495732232928\n",
      "Episode 51: Policy Loss = 17.38982391357422, Value Loss = 1282.2479248046875, KL Reg = 0.0014272576663643122\n",
      "Episode 52: Policy Loss = 29.680084228515625, Value Loss = 2016.447998046875, KL Reg = 0.0014719062019139528\n",
      "Episode 53: Policy Loss = 11.79954719543457, Value Loss = 1149.929931640625, KL Reg = 0.0014655576087534428\n",
      "Episode 54: Policy Loss = 23.00006866455078, Value Loss = 1661.5037841796875, KL Reg = 0.0015292044263333082\n",
      "Episode 55: Policy Loss = 23.547985076904297, Value Loss = 1753.907958984375, KL Reg = 0.001495844917371869\n",
      "Episode 56: Policy Loss = 24.94106674194336, Value Loss = 1835.0008544921875, KL Reg = 0.0015019116690382361\n",
      "Episode 57: Policy Loss = -5.149930000305176, Value Loss = 511.3501892089844, KL Reg = 0.0015296770725399256\n",
      "Episode 58: Policy Loss = 13.883113861083984, Value Loss = 1422.326171875, KL Reg = 0.0017102197743952274\n",
      "Episode 59: Policy Loss = 7.67513370513916, Value Loss = 732.6346435546875, KL Reg = 0.0017300830222666264\n",
      "Episode 60: Policy Loss = 12.059598922729492, Value Loss = 1125.691650390625, KL Reg = 0.0017340120393782854\n",
      "Episode 61: Policy Loss = 15.709847450256348, Value Loss = 1766.28857421875, KL Reg = 0.0018668725388124585\n",
      "Episode 62: Policy Loss = 5.4572014808654785, Value Loss = 1161.1790771484375, KL Reg = 0.002030283445492387\n",
      "Episode 63: Policy Loss = 23.395328521728516, Value Loss = 2186.6181640625, KL Reg = 0.00204614270478487\n",
      "Episode 64: Policy Loss = 16.78069305419922, Value Loss = 1870.279541015625, KL Reg = 0.0022501377388834953\n",
      "Episode 65: Policy Loss = 28.95863151550293, Value Loss = 2399.03662109375, KL Reg = 0.002246253890916705\n",
      "Episode 66: Policy Loss = 22.542354583740234, Value Loss = 2045.9261474609375, KL Reg = 0.0022481109481304884\n",
      "Episode 67: Policy Loss = -4.074537754058838, Value Loss = 280.06951904296875, KL Reg = 0.0022688701283186674\n",
      "Episode 68: Policy Loss = 27.708972930908203, Value Loss = 2781.60546875, KL Reg = 0.002218295121565461\n",
      "Episode 69: Policy Loss = 24.19673728942871, Value Loss = 2697.887939453125, KL Reg = 0.0022619501687586308\n",
      "Episode 70: Policy Loss = 22.97617530822754, Value Loss = 2200.4169921875, KL Reg = 0.002404967788606882\n",
      "Episode 71: Policy Loss = 22.628719329833984, Value Loss = 2179.709228515625, KL Reg = 0.0025632695760577917\n",
      "Episode 72: Policy Loss = 16.672555923461914, Value Loss = 1933.6915283203125, KL Reg = 0.002541090128943324\n",
      "Episode 73: Policy Loss = 33.51470947265625, Value Loss = 2963.567138671875, KL Reg = 0.0026352268178015947\n",
      "Episode 74: Policy Loss = 32.40113830566406, Value Loss = 2957.48583984375, KL Reg = 0.0027881667483597994\n",
      "Episode 75: Policy Loss = 28.953433990478516, Value Loss = 3006.818115234375, KL Reg = 0.002638637786731124\n",
      "Episode 76: Policy Loss = 35.806278228759766, Value Loss = 2963.43994140625, KL Reg = 0.002550369594246149\n",
      "Episode 77: Policy Loss = 29.11386489868164, Value Loss = 2511.348388671875, KL Reg = 0.0026460879016667604\n",
      "Episode 78: Policy Loss = 33.9842643737793, Value Loss = 3255.97314453125, KL Reg = 0.002722285222262144\n",
      "Episode 79: Policy Loss = 33.17262268066406, Value Loss = 3113.37451171875, KL Reg = 0.0028142950031906366\n",
      "Episode 80: Policy Loss = 9.76301097869873, Value Loss = 2635.149169921875, KL Reg = 0.002909358125180006\n",
      "Episode 81: Policy Loss = 35.51625061035156, Value Loss = 3211.10009765625, KL Reg = 0.003050327766686678\n",
      "Episode 82: Policy Loss = 35.1386604309082, Value Loss = 3105.076171875, KL Reg = 0.003427396062761545\n",
      "Episode 83: Policy Loss = 22.997570037841797, Value Loss = 2641.7509765625, KL Reg = 0.0031463780906051397\n",
      "Episode 84: Policy Loss = 24.973464965820312, Value Loss = 3490.331298828125, KL Reg = 0.0032278583385050297\n",
      "Episode 85: Policy Loss = 21.348371505737305, Value Loss = 2581.174560546875, KL Reg = 0.003498596139252186\n",
      "Episode 86: Policy Loss = 20.859275817871094, Value Loss = 2934.350341796875, KL Reg = 0.002934427931904793\n",
      "Episode 87: Policy Loss = 38.442054748535156, Value Loss = 3452.033935546875, KL Reg = 0.0032109126914292574\n",
      "Episode 88: Policy Loss = 40.94596862792969, Value Loss = 3632.5205078125, KL Reg = 0.0033267405815422535\n",
      "Episode 89: Policy Loss = 34.69622039794922, Value Loss = 3698.81689453125, KL Reg = 0.003082100534811616\n",
      "Episode 90: Policy Loss = 38.778076171875, Value Loss = 3792.481201171875, KL Reg = 0.003448265604674816\n",
      "Episode 91: Policy Loss = 37.18248748779297, Value Loss = 3832.8330078125, KL Reg = 0.003303602570667863\n",
      "Episode 92: Policy Loss = 35.48551559448242, Value Loss = 3166.802978515625, KL Reg = 0.003550510387867689\n",
      "Episode 93: Policy Loss = 20.86754608154297, Value Loss = 3463.888916015625, KL Reg = 0.0036803055554628372\n",
      "Episode 94: Policy Loss = 35.14580535888672, Value Loss = 3890.347900390625, KL Reg = 0.0035176482051610947\n",
      "Episode 95: Policy Loss = 34.446163177490234, Value Loss = 3992.851318359375, KL Reg = 0.003988741431385279\n",
      "Episode 96: Policy Loss = 11.324823379516602, Value Loss = 2572.237060546875, KL Reg = 0.0032906117849051952\n",
      "Episode 97: Policy Loss = 39.11376953125, Value Loss = 3243.10986328125, KL Reg = 0.0035628837067633867\n",
      "Episode 98: Policy Loss = 32.541053771972656, Value Loss = 3273.417236328125, KL Reg = 0.0034077567979693413\n",
      "Episode 99: Policy Loss = 51.95586395263672, Value Loss = 5815.44189453125, KL Reg = 0.0042447964660823345\n",
      "Episode 100: Policy Loss = 39.39640808105469, Value Loss = 3621.012939453125, KL Reg = 0.004401464946568012\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    RobustAgent = SAPPOAgent(state_dim, action_dim, discrete)\n",
    "    RobustAgent.train(env, max_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:01:21.910347Z",
     "iopub.status.busy": "2024-12-05T15:01:21.909987Z",
     "iopub.status.idle": "2024-12-05T15:01:43.572860Z",
     "shell.execute_reply": "2024-12-05T15:01:43.571935Z",
     "shell.execute_reply.started": "2024-12-05T15:01:21.910308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 578.1060141282485\n",
      "Episode 2: Reward = 191.48752509190723\n",
      "Episode 3: Reward = 516.8291325792161\n",
      "Episode 4: Reward = 518.6161120277076\n",
      "Episode 5: Reward = 559.901385298283\n",
      "Episode 6: Reward = 319.5635004596396\n",
      "Episode 7: Reward = 140.9922717474108\n",
      "Episode 8: Reward = 182.01958497695213\n",
      "Episode 9: Reward = 674.3009771136836\n",
      "Episode 10: Reward = 263.18558627778674\n",
      "Episode 11: Reward = 374.87454495694556\n",
      "Episode 12: Reward = 318.9757919315372\n",
      "Episode 13: Reward = 126.44828207642213\n",
      "Episode 14: Reward = 518.5822160722652\n",
      "Episode 15: Reward = 644.7057108556683\n",
      "Episode 16: Reward = 632.5270357631548\n",
      "Episode 17: Reward = 564.3636714239185\n",
      "Episode 18: Reward = 664.2489390736816\n",
      "Episode 19: Reward = 573.6683301090893\n",
      "Episode 20: Reward = 305.52238955232593\n",
      "Episode 21: Reward = 611.2088911674298\n",
      "Episode 22: Reward = 354.85071686288865\n",
      "Episode 23: Reward = 330.75717447749935\n",
      "Episode 24: Reward = 511.43509659663096\n",
      "Episode 25: Reward = 348.41043835437176\n",
      "Episode 26: Reward = 643.327950545341\n",
      "Episode 27: Reward = 461.65142123710393\n",
      "Episode 28: Reward = 414.6655068203736\n",
      "Episode 29: Reward = 315.2663461868649\n",
      "Episode 30: Reward = 568.600127583\n",
      "Episode 31: Reward = 544.1119458832194\n",
      "Episode 32: Reward = 452.69474364900816\n",
      "Episode 33: Reward = 610.8781194698868\n",
      "Episode 34: Reward = 321.03713434280996\n",
      "Episode 35: Reward = 611.9834839566129\n",
      "Episode 36: Reward = 557.3131019514302\n",
      "Episode 37: Reward = 574.07064996235\n",
      "Episode 38: Reward = 464.6187998361534\n",
      "Episode 39: Reward = 628.5424485011067\n",
      "Episode 40: Reward = 574.5152784482904\n",
      "Episode 41: Reward = 644.7106593045824\n",
      "Episode 42: Reward = 572.0817221637975\n",
      "Episode 43: Reward = 571.4406024224256\n",
      "Episode 44: Reward = 612.1550938067144\n",
      "Episode 45: Reward = 216.256801198691\n",
      "Episode 46: Reward = 293.99651450356265\n",
      "Episode 47: Reward = 664.1679667507817\n",
      "Episode 48: Reward = 133.4432452946481\n",
      "Episode 49: Reward = 325.3957614350417\n",
      "Episode 50: Reward = 447.6459975463178\n",
      "Episode 51: Reward = 668.4151165153452\n",
      "Episode 52: Reward = 586.1528032519637\n",
      "Episode 53: Reward = 563.4722662305093\n",
      "Episode 54: Reward = 494.74428392630017\n",
      "Episode 55: Reward = 571.9410883064321\n",
      "Episode 56: Reward = 505.19067478966394\n",
      "Episode 57: Reward = 602.042885126491\n",
      "Episode 58: Reward = 902.2879350468758\n",
      "Episode 59: Reward = 608.228603983518\n",
      "Episode 60: Reward = 677.6566580195932\n",
      "Episode 61: Reward = 307.7515176364144\n",
      "Episode 62: Reward = 320.8435329939328\n",
      "Episode 63: Reward = 350.4562995856722\n",
      "Episode 64: Reward = 291.1896997548772\n",
      "Episode 65: Reward = 339.42005318402494\n",
      "Episode 66: Reward = 400.351216960519\n",
      "Episode 67: Reward = 609.1868456210572\n",
      "Episode 68: Reward = 556.791070838709\n",
      "Episode 69: Reward = 538.420387961814\n",
      "Episode 70: Reward = 614.0999076225072\n",
      "Episode 71: Reward = 591.4314674300049\n",
      "Episode 72: Reward = 491.2904231484224\n",
      "Episode 73: Reward = 342.47712308741126\n",
      "Episode 74: Reward = 653.4338718883839\n",
      "Episode 75: Reward = 206.59171225010786\n",
      "Episode 76: Reward = 368.65106390333517\n",
      "Episode 77: Reward = 579.1015943020224\n",
      "Episode 78: Reward = 606.5366478209025\n",
      "Episode 79: Reward = 149.64877926130694\n",
      "Episode 80: Reward = 541.0990266522388\n",
      "Episode 81: Reward = 402.3263948311904\n",
      "Episode 82: Reward = 366.9913101532087\n",
      "Episode 83: Reward = 261.7004991422637\n",
      "Episode 84: Reward = 611.6107326924081\n",
      "Episode 85: Reward = 125.50184535910958\n",
      "Episode 86: Reward = 537.5697527551063\n",
      "Episode 87: Reward = 559.1665751744\n",
      "Episode 88: Reward = 453.039736980122\n",
      "Episode 89: Reward = 624.4777243509047\n",
      "Episode 90: Reward = 552.5104752506011\n",
      "Episode 91: Reward = 399.2472669794968\n",
      "Episode 92: Reward = 578.6733391868662\n",
      "Episode 93: Reward = 592.100574045002\n",
      "Episode 94: Reward = 610.6802821338788\n",
      "Episode 95: Reward = 614.1954618565981\n",
      "Episode 96: Reward = 533.4350306900033\n",
      "Episode 97: Reward = 577.2663121682567\n",
      "Episode 98: Reward = 309.7183880052355\n",
      "Episode 99: Reward = 548.6361419991987\n",
      "Episode 100: Reward = 301.4120644325085\n",
      "Episode 101: Reward = 686.6967864627903\n",
      "Episode 102: Reward = 486.5897639866639\n",
      "Episode 103: Reward = 633.116899004689\n",
      "Episode 104: Reward = 606.6867331473604\n",
      "Episode 105: Reward = 608.2059416389853\n",
      "Episode 106: Reward = 630.7174402463528\n",
      "Episode 107: Reward = 563.7321413241243\n",
      "Episode 108: Reward = 605.1526281479199\n",
      "Episode 109: Reward = 481.76782606632315\n",
      "Episode 110: Reward = 526.5908746361582\n",
      "Episode 111: Reward = 545.8005946409328\n",
      "Episode 112: Reward = 586.6264042375843\n",
      "Episode 113: Reward = 644.9615418877576\n",
      "Episode 114: Reward = 583.8170970342205\n",
      "Episode 115: Reward = 553.7428695010499\n",
      "Episode 116: Reward = 337.6319480367827\n",
      "Episode 117: Reward = 637.2507179162031\n",
      "Episode 118: Reward = 587.1862460440432\n",
      "Episode 119: Reward = 538.5533790199919\n",
      "Episode 120: Reward = 530.701838707466\n",
      "Episode 121: Reward = 334.7855326829923\n",
      "Episode 122: Reward = 316.5325313337832\n",
      "Episode 123: Reward = 500.44607657338287\n",
      "Episode 124: Reward = 499.1356245506901\n",
      "Episode 125: Reward = 341.2812857258509\n",
      "Episode 126: Reward = 520.8365425716153\n",
      "Episode 127: Reward = 352.0287540333039\n",
      "Episode 128: Reward = 517.2396041484301\n",
      "Episode 129: Reward = 739.7373160059662\n",
      "Episode 130: Reward = 521.3535676428123\n",
      "Episode 131: Reward = 318.5160304337427\n",
      "Episode 132: Reward = 292.309892467821\n",
      "Episode 133: Reward = 546.3502095396653\n",
      "Episode 134: Reward = 379.2870821340961\n",
      "Episode 135: Reward = 486.6848021436455\n",
      "Episode 136: Reward = 309.9892836305359\n",
      "Episode 137: Reward = 963.1136191934379\n",
      "Episode 138: Reward = 712.9772419506257\n",
      "Episode 139: Reward = 318.32659358515053\n",
      "Episode 140: Reward = 533.405025781678\n",
      "Episode 141: Reward = 665.1855331598863\n",
      "Episode 142: Reward = 478.06265385409165\n",
      "Episode 143: Reward = 620.6439962789714\n",
      "Episode 144: Reward = 469.36832178136757\n",
      "Episode 145: Reward = 250.8563751380784\n",
      "Episode 146: Reward = 643.4780748455421\n",
      "Episode 147: Reward = 536.5829558292203\n",
      "Episode 148: Reward = 325.6583532659074\n",
      "Episode 149: Reward = 611.214943439194\n",
      "Episode 150: Reward = 351.2259884122416\n",
      "Episode 151: Reward = 558.5236041489607\n",
      "Episode 152: Reward = 677.5552391922281\n",
      "Episode 153: Reward = 542.0875075132847\n",
      "Episode 154: Reward = 558.2793433814824\n",
      "Episode 155: Reward = 561.8130089736736\n",
      "Episode 156: Reward = 314.194879753742\n",
      "Episode 157: Reward = 434.032808851723\n",
      "Episode 158: Reward = 602.0900476711284\n",
      "Episode 159: Reward = 584.9642344912323\n",
      "Episode 160: Reward = 877.9484903964989\n",
      "Episode 161: Reward = 597.699430762897\n",
      "Episode 162: Reward = 608.2441078244877\n",
      "Episode 163: Reward = 312.6890892630975\n",
      "Episode 164: Reward = 566.4939221947935\n",
      "Episode 165: Reward = 626.995759220625\n",
      "Episode 166: Reward = 488.56782446113374\n",
      "Episode 167: Reward = 521.7044209548982\n",
      "Episode 168: Reward = 448.4167635828074\n",
      "Episode 169: Reward = 629.0840905227401\n",
      "Episode 170: Reward = 624.7371487627823\n",
      "Episode 171: Reward = 623.5002251844959\n",
      "Episode 172: Reward = 503.8545531008516\n",
      "Episode 173: Reward = 682.730630952835\n",
      "Episode 174: Reward = 650.5146815535453\n",
      "Episode 175: Reward = 612.582635817437\n",
      "Episode 176: Reward = 575.5379440373629\n",
      "Episode 177: Reward = 543.8456497239611\n",
      "Episode 178: Reward = 596.8460566474919\n",
      "Episode 179: Reward = 659.5709594626124\n",
      "Episode 180: Reward = 570.2677545572416\n",
      "Episode 181: Reward = 561.5554642963841\n",
      "Episode 182: Reward = 547.9781192858658\n",
      "Episode 183: Reward = 694.7427751251213\n",
      "Episode 184: Reward = 456.05344209480387\n",
      "Episode 185: Reward = 618.7756776301134\n",
      "Episode 186: Reward = 633.5413870354649\n",
      "Episode 187: Reward = 336.80505622047644\n",
      "Episode 188: Reward = 522.4885961117069\n",
      "Episode 189: Reward = 664.5817935573348\n",
      "Episode 190: Reward = 223.28647098048222\n",
      "Episode 191: Reward = 608.3718919668106\n",
      "Episode 192: Reward = 315.8934929269332\n",
      "Episode 193: Reward = 189.78062315806346\n",
      "Episode 194: Reward = 584.845485399662\n",
      "Episode 195: Reward = 839.1182463733322\n",
      "Episode 196: Reward = 446.8850659362644\n",
      "Episode 197: Reward = 548.0056386439823\n",
      "Episode 198: Reward = 502.63100399084817\n",
      "Episode 199: Reward = 466.991944502129\n",
      "Episode 200: Reward = 602.4715891132241\n",
      "Average Reward over 200 Episodes: 505.00818634096834\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, RobustAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:01:43.574810Z",
     "iopub.status.busy": "2024-12-05T15:01:43.574392Z",
     "iopub.status.idle": "2024-12-05T15:02:04.297840Z",
     "shell.execute_reply": "2024-12-05T15:02:04.296937Z",
     "shell.execute_reply.started": "2024-12-05T15:01:43.574767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 496.5445431706002\n",
      "Episode 2: Reward = 627.907378253641\n",
      "Episode 3: Reward = 631.8893375377107\n",
      "Episode 4: Reward = 634.9780713895062\n",
      "Episode 5: Reward = 637.997521321534\n",
      "Episode 6: Reward = 612.4174651703072\n",
      "Episode 7: Reward = 638.3541925099875\n",
      "Episode 8: Reward = 630.1991384244017\n",
      "Episode 9: Reward = 631.4799199351878\n",
      "Episode 10: Reward = 651.6785464018815\n",
      "Episode 11: Reward = 632.7606364388131\n",
      "Episode 12: Reward = 632.957587240793\n",
      "Episode 13: Reward = 621.6552098302561\n",
      "Episode 14: Reward = 643.6775720853248\n",
      "Episode 15: Reward = 647.0049766545009\n",
      "Episode 16: Reward = 636.8676021049968\n",
      "Episode 17: Reward = 617.4209921512115\n",
      "Episode 18: Reward = 630.6379954478923\n",
      "Episode 19: Reward = 630.217643606485\n",
      "Episode 20: Reward = 632.8507157833255\n",
      "Episode 21: Reward = 644.7251352426781\n",
      "Episode 22: Reward = 640.5816812446527\n",
      "Episode 23: Reward = 632.7348806005328\n",
      "Episode 24: Reward = 627.3222975359677\n",
      "Episode 25: Reward = 640.9199106153292\n",
      "Episode 26: Reward = 580.0525721600941\n",
      "Episode 27: Reward = 632.2795447837448\n",
      "Episode 28: Reward = 644.5059282175762\n",
      "Episode 29: Reward = 648.9040878598215\n",
      "Episode 30: Reward = 622.0362171290915\n",
      "Episode 31: Reward = 643.0849496162735\n",
      "Episode 32: Reward = 643.2648604288087\n",
      "Episode 33: Reward = 631.9547169508083\n",
      "Episode 34: Reward = 635.9426704418737\n",
      "Episode 35: Reward = 637.0313887577169\n",
      "Episode 36: Reward = 617.4824386016207\n",
      "Episode 37: Reward = 541.388869532943\n",
      "Episode 38: Reward = 647.3158167497324\n",
      "Episode 39: Reward = 633.794868473074\n",
      "Episode 40: Reward = 625.6734601363046\n",
      "Episode 41: Reward = 628.8459886533979\n",
      "Episode 42: Reward = 633.7095510156088\n",
      "Episode 43: Reward = 635.195796453435\n",
      "Episode 44: Reward = 645.2854088720455\n",
      "Episode 45: Reward = 644.4510484418892\n",
      "Episode 46: Reward = 618.7803345752108\n",
      "Episode 47: Reward = 620.2508033886636\n",
      "Episode 48: Reward = 637.2357254822389\n",
      "Episode 49: Reward = 637.5457200472961\n",
      "Episode 50: Reward = 620.6702890951531\n",
      "Episode 51: Reward = 629.9883947581235\n",
      "Episode 52: Reward = 641.8130041576824\n",
      "Episode 53: Reward = 633.1380196001314\n",
      "Episode 54: Reward = 633.4346267651671\n",
      "Episode 55: Reward = 645.8735498127182\n",
      "Episode 56: Reward = 640.8980606925808\n",
      "Episode 57: Reward = 634.868165296091\n",
      "Episode 58: Reward = 622.3205306013041\n",
      "Episode 59: Reward = 597.8010846237657\n",
      "Episode 60: Reward = 635.3332329727875\n",
      "Episode 61: Reward = 628.3669449651657\n",
      "Episode 62: Reward = 612.3883631313396\n",
      "Episode 63: Reward = 637.6466198774065\n",
      "Episode 64: Reward = 547.131238263099\n",
      "Episode 65: Reward = 550.3254009239436\n",
      "Episode 66: Reward = 632.9152280128188\n",
      "Episode 67: Reward = 622.2081172985268\n",
      "Episode 68: Reward = 632.7315061956685\n",
      "Episode 69: Reward = 613.9757391727028\n",
      "Episode 70: Reward = 637.6141208883954\n",
      "Episode 71: Reward = 617.8759226550085\n",
      "Episode 72: Reward = 639.8380339663977\n",
      "Episode 73: Reward = 642.6915809908614\n",
      "Episode 74: Reward = 544.725701079859\n",
      "Episode 75: Reward = 628.4621159451688\n",
      "Episode 76: Reward = 618.1604862459232\n",
      "Episode 77: Reward = 621.5725783151678\n",
      "Episode 78: Reward = 645.728480379987\n",
      "Episode 79: Reward = 638.5797839962006\n",
      "Episode 80: Reward = 635.5164828809415\n",
      "Episode 81: Reward = 640.7539024400288\n",
      "Episode 82: Reward = 628.1157311421604\n",
      "Episode 83: Reward = 621.4348835846027\n",
      "Episode 84: Reward = 642.7257891412802\n",
      "Episode 85: Reward = 641.7020433820727\n",
      "Episode 86: Reward = 633.7426605930555\n",
      "Episode 87: Reward = 619.0063885154298\n",
      "Episode 88: Reward = 604.6892159982714\n",
      "Episode 89: Reward = 636.5155875432649\n",
      "Episode 90: Reward = 639.4060284160023\n",
      "Episode 91: Reward = 646.4317524758961\n",
      "Episode 92: Reward = 649.0692642503326\n",
      "Episode 93: Reward = 640.2415828695481\n",
      "Episode 94: Reward = 612.3210856429779\n",
      "Episode 95: Reward = 620.1189917945501\n",
      "Episode 96: Reward = 645.638286921184\n",
      "Episode 97: Reward = 653.5754541681143\n",
      "Episode 98: Reward = 652.1824732561611\n",
      "Episode 99: Reward = 637.6100493100129\n",
      "Episode 100: Reward = 560.8671579764205\n",
      "Episode 101: Reward = 637.5318145088725\n",
      "Episode 102: Reward = 633.0909784749504\n",
      "Episode 103: Reward = 481.3475929129308\n",
      "Episode 104: Reward = 640.3370619473058\n",
      "Episode 105: Reward = 639.6540440404262\n",
      "Episode 106: Reward = 642.1127369678549\n",
      "Episode 107: Reward = 520.4102393642701\n",
      "Episode 108: Reward = 650.5535808848679\n",
      "Episode 109: Reward = 612.6504334047045\n",
      "Episode 110: Reward = 645.4778965855304\n",
      "Episode 111: Reward = 608.5320696117533\n",
      "Episode 112: Reward = 622.3895653542255\n",
      "Episode 113: Reward = 636.4136062010197\n",
      "Episode 114: Reward = 634.8920755193458\n",
      "Episode 115: Reward = 640.4944652788618\n",
      "Episode 116: Reward = 639.5210501482946\n",
      "Episode 117: Reward = 639.1228540891835\n",
      "Episode 118: Reward = 622.8586379784415\n",
      "Episode 119: Reward = 587.8190679470072\n",
      "Episode 120: Reward = 637.1989397320411\n",
      "Episode 121: Reward = 566.5812967076572\n",
      "Episode 122: Reward = 645.5693780278232\n",
      "Episode 123: Reward = 641.1184347692841\n",
      "Episode 124: Reward = 655.1364527015729\n",
      "Episode 125: Reward = 619.5334881091477\n",
      "Episode 126: Reward = 637.2750809714673\n",
      "Episode 127: Reward = 626.6185152001965\n",
      "Episode 128: Reward = 551.3644102508384\n",
      "Episode 129: Reward = 622.5389229909291\n",
      "Episode 130: Reward = 622.8579213026846\n",
      "Episode 131: Reward = 571.127124071336\n",
      "Episode 132: Reward = 643.8323821043887\n",
      "Episode 133: Reward = 631.9916108311209\n",
      "Episode 134: Reward = 643.1556023949356\n",
      "Episode 135: Reward = 633.1734090681464\n",
      "Episode 136: Reward = 643.6114848047762\n",
      "Episode 137: Reward = 643.8271604702343\n",
      "Episode 138: Reward = 618.0961144519205\n",
      "Episode 139: Reward = 625.0991744940543\n",
      "Episode 140: Reward = 641.9115896848739\n",
      "Episode 141: Reward = 636.2493829413681\n",
      "Episode 142: Reward = 627.6821440706685\n",
      "Episode 143: Reward = 638.6791096410191\n",
      "Episode 144: Reward = 640.3372966777328\n",
      "Episode 145: Reward = 633.5566500405009\n",
      "Episode 146: Reward = 639.9471260771863\n",
      "Episode 147: Reward = 624.7562810247173\n",
      "Episode 148: Reward = 619.5260160796905\n",
      "Episode 149: Reward = 591.0614201444112\n",
      "Episode 150: Reward = 634.047646439496\n",
      "Episode 151: Reward = 640.0790707426814\n",
      "Episode 152: Reward = 648.7388471318507\n",
      "Episode 153: Reward = 646.1132745754871\n",
      "Episode 154: Reward = 624.1018395404727\n",
      "Episode 155: Reward = 641.9399768249957\n",
      "Episode 156: Reward = 642.0495538482862\n",
      "Episode 157: Reward = 624.1050171929405\n",
      "Episode 158: Reward = 620.8821699876091\n",
      "Episode 159: Reward = 570.0105025838759\n",
      "Episode 160: Reward = 644.83166041312\n",
      "Episode 161: Reward = 654.2630584758999\n",
      "Episode 162: Reward = 591.138985226872\n",
      "Episode 163: Reward = 614.5632359055913\n",
      "Episode 164: Reward = 568.6118067361831\n",
      "Episode 165: Reward = 648.5918756925042\n",
      "Episode 166: Reward = 622.1428864891412\n",
      "Episode 167: Reward = 644.3928277080871\n",
      "Episode 168: Reward = 628.0258531912901\n",
      "Episode 169: Reward = 564.5784001708251\n",
      "Episode 170: Reward = 588.9192859783653\n",
      "Episode 171: Reward = 630.0754537719196\n",
      "Episode 172: Reward = 618.8991441381806\n",
      "Episode 173: Reward = 643.8954221525225\n",
      "Episode 174: Reward = 638.095212408545\n",
      "Episode 175: Reward = 605.2884052966172\n",
      "Episode 176: Reward = 639.5637839235816\n",
      "Episode 177: Reward = 634.1356786204761\n",
      "Episode 178: Reward = 614.0735597743208\n",
      "Episode 179: Reward = 628.7864440652927\n",
      "Episode 180: Reward = 617.3897267371465\n",
      "Episode 181: Reward = 628.6541791176579\n",
      "Episode 182: Reward = 631.1554189044431\n",
      "Episode 183: Reward = 636.6882395988507\n",
      "Episode 184: Reward = 638.7746598203778\n",
      "Episode 185: Reward = 625.5971606457847\n",
      "Episode 186: Reward = 622.3342792065981\n",
      "Episode 187: Reward = 641.9233720457197\n",
      "Episode 188: Reward = 638.6145528130128\n",
      "Episode 189: Reward = 633.0318177720288\n",
      "Episode 190: Reward = 643.8501650325439\n",
      "Episode 191: Reward = 635.6970044426083\n",
      "Episode 192: Reward = 649.1945407937815\n",
      "Episode 193: Reward = 627.5526763152739\n",
      "Episode 194: Reward = 629.6849754033428\n",
      "Episode 195: Reward = 644.7632825999443\n",
      "Episode 196: Reward = 639.4615079565015\n",
      "Episode 197: Reward = 616.9046248400081\n",
      "Episode 198: Reward = 627.1867983406796\n",
      "Episode 199: Reward = 588.0340075353778\n",
      "Episode 200: Reward = 626.8684236549918\n",
      "Average Reward over 200 episodes: 625.8473071704427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "625.8473071704427"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:02:04.299207Z",
     "iopub.status.busy": "2024-12-05T15:02:04.298940Z",
     "iopub.status.idle": "2024-12-05T15:14:19.975439Z",
     "shell.execute_reply": "2024-12-05T15:14:19.974484Z",
     "shell.execute_reply.started": "2024-12-05T15:02:04.299182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 318.07371935394053\n",
      "Episode 2/200: Reward = 367.1858283355137\n",
      "Episode 3/200: Reward = 327.863413739362\n",
      "Episode 4/200: Reward = 299.5880341788426\n",
      "Episode 5/200: Reward = 461.847489339165\n",
      "Episode 6/200: Reward = 323.335270603237\n",
      "Episode 7/200: Reward = 452.83587682220315\n",
      "Episode 8/200: Reward = 637.105368294214\n",
      "Episode 9/200: Reward = 199.4958548565722\n",
      "Episode 10/200: Reward = 514.0955506078654\n",
      "Episode 11/200: Reward = 230.7348997691168\n",
      "Episode 12/200: Reward = 629.7321137042236\n",
      "Episode 13/200: Reward = 516.6775970784363\n",
      "Episode 14/200: Reward = 328.4048491459208\n",
      "Episode 15/200: Reward = 960.4494275438652\n",
      "Episode 16/200: Reward = 308.38990097317725\n",
      "Episode 17/200: Reward = 483.1248303359048\n",
      "Episode 18/200: Reward = 579.5756659889504\n",
      "Episode 19/200: Reward = 527.8560888025761\n",
      "Episode 20/200: Reward = 376.3116510315786\n",
      "Episode 21/200: Reward = 340.7283630567716\n",
      "Episode 22/200: Reward = 591.6572040286092\n",
      "Episode 23/200: Reward = 331.5584265904711\n",
      "Episode 24/200: Reward = 606.3812848186594\n",
      "Episode 25/200: Reward = 557.23805828115\n",
      "Episode 26/200: Reward = 580.2851781835648\n",
      "Episode 27/200: Reward = 288.60960394319824\n",
      "Episode 28/200: Reward = 346.0441887122299\n",
      "Episode 29/200: Reward = 460.1128125759375\n",
      "Episode 30/200: Reward = 617.1250972342347\n",
      "Episode 31/200: Reward = 597.9494077085728\n",
      "Episode 32/200: Reward = 495.33422477250866\n",
      "Episode 33/200: Reward = 355.34631212621275\n",
      "Episode 34/200: Reward = 638.2496839337912\n",
      "Episode 35/200: Reward = 336.49478516682547\n",
      "Episode 36/200: Reward = 564.394537586372\n",
      "Episode 37/200: Reward = 331.91503854324674\n",
      "Episode 38/200: Reward = 561.3920076053762\n",
      "Episode 39/200: Reward = 581.3579844509145\n",
      "Episode 40/200: Reward = 588.6771801103334\n",
      "Episode 41/200: Reward = 535.4994762194923\n",
      "Episode 42/200: Reward = 593.098739441334\n",
      "Episode 43/200: Reward = 494.82349252004803\n",
      "Episode 44/200: Reward = 546.7996412850302\n",
      "Episode 45/200: Reward = 503.69290007227147\n",
      "Episode 46/200: Reward = 303.9971638273831\n",
      "Episode 47/200: Reward = 324.9314872798702\n",
      "Episode 48/200: Reward = 685.0966233022436\n",
      "Episode 49/200: Reward = 459.43942466143324\n",
      "Episode 50/200: Reward = 445.3576439598406\n",
      "Episode 51/200: Reward = 524.7465101647275\n",
      "Episode 52/200: Reward = 519.4012678249281\n",
      "Episode 53/200: Reward = 562.0043060226652\n",
      "Episode 54/200: Reward = 491.4704876516368\n",
      "Episode 55/200: Reward = 561.3105454178622\n",
      "Episode 56/200: Reward = 572.9310397551562\n",
      "Episode 57/200: Reward = 442.6156088083428\n",
      "Episode 58/200: Reward = 588.1986157222294\n",
      "Episode 59/200: Reward = 498.0896017542185\n",
      "Episode 60/200: Reward = 521.5550711609853\n",
      "Episode 61/200: Reward = 536.854858223209\n",
      "Episode 62/200: Reward = 841.7729191903841\n",
      "Episode 63/200: Reward = 309.5378712835258\n",
      "Episode 64/200: Reward = 596.4378349028307\n",
      "Episode 65/200: Reward = 364.2552595369028\n",
      "Episode 66/200: Reward = 702.9823940943979\n",
      "Episode 67/200: Reward = 361.41164101556654\n",
      "Episode 68/200: Reward = 329.81542575368\n",
      "Episode 69/200: Reward = 568.5919132188518\n",
      "Episode 70/200: Reward = 483.3908130281464\n",
      "Episode 71/200: Reward = 538.4487696620853\n",
      "Episode 72/200: Reward = 310.8588696969033\n",
      "Episode 73/200: Reward = 571.627022612042\n",
      "Episode 74/200: Reward = 633.3159147944156\n",
      "Episode 75/200: Reward = 643.5262329868376\n",
      "Episode 76/200: Reward = 352.8513829734828\n",
      "Episode 77/200: Reward = 348.6008685416605\n",
      "Episode 78/200: Reward = 564.4100306184118\n",
      "Episode 79/200: Reward = 656.2100331198669\n",
      "Episode 80/200: Reward = 644.9222398689886\n",
      "Episode 81/200: Reward = 557.0059045243944\n",
      "Episode 82/200: Reward = 191.37922590874595\n",
      "Episode 83/200: Reward = 153.70086225532563\n",
      "Episode 84/200: Reward = 466.27769227215754\n",
      "Episode 85/200: Reward = 539.0705082252788\n",
      "Episode 86/200: Reward = 526.6590430359871\n",
      "Episode 87/200: Reward = 329.2399014235096\n",
      "Episode 88/200: Reward = 556.42353415547\n",
      "Episode 89/200: Reward = 641.313451489275\n",
      "Episode 90/200: Reward = 201.06688199010304\n",
      "Episode 91/200: Reward = 611.2335617711003\n",
      "Episode 92/200: Reward = 473.09289870449976\n",
      "Episode 93/200: Reward = 434.3454296424519\n",
      "Episode 94/200: Reward = 189.7612524948869\n",
      "Episode 95/200: Reward = 382.324463433103\n",
      "Episode 96/200: Reward = 300.733548110398\n",
      "Episode 97/200: Reward = 587.5351928340534\n",
      "Episode 98/200: Reward = 508.00253441785776\n",
      "Episode 99/200: Reward = 659.0520625205004\n",
      "Episode 100/200: Reward = 331.1622629531484\n",
      "Episode 101/200: Reward = 183.63117336359576\n",
      "Episode 102/200: Reward = 275.68787535501525\n",
      "Episode 103/200: Reward = 521.7889796685721\n",
      "Episode 104/200: Reward = 290.638972233018\n",
      "Episode 105/200: Reward = 533.6249823659421\n",
      "Episode 106/200: Reward = 565.122875593649\n",
      "Episode 107/200: Reward = 312.0501744373278\n",
      "Episode 108/200: Reward = 134.28067465366067\n",
      "Episode 109/200: Reward = 884.0863925689824\n",
      "Episode 110/200: Reward = 284.2747639719026\n",
      "Episode 111/200: Reward = 294.6535583536907\n",
      "Episode 112/200: Reward = 657.3651454684957\n",
      "Episode 113/200: Reward = 433.33917885835615\n",
      "Episode 114/200: Reward = 706.0327967948875\n",
      "Episode 115/200: Reward = 765.4802642316083\n",
      "Episode 116/200: Reward = 241.4239420673129\n",
      "Episode 117/200: Reward = 351.6486291443945\n",
      "Episode 118/200: Reward = 322.38213356457925\n",
      "Episode 119/200: Reward = 418.44203313787\n",
      "Episode 120/200: Reward = 510.5086504731435\n",
      "Episode 121/200: Reward = 571.5116582260436\n",
      "Episode 122/200: Reward = 646.4081935766197\n",
      "Episode 123/200: Reward = 426.0375976713737\n",
      "Episode 124/200: Reward = 322.43630648729174\n",
      "Episode 125/200: Reward = 541.1621264537404\n",
      "Episode 126/200: Reward = 609.6711329925009\n",
      "Episode 127/200: Reward = 333.32354010909415\n",
      "Episode 128/200: Reward = 633.1714396699223\n",
      "Episode 129/200: Reward = 288.309635498775\n",
      "Episode 130/200: Reward = 322.373979275554\n",
      "Episode 131/200: Reward = 145.52118185749507\n",
      "Episode 132/200: Reward = 504.90887576668524\n",
      "Episode 133/200: Reward = 313.87808626176576\n",
      "Episode 134/200: Reward = 587.2306442555619\n",
      "Episode 135/200: Reward = 353.2718306232786\n",
      "Episode 136/200: Reward = 562.2706584347804\n",
      "Episode 137/200: Reward = 594.8647005757718\n",
      "Episode 138/200: Reward = 304.07616510189894\n",
      "Episode 139/200: Reward = 189.29376344252702\n",
      "Episode 140/200: Reward = 362.4480584818887\n",
      "Episode 141/200: Reward = 621.7256732127407\n",
      "Episode 142/200: Reward = 546.4330530786165\n",
      "Episode 143/200: Reward = 492.7129240608795\n",
      "Episode 144/200: Reward = 301.9373419657666\n",
      "Episode 145/200: Reward = 371.6111256670204\n",
      "Episode 146/200: Reward = 588.1082470054703\n",
      "Episode 147/200: Reward = 943.7041822732513\n",
      "Episode 148/200: Reward = 596.193687918274\n",
      "Episode 149/200: Reward = 653.4283607900347\n",
      "Episode 150/200: Reward = 532.9170584686286\n",
      "Episode 151/200: Reward = 616.9373233785095\n",
      "Episode 152/200: Reward = 488.30453842324215\n",
      "Episode 153/200: Reward = 393.1595237377925\n",
      "Episode 154/200: Reward = 592.192083447816\n",
      "Episode 155/200: Reward = 357.2196838465674\n",
      "Episode 156/200: Reward = 566.3213168261832\n",
      "Episode 157/200: Reward = 295.8048938798436\n",
      "Episode 158/200: Reward = 269.8794025467892\n",
      "Episode 159/200: Reward = 344.4823589389132\n",
      "Episode 160/200: Reward = 649.7511476270804\n",
      "Episode 161/200: Reward = 295.25921877822583\n",
      "Episode 162/200: Reward = 333.89896538589545\n",
      "Episode 163/200: Reward = 612.6376973098963\n",
      "Episode 164/200: Reward = 571.5705446207293\n",
      "Episode 165/200: Reward = 468.8766687373278\n",
      "Episode 166/200: Reward = 614.9671528913248\n",
      "Episode 167/200: Reward = 292.77858606669497\n",
      "Episode 168/200: Reward = 672.6683573075735\n",
      "Episode 169/200: Reward = 809.8339785565956\n",
      "Episode 170/200: Reward = 338.87617259263334\n",
      "Episode 171/200: Reward = 242.4315934901095\n",
      "Episode 172/200: Reward = 583.3324954556914\n",
      "Episode 173/200: Reward = 351.2978786536491\n",
      "Episode 174/200: Reward = 588.2599970266742\n",
      "Episode 175/200: Reward = 225.05457283283303\n",
      "Episode 176/200: Reward = 496.8864324939402\n",
      "Episode 177/200: Reward = 659.0195908685421\n",
      "Episode 178/200: Reward = 469.1440487744509\n",
      "Episode 179/200: Reward = 381.9034123455086\n",
      "Episode 180/200: Reward = 144.20382073552383\n",
      "Episode 181/200: Reward = 271.2950064433232\n",
      "Episode 182/200: Reward = 584.7462394749541\n",
      "Episode 183/200: Reward = 738.2201870560327\n",
      "Episode 184/200: Reward = 581.4535475606942\n",
      "Episode 185/200: Reward = 595.0252720365959\n",
      "Episode 186/200: Reward = 376.97824543479754\n",
      "Episode 187/200: Reward = 585.1477961444198\n",
      "Episode 188/200: Reward = 88.18960778520888\n",
      "Episode 189/200: Reward = 558.1297188360755\n",
      "Episode 190/200: Reward = 125.89514417201251\n",
      "Episode 191/200: Reward = 581.4370476416663\n",
      "Episode 192/200: Reward = 590.8452436907421\n",
      "Episode 193/200: Reward = 624.3286247297091\n",
      "Episode 194/200: Reward = 623.8622394335001\n",
      "Episode 195/200: Reward = 433.5896453642232\n",
      "Episode 196/200: Reward = 615.9953810238642\n",
      "Episode 197/200: Reward = 557.4823693307694\n",
      "Episode 198/200: Reward = 587.7313416106682\n",
      "Episode 199/200: Reward = 666.0028815868311\n",
      "Episode 200/200: Reward = 596.8365948109251\n",
      "Average Reward under MAD attack: 474.226734906613\n",
      "Final Average Reward under MAD Attack: 474.226734906613\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:14:19.976834Z",
     "iopub.status.busy": "2024-12-05T15:14:19.976522Z",
     "iopub.status.idle": "2024-12-05T15:18:42.590429Z",
     "shell.execute_reply": "2024-12-05T15:18:42.589718Z",
     "shell.execute_reply.started": "2024-12-05T15:14:19.976805Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000, TD Loss: 10.3163, Robust Loss: 0.0000\n",
      "Step 100/5000, TD Loss: 11.3099, Robust Loss: 0.1245\n",
      "Step 200/5000, TD Loss: 12.5339, Robust Loss: 0.4999\n",
      "Step 300/5000, TD Loss: 11.3194, Robust Loss: 1.4924\n",
      "Step 400/5000, TD Loss: 9.2221, Robust Loss: 1.7934\n",
      "Step 500/5000, TD Loss: 10.8289, Robust Loss: 2.2487\n",
      "Step 600/5000, TD Loss: 8.1078, Robust Loss: 2.4350\n",
      "Step 700/5000, TD Loss: 8.9582, Robust Loss: 3.9284\n",
      "Step 800/5000, TD Loss: 7.0487, Robust Loss: 6.0340\n",
      "Step 900/5000, TD Loss: 7.7164, Robust Loss: 6.7105\n",
      "Step 1000/5000, TD Loss: 5.0359, Robust Loss: 7.2438\n",
      "Step 1100/5000, TD Loss: 6.2030, Robust Loss: 12.0028\n",
      "Step 1200/5000, TD Loss: 5.2381, Robust Loss: 8.9190\n",
      "Step 1300/5000, TD Loss: 4.9883, Robust Loss: 9.8233\n",
      "Step 1400/5000, TD Loss: 5.3557, Robust Loss: 13.8114\n",
      "Step 1500/5000, TD Loss: 4.9896, Robust Loss: 14.0494\n",
      "Step 1600/5000, TD Loss: 5.3772, Robust Loss: 11.3728\n",
      "Step 1700/5000, TD Loss: 5.4830, Robust Loss: 7.4291\n",
      "Step 1800/5000, TD Loss: 4.2205, Robust Loss: 17.4848\n",
      "Step 1900/5000, TD Loss: 4.4547, Robust Loss: 14.5616\n",
      "Step 2000/5000, TD Loss: 4.1420, Robust Loss: 9.0566\n",
      "Step 2100/5000, TD Loss: 4.4811, Robust Loss: 13.9947\n",
      "Step 2200/5000, TD Loss: 5.1597, Robust Loss: 10.7877\n",
      "Step 2300/5000, TD Loss: 2.8491, Robust Loss: 9.0216\n",
      "Step 2400/5000, TD Loss: 3.4798, Robust Loss: 11.7078\n",
      "Step 2500/5000, TD Loss: 4.1255, Robust Loss: 12.5965\n",
      "Step 2600/5000, TD Loss: 3.1924, Robust Loss: 8.6666\n",
      "Step 2700/5000, TD Loss: 2.9921, Robust Loss: 13.8350\n",
      "Step 2800/5000, TD Loss: 2.9957, Robust Loss: 14.7242\n",
      "Step 2900/5000, TD Loss: 5.4620, Robust Loss: 11.8641\n",
      "Step 3000/5000, TD Loss: 3.2322, Robust Loss: 8.2353\n",
      "Step 3100/5000, TD Loss: 2.4490, Robust Loss: 13.1052\n",
      "Step 3200/5000, TD Loss: 3.7293, Robust Loss: 16.5767\n",
      "Step 3300/5000, TD Loss: 3.2032, Robust Loss: 9.0499\n",
      "Step 3400/5000, TD Loss: 3.2841, Robust Loss: 9.3079\n",
      "Step 3500/5000, TD Loss: 2.3782, Robust Loss: 12.4130\n",
      "Step 3600/5000, TD Loss: 2.9704, Robust Loss: 12.1250\n",
      "Step 3700/5000, TD Loss: 2.6642, Robust Loss: 10.8377\n",
      "Step 3800/5000, TD Loss: 2.3014, Robust Loss: 6.8659\n",
      "Step 3900/5000, TD Loss: 3.5488, Robust Loss: 16.4213\n",
      "Step 4000/5000, TD Loss: 2.5180, Robust Loss: 14.5200\n",
      "Step 4100/5000, TD Loss: 3.3195, Robust Loss: 17.5724\n",
      "Step 4200/5000, TD Loss: 2.3755, Robust Loss: 12.1715\n",
      "Step 4300/5000, TD Loss: 2.9568, Robust Loss: 11.1244\n",
      "Step 4400/5000, TD Loss: 1.9329, Robust Loss: 13.8685\n",
      "Step 4500/5000, TD Loss: 2.2242, Robust Loss: 10.1813\n",
      "Step 4600/5000, TD Loss: 2.7685, Robust Loss: 12.9801\n",
      "Step 4700/5000, TD Loss: 1.8493, Robust Loss: 12.6147\n",
      "Step 4800/5000, TD Loss: 3.0358, Robust Loss: 16.9504\n",
      "Step 4900/5000, TD Loss: 1.7291, Robust Loss: 11.5080\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = RobustAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:18:42.592074Z",
     "iopub.status.busy": "2024-12-05T15:18:42.591827Z",
     "iopub.status.idle": "2024-12-05T15:24:58.841725Z",
     "shell.execute_reply": "2024-12-05T15:24:58.840780Z",
     "shell.execute_reply.started": "2024-12-05T15:18:42.592050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 588.9364825010562\n",
      "Episode 2/200: Reward = 639.4635806012512\n",
      "Episode 3/200: Reward = 640.0339782348864\n",
      "Episode 4/200: Reward = 636.8171960698895\n",
      "Episode 5/200: Reward = 617.270314288993\n",
      "Episode 6/200: Reward = 595.5929024452387\n",
      "Episode 7/200: Reward = 628.8025170898864\n",
      "Episode 8/200: Reward = 641.3431688009284\n",
      "Episode 9/200: Reward = 603.1333039306372\n",
      "Episode 10/200: Reward = 634.992165146953\n",
      "Episode 11/200: Reward = 608.2545209168555\n",
      "Episode 12/200: Reward = 527.6472692233533\n",
      "Episode 13/200: Reward = 631.2413129989108\n",
      "Episode 14/200: Reward = 652.1245097791393\n",
      "Episode 15/200: Reward = 633.0383636754088\n",
      "Episode 16/200: Reward = 651.2701865732529\n",
      "Episode 17/200: Reward = 616.7933325307457\n",
      "Episode 18/200: Reward = 637.517894604106\n",
      "Episode 19/200: Reward = 641.2725114977931\n",
      "Episode 20/200: Reward = 646.8306950266438\n",
      "Episode 21/200: Reward = 636.332520118184\n",
      "Episode 22/200: Reward = 636.4219753484267\n",
      "Episode 23/200: Reward = 585.2761016767313\n",
      "Episode 24/200: Reward = 609.6043452309189\n",
      "Episode 25/200: Reward = 646.6483967494246\n",
      "Episode 26/200: Reward = 652.0120607546393\n",
      "Episode 27/200: Reward = 625.6323281288486\n",
      "Episode 28/200: Reward = 572.792951478022\n",
      "Episode 29/200: Reward = 600.8196298451737\n",
      "Episode 30/200: Reward = 638.7226331533495\n",
      "Episode 31/200: Reward = 655.3183297238588\n",
      "Episode 32/200: Reward = 651.8924286851656\n",
      "Episode 33/200: Reward = 606.1727735140853\n",
      "Episode 34/200: Reward = 635.9313240106113\n",
      "Episode 35/200: Reward = 575.919932782196\n",
      "Episode 36/200: Reward = 642.0479681819168\n",
      "Episode 37/200: Reward = 646.5390661475196\n",
      "Episode 38/200: Reward = 627.3756504511232\n",
      "Episode 39/200: Reward = 629.0291386098479\n",
      "Episode 40/200: Reward = 651.2506859830974\n",
      "Episode 41/200: Reward = 598.3326392096193\n",
      "Episode 42/200: Reward = 604.5671314337523\n",
      "Episode 43/200: Reward = 643.9205694482499\n",
      "Episode 44/200: Reward = 652.5434949200538\n",
      "Episode 45/200: Reward = 648.8489454340863\n",
      "Episode 46/200: Reward = 639.1857806495301\n",
      "Episode 47/200: Reward = 543.502476643607\n",
      "Episode 48/200: Reward = 638.9870972499015\n",
      "Episode 49/200: Reward = 639.2204387177571\n",
      "Episode 50/200: Reward = 628.7654694555707\n",
      "Episode 51/200: Reward = 652.0217986146212\n",
      "Episode 52/200: Reward = 632.1742243333066\n",
      "Episode 53/200: Reward = 649.3457484245879\n",
      "Episode 54/200: Reward = 649.6220592836339\n",
      "Episode 55/200: Reward = 644.4607841494932\n",
      "Episode 56/200: Reward = 645.6831552538149\n",
      "Episode 57/200: Reward = 634.4827100356862\n",
      "Episode 58/200: Reward = 645.262930888168\n",
      "Episode 59/200: Reward = 648.079831945346\n",
      "Episode 60/200: Reward = 626.6656988277068\n",
      "Episode 61/200: Reward = 654.5439945038771\n",
      "Episode 62/200: Reward = 636.3812864819729\n",
      "Episode 63/200: Reward = 635.475760907919\n",
      "Episode 64/200: Reward = 652.2840033888994\n",
      "Episode 65/200: Reward = 651.9616758788595\n",
      "Episode 66/200: Reward = 636.7976586641154\n",
      "Episode 67/200: Reward = 639.7422172192615\n",
      "Episode 68/200: Reward = 650.5469207116761\n",
      "Episode 69/200: Reward = 576.5810447836432\n",
      "Episode 70/200: Reward = 652.2788103980237\n",
      "Episode 71/200: Reward = 592.3693209014086\n",
      "Episode 72/200: Reward = 638.2546918730409\n",
      "Episode 73/200: Reward = 650.8304991091441\n",
      "Episode 74/200: Reward = 652.2637313782794\n",
      "Episode 75/200: Reward = 563.0718719454869\n",
      "Episode 76/200: Reward = 636.7204570439118\n",
      "Episode 77/200: Reward = 633.0942662102113\n",
      "Episode 78/200: Reward = 549.7304041863802\n",
      "Episode 79/200: Reward = 649.4824812433304\n",
      "Episode 80/200: Reward = 643.6592115601959\n",
      "Episode 81/200: Reward = 651.1588719532049\n",
      "Episode 82/200: Reward = 642.0672291442055\n",
      "Episode 83/200: Reward = 637.1787668565578\n",
      "Episode 84/200: Reward = 654.554783750514\n",
      "Episode 85/200: Reward = 630.6376532237823\n",
      "Episode 86/200: Reward = 643.1093419164769\n",
      "Episode 87/200: Reward = 638.2882795803571\n",
      "Episode 88/200: Reward = 650.0151978920536\n",
      "Episode 89/200: Reward = 634.3234827083212\n",
      "Episode 90/200: Reward = 644.6835515865332\n",
      "Episode 91/200: Reward = 626.3412185582949\n",
      "Episode 92/200: Reward = 598.1301422325344\n",
      "Episode 93/200: Reward = 620.2368698659176\n",
      "Episode 94/200: Reward = 637.3376191679854\n",
      "Episode 95/200: Reward = 634.179582559504\n",
      "Episode 96/200: Reward = 633.5523535524018\n",
      "Episode 97/200: Reward = 641.8559444730871\n",
      "Episode 98/200: Reward = 644.7959206820224\n",
      "Episode 99/200: Reward = 546.264932299871\n",
      "Episode 100/200: Reward = 630.6441105805021\n",
      "Episode 101/200: Reward = 651.5622655258184\n",
      "Episode 102/200: Reward = 640.5065210530898\n",
      "Episode 103/200: Reward = 640.9070136287589\n",
      "Episode 104/200: Reward = 617.7047700522269\n",
      "Episode 105/200: Reward = 634.0279916713633\n",
      "Episode 106/200: Reward = 653.4976522295564\n",
      "Episode 107/200: Reward = 616.6683556377833\n",
      "Episode 108/200: Reward = 629.8612578179483\n",
      "Episode 109/200: Reward = 643.1395171219968\n",
      "Episode 110/200: Reward = 597.7686078386254\n",
      "Episode 111/200: Reward = 631.4454875933187\n",
      "Episode 112/200: Reward = 651.7390606256166\n",
      "Episode 113/200: Reward = 651.2880703414468\n",
      "Episode 114/200: Reward = 637.1701731537021\n",
      "Episode 115/200: Reward = 622.0066987382038\n",
      "Episode 116/200: Reward = 629.105346177792\n",
      "Episode 117/200: Reward = 632.9619921006469\n",
      "Episode 118/200: Reward = 629.381788565983\n",
      "Episode 119/200: Reward = 591.4697895773788\n",
      "Episode 120/200: Reward = 581.3908390002671\n",
      "Episode 121/200: Reward = 648.834578477994\n",
      "Episode 122/200: Reward = 644.1819680438881\n",
      "Episode 123/200: Reward = 626.4763688309927\n",
      "Episode 124/200: Reward = 644.3022829267301\n",
      "Episode 125/200: Reward = 637.418426096082\n",
      "Episode 126/200: Reward = 653.7897522232037\n",
      "Episode 127/200: Reward = 648.8152848757315\n",
      "Episode 128/200: Reward = 655.5709130908742\n",
      "Episode 129/200: Reward = 603.9500486349384\n",
      "Episode 130/200: Reward = 648.3636410812964\n",
      "Episode 131/200: Reward = 638.0267756398205\n",
      "Episode 132/200: Reward = 645.0727301170783\n",
      "Episode 133/200: Reward = 596.9792023892253\n",
      "Episode 134/200: Reward = 633.3493663454973\n",
      "Episode 135/200: Reward = 628.6228237599839\n",
      "Episode 136/200: Reward = 647.0470062258021\n",
      "Episode 137/200: Reward = 604.8100454536075\n",
      "Episode 138/200: Reward = 628.8905466396582\n",
      "Episode 139/200: Reward = 657.890758922279\n",
      "Episode 140/200: Reward = 648.2112513042589\n",
      "Episode 141/200: Reward = 633.5734954064733\n",
      "Episode 142/200: Reward = 546.0618667212725\n",
      "Episode 143/200: Reward = 629.8955964578594\n",
      "Episode 144/200: Reward = 625.7390997075728\n",
      "Episode 145/200: Reward = 648.0567500494386\n",
      "Episode 146/200: Reward = 631.1685052073954\n",
      "Episode 147/200: Reward = 631.7366023001653\n",
      "Episode 148/200: Reward = 649.3338494803834\n",
      "Episode 149/200: Reward = 640.674878965572\n",
      "Episode 150/200: Reward = 652.6967745876668\n",
      "Episode 151/200: Reward = 650.7986060789939\n",
      "Episode 152/200: Reward = 612.761592923969\n",
      "Episode 153/200: Reward = 640.5575286119587\n",
      "Episode 154/200: Reward = 618.2905350225368\n",
      "Episode 155/200: Reward = 644.5580198120525\n",
      "Episode 156/200: Reward = 626.0714378282524\n",
      "Episode 157/200: Reward = 645.7302720923572\n",
      "Episode 158/200: Reward = 560.4667822899305\n",
      "Episode 159/200: Reward = 639.6004994424234\n",
      "Episode 160/200: Reward = 636.5259834022411\n",
      "Episode 161/200: Reward = 654.1438251806441\n",
      "Episode 162/200: Reward = 621.7966790688743\n",
      "Episode 163/200: Reward = 644.6337496647516\n",
      "Episode 164/200: Reward = 646.4123310418718\n",
      "Episode 165/200: Reward = 632.69067273478\n",
      "Episode 166/200: Reward = 645.7369847676339\n",
      "Episode 167/200: Reward = 640.830047072137\n",
      "Episode 168/200: Reward = 653.0896165705163\n",
      "Episode 169/200: Reward = 637.0221716901734\n",
      "Episode 170/200: Reward = 652.1650927426101\n",
      "Episode 171/200: Reward = 652.369089165064\n",
      "Episode 172/200: Reward = 653.0583475525892\n",
      "Episode 173/200: Reward = 646.2053284959425\n",
      "Episode 174/200: Reward = 601.8212315608822\n",
      "Episode 175/200: Reward = 648.5955254211314\n",
      "Episode 176/200: Reward = 645.4455704619292\n",
      "Episode 177/200: Reward = 634.909173740006\n",
      "Episode 178/200: Reward = 636.192443409099\n",
      "Episode 179/200: Reward = 635.5362555035068\n",
      "Episode 180/200: Reward = 533.3958242888821\n",
      "Episode 181/200: Reward = 627.4565512585889\n",
      "Episode 182/200: Reward = 588.5394370192913\n",
      "Episode 183/200: Reward = 649.9082435866233\n",
      "Episode 184/200: Reward = 606.9130228548556\n",
      "Episode 185/200: Reward = 650.6137005671054\n",
      "Episode 186/200: Reward = 645.3098414027169\n",
      "Episode 187/200: Reward = 649.0677935724893\n",
      "Episode 188/200: Reward = 649.1762305118231\n",
      "Episode 189/200: Reward = 642.1848769043123\n",
      "Episode 190/200: Reward = 651.4568667339354\n",
      "Episode 191/200: Reward = 550.9966313930726\n",
      "Episode 192/200: Reward = 635.19732046293\n",
      "Episode 193/200: Reward = 641.4290249544086\n",
      "Episode 194/200: Reward = 521.0696816658447\n",
      "Episode 195/200: Reward = 640.5632795676859\n",
      "Episode 196/200: Reward = 562.0908188813712\n",
      "Episode 197/200: Reward = 650.3412607865081\n",
      "Episode 198/200: Reward = 585.0568059623909\n",
      "Episode 199/200: Reward = 649.131476497082\n",
      "Episode 200/200: Reward = 618.5248550194038\n",
      "Average Reward under Robust Sarsa Critic-based attack: 629.5939945902791\n",
      "Final Average Reward under Robust Sarsa Attack: 629.5939945902791\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:25:20.370864Z",
     "iopub.status.busy": "2024-12-05T15:25:20.370500Z",
     "iopub.status.idle": "2024-12-05T15:25:20.380837Z",
     "shell.execute_reply": "2024-12-05T15:25:20.379896Z",
     "shell.execute_reply.started": "2024-12-05T15:25:20.370830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_state_value_attack(env, policy_net, value_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a State Value Attack using a value network.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        value_net (torch.nn.Module): The trained value network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under the state value attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute value for the perturbed state\n",
    "                value = value_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Minimize or maximize the value\n",
    "                loss = -value.mean()  # Gradient ascent to maximize adversarial effect\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                action_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(action_output, tuple):\n",
    "                    action = action_output[0]  # Extract mean for continuous actions\n",
    "                else:\n",
    "                    action = action_output\n",
    "\n",
    "                action = action.squeeze().cpu().numpy()  # Ensure the action is in NumPy format\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under State Value Attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:25:30.482372Z",
     "iopub.status.busy": "2024-12-05T15:25:30.482032Z",
     "iopub.status.idle": "2024-12-05T15:25:30.493399Z",
     "shell.execute_reply": "2024-12-05T15:25:30.492591Z",
     "shell.execute_reply.started": "2024-12-05T15:25:30.482340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_target_policy_attack(env, policy_net, target_action, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Target Policy Misclassification attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        target_action (torch.Tensor): The target action to force the policy to output.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Target Policy Misclassification attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Get policy output for the perturbed state\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    logits = policy_output  # For discrete actions\n",
    "                    loss = torch.nn.functional.cross_entropy(logits, target_action)  # Cross-entropy loss\n",
    "                elif isinstance(env.action_space, gym.spaces.Box):\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Mean and std\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n",
    "                else:\n",
    "                    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "                # Backpropagate to compute gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Apply gradient-based perturbation\n",
    "                grad = perturbed_state.grad\n",
    "                perturbation = step_epsilon * grad.sign()\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action = torch.argmax(policy_output, dim=1).item()  # Discrete action\n",
    "                else:\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()  # Continuous action\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Target Policy Misclassification attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:25:35.681502Z",
     "iopub.status.busy": "2024-12-05T15:25:35.680601Z",
     "iopub.status.idle": "2024-12-05T15:30:17.759485Z",
     "shell.execute_reply": "2024-12-05T15:30:17.758578Z",
     "shell.execute_reply.started": "2024-12-05T15:25:35.681465Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 528.5545755389755\n",
      "Episode 2/200: Reward = 347.06878834792644\n",
      "Episode 3/200: Reward = 449.841541716543\n",
      "Episode 4/200: Reward = 334.852160727421\n",
      "Episode 5/200: Reward = 449.82095804270386\n",
      "Episode 6/200: Reward = 477.6854111164778\n",
      "Episode 7/200: Reward = 543.3728646666646\n",
      "Episode 8/200: Reward = 508.9143774322205\n",
      "Episode 9/200: Reward = 434.0940030937651\n",
      "Episode 10/200: Reward = 478.15810736111825\n",
      "Episode 11/200: Reward = 528.4901685068223\n",
      "Episode 12/200: Reward = 286.3512783842519\n",
      "Episode 13/200: Reward = 352.2279821936611\n",
      "Episode 14/200: Reward = 465.85148986936485\n",
      "Episode 15/200: Reward = 337.39294680939804\n",
      "Episode 16/200: Reward = 539.416360488241\n",
      "Episode 17/200: Reward = 444.51221159340434\n",
      "Episode 18/200: Reward = 501.67749701286044\n",
      "Episode 19/200: Reward = 306.227921734185\n",
      "Episode 20/200: Reward = 483.16695168524853\n",
      "Episode 21/200: Reward = 516.9780774655194\n",
      "Episode 22/200: Reward = 268.4419432589775\n",
      "Episode 23/200: Reward = 452.4459223901904\n",
      "Episode 24/200: Reward = 538.0313862243925\n",
      "Episode 25/200: Reward = 426.91452864933416\n",
      "Episode 26/200: Reward = 341.03343005538386\n",
      "Episode 27/200: Reward = 525.7538616126586\n",
      "Episode 28/200: Reward = 527.0236823775728\n",
      "Episode 29/200: Reward = 536.4619853249968\n",
      "Episode 30/200: Reward = 291.14833125144946\n",
      "Episode 31/200: Reward = 526.6210373900153\n",
      "Episode 32/200: Reward = 523.446173062145\n",
      "Episode 33/200: Reward = 440.8081821534671\n",
      "Episode 34/200: Reward = 562.2829278267483\n",
      "Episode 35/200: Reward = 523.4131868758466\n",
      "Episode 36/200: Reward = 545.4216929275113\n",
      "Episode 37/200: Reward = 487.5355726880945\n",
      "Episode 38/200: Reward = 532.0340484382912\n",
      "Episode 39/200: Reward = 288.21720818881363\n",
      "Episode 40/200: Reward = 333.3544353075541\n",
      "Episode 41/200: Reward = 553.9510995949612\n",
      "Episode 42/200: Reward = 536.6146519490023\n",
      "Episode 43/200: Reward = 472.56414763999044\n",
      "Episode 44/200: Reward = 498.56496791894585\n",
      "Episode 45/200: Reward = 360.5135162596744\n",
      "Episode 46/200: Reward = 327.01135890467845\n",
      "Episode 47/200: Reward = 536.4361967513104\n",
      "Episode 48/200: Reward = 436.219773596233\n",
      "Episode 49/200: Reward = 562.5181911383903\n",
      "Episode 50/200: Reward = 526.6180273500164\n",
      "Episode 51/200: Reward = 421.0861832229443\n",
      "Episode 52/200: Reward = 532.4649628367165\n",
      "Episode 53/200: Reward = 454.0569517715237\n",
      "Episode 54/200: Reward = 445.016914699377\n",
      "Episode 55/200: Reward = 524.9450676779283\n",
      "Episode 56/200: Reward = 547.7891429916964\n",
      "Episode 57/200: Reward = 425.51994962201513\n",
      "Episode 58/200: Reward = 341.1089584736362\n",
      "Episode 59/200: Reward = 519.2061613066638\n",
      "Episode 60/200: Reward = 480.54989752953486\n",
      "Episode 61/200: Reward = 279.88734923983276\n",
      "Episode 62/200: Reward = 338.96953541534896\n",
      "Episode 63/200: Reward = 509.12313566767045\n",
      "Episode 64/200: Reward = 286.68257581598635\n",
      "Episode 65/200: Reward = 467.54427195806784\n",
      "Episode 66/200: Reward = 312.9953198884788\n",
      "Episode 67/200: Reward = 507.9467955349911\n",
      "Episode 68/200: Reward = 341.9425173023876\n",
      "Episode 69/200: Reward = 280.72329683876615\n",
      "Episode 70/200: Reward = 537.4201425211565\n",
      "Episode 71/200: Reward = 541.7764462733772\n",
      "Episode 72/200: Reward = 487.7769064324598\n",
      "Episode 73/200: Reward = 531.9157086358111\n",
      "Episode 74/200: Reward = 448.73908882028195\n",
      "Episode 75/200: Reward = 560.4006550615699\n",
      "Episode 76/200: Reward = 511.8182526585496\n",
      "Episode 77/200: Reward = 418.79235956654924\n",
      "Episode 78/200: Reward = 546.1088881779563\n",
      "Episode 79/200: Reward = 577.0528426604895\n",
      "Episode 80/200: Reward = 276.21684567408766\n",
      "Episode 81/200: Reward = 347.2721303332974\n",
      "Episode 82/200: Reward = 476.1667569274151\n",
      "Episode 83/200: Reward = 524.2498751824952\n",
      "Episode 84/200: Reward = 542.0936819880873\n",
      "Episode 85/200: Reward = 475.1038852451362\n",
      "Episode 86/200: Reward = 422.0768682010978\n",
      "Episode 87/200: Reward = 342.91049851216104\n",
      "Episode 88/200: Reward = 560.3097130674151\n",
      "Episode 89/200: Reward = 469.3727961938268\n",
      "Episode 90/200: Reward = 542.3112786605428\n",
      "Episode 91/200: Reward = 409.00590076100116\n",
      "Episode 92/200: Reward = 537.9049257166329\n",
      "Episode 93/200: Reward = 506.1406352678336\n",
      "Episode 94/200: Reward = 383.7932346490812\n",
      "Episode 95/200: Reward = 429.7080979785056\n",
      "Episode 96/200: Reward = 335.3198288530366\n",
      "Episode 97/200: Reward = 525.3243481757415\n",
      "Episode 98/200: Reward = 440.3214902301484\n",
      "Episode 99/200: Reward = 553.9607236959118\n",
      "Episode 100/200: Reward = 448.8182683277155\n",
      "Episode 101/200: Reward = 381.6657495848633\n",
      "Episode 102/200: Reward = 505.933715621893\n",
      "Episode 103/200: Reward = 444.4713163934976\n",
      "Episode 104/200: Reward = 291.9819013558479\n",
      "Episode 105/200: Reward = 273.1756533270502\n",
      "Episode 106/200: Reward = 555.9380653523681\n",
      "Episode 107/200: Reward = 498.76476722016844\n",
      "Episode 108/200: Reward = 435.84964014079384\n",
      "Episode 109/200: Reward = 525.9973465697773\n",
      "Episode 110/200: Reward = 548.4471640093241\n",
      "Episode 111/200: Reward = 317.55477065705804\n",
      "Episode 112/200: Reward = 322.01362611986895\n",
      "Episode 113/200: Reward = 422.71629819065913\n",
      "Episode 114/200: Reward = 548.9431982990467\n",
      "Episode 115/200: Reward = 411.7951265374583\n",
      "Episode 116/200: Reward = 412.9822268487425\n",
      "Episode 117/200: Reward = 353.5539227247887\n",
      "Episode 118/200: Reward = 355.80686105434086\n",
      "Episode 119/200: Reward = 401.7675671027265\n",
      "Episode 120/200: Reward = 534.5390414188216\n",
      "Episode 121/200: Reward = 522.9018202996899\n",
      "Episode 122/200: Reward = 436.73168725321614\n",
      "Episode 123/200: Reward = 328.5678816771538\n",
      "Episode 124/200: Reward = 331.7886630003831\n",
      "Episode 125/200: Reward = 325.65923131057855\n",
      "Episode 126/200: Reward = 492.0578160510868\n",
      "Episode 127/200: Reward = 386.8103240168783\n",
      "Episode 128/200: Reward = 501.33378490398303\n",
      "Episode 129/200: Reward = 345.11541686925455\n",
      "Episode 130/200: Reward = 431.80417557640004\n",
      "Episode 131/200: Reward = 474.5486983601499\n",
      "Episode 132/200: Reward = 323.7825910822799\n",
      "Episode 133/200: Reward = 524.6719952931894\n",
      "Episode 134/200: Reward = 513.0803713547755\n",
      "Episode 135/200: Reward = 496.0794768901525\n",
      "Episode 136/200: Reward = 528.9830345592608\n",
      "Episode 137/200: Reward = 273.08942027726005\n",
      "Episode 138/200: Reward = 300.65302026910933\n",
      "Episode 139/200: Reward = 323.0195649842234\n",
      "Episode 140/200: Reward = 524.9455221778887\n",
      "Episode 141/200: Reward = 527.0556411808436\n",
      "Episode 142/200: Reward = 339.1652836918045\n",
      "Episode 143/200: Reward = 315.9880140779342\n",
      "Episode 144/200: Reward = 534.2050651928164\n",
      "Episode 145/200: Reward = 500.8259412727455\n",
      "Episode 146/200: Reward = 521.818125523077\n",
      "Episode 147/200: Reward = 396.00209611243014\n",
      "Episode 148/200: Reward = 422.85697793843525\n",
      "Episode 149/200: Reward = 539.2825365435623\n",
      "Episode 150/200: Reward = 539.1916325105376\n",
      "Episode 151/200: Reward = 536.9812888936053\n",
      "Episode 152/200: Reward = 468.2599486954763\n",
      "Episode 153/200: Reward = 457.4472541431351\n",
      "Episode 154/200: Reward = 430.38858830077504\n",
      "Episode 155/200: Reward = 544.7948887516376\n",
      "Episode 156/200: Reward = 380.2998644118089\n",
      "Episode 157/200: Reward = 551.6289264457445\n",
      "Episode 158/200: Reward = 281.231102992147\n",
      "Episode 159/200: Reward = 324.21964423764643\n",
      "Episode 160/200: Reward = 474.44409815060646\n",
      "Episode 161/200: Reward = 479.73849505222586\n",
      "Episode 162/200: Reward = 351.2331188769454\n",
      "Episode 163/200: Reward = 347.5616538902393\n",
      "Episode 164/200: Reward = 268.68806403577327\n",
      "Episode 165/200: Reward = 510.36892575826045\n",
      "Episode 166/200: Reward = 514.4334290263608\n",
      "Episode 167/200: Reward = 269.85344501846583\n",
      "Episode 168/200: Reward = 514.8875729924403\n",
      "Episode 169/200: Reward = 520.3340675924953\n",
      "Episode 170/200: Reward = 476.0818056780435\n",
      "Episode 171/200: Reward = 342.18759124874765\n",
      "Episode 172/200: Reward = 333.3533117482334\n",
      "Episode 173/200: Reward = 515.6222738183831\n",
      "Episode 174/200: Reward = 535.4424913381077\n",
      "Episode 175/200: Reward = 345.5454160859714\n",
      "Episode 176/200: Reward = 338.20904611602424\n",
      "Episode 177/200: Reward = 495.4026440949415\n",
      "Episode 178/200: Reward = 506.59305448909106\n",
      "Episode 179/200: Reward = 548.2856458471578\n",
      "Episode 180/200: Reward = 336.345446166343\n",
      "Episode 181/200: Reward = 480.13301492005854\n",
      "Episode 182/200: Reward = 529.1082425948445\n",
      "Episode 183/200: Reward = 342.35532646023415\n",
      "Episode 184/200: Reward = 476.01780088514255\n",
      "Episode 185/200: Reward = 335.5043051965637\n",
      "Episode 186/200: Reward = 559.6366351996033\n",
      "Episode 187/200: Reward = 511.53018599459966\n",
      "Episode 188/200: Reward = 329.1660924767321\n",
      "Episode 189/200: Reward = 489.552671642871\n",
      "Episode 190/200: Reward = 509.73086749108495\n",
      "Episode 191/200: Reward = 473.3257605517393\n",
      "Episode 192/200: Reward = 554.1601251117605\n",
      "Episode 193/200: Reward = 267.74914895696145\n",
      "Episode 194/200: Reward = 463.7915365682679\n",
      "Episode 195/200: Reward = 521.741431458061\n",
      "Episode 196/200: Reward = 470.80423240269516\n",
      "Episode 197/200: Reward = 555.061349136003\n",
      "Episode 198/200: Reward = 280.8045541755478\n",
      "Episode 199/200: Reward = 476.18944763186823\n",
      "Episode 200/200: Reward = 480.0930350559378\n",
      "Average Reward under State Value Attack: 445.98015919861956\n",
      "Average Reward under State Action Value Attack: 445.98015919861956\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    value_net=RobustAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:30:50.871206Z",
     "iopub.status.busy": "2024-12-05T15:30:50.870483Z",
     "iopub.status.idle": "2024-12-05T15:44:48.070624Z",
     "shell.execute_reply": "2024-12-05T15:44:48.069623Z",
     "shell.execute_reply.started": "2024-12-05T15:30:50.871172Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 693.1901192295966\n",
      "Episode 2/200: Reward = 943.3643621807067\n",
      "Episode 3/200: Reward = 815.7333813447087\n",
      "Episode 4/200: Reward = 1445.3845959036246\n",
      "Episode 5/200: Reward = 1465.9494087367648\n",
      "Episode 6/200: Reward = 999.6279886334008\n",
      "Episode 7/200: Reward = 1032.3364137018925\n",
      "Episode 8/200: Reward = 1875.0666188422372\n",
      "Episode 9/200: Reward = 3529.7762874686036\n",
      "Episode 10/200: Reward = 1412.862719511698\n",
      "Episode 11/200: Reward = 1138.8154669744006\n",
      "Episode 12/200: Reward = 788.2967195801293\n",
      "Episode 13/200: Reward = 808.8026768203479\n",
      "Episode 14/200: Reward = 1010.8265830504154\n",
      "Episode 15/200: Reward = 811.7728874268169\n",
      "Episode 16/200: Reward = 1415.5361181574804\n",
      "Episode 17/200: Reward = 1086.909832051973\n",
      "Episode 18/200: Reward = 803.7502712284085\n",
      "Episode 19/200: Reward = 790.1769677103415\n",
      "Episode 20/200: Reward = 759.5326387876972\n",
      "Episode 21/200: Reward = 1258.6111460251423\n",
      "Episode 22/200: Reward = 790.5083134785166\n",
      "Episode 23/200: Reward = 789.743623411393\n",
      "Episode 24/200: Reward = 812.240738861744\n",
      "Episode 25/200: Reward = 1145.6766033287693\n",
      "Episode 26/200: Reward = 1422.2357690084195\n",
      "Episode 27/200: Reward = 1027.4884860955794\n",
      "Episode 28/200: Reward = 1677.1609339034003\n",
      "Episode 29/200: Reward = 1416.5783081190093\n",
      "Episode 30/200: Reward = 1015.2055950862098\n",
      "Episode 31/200: Reward = 1022.8983776179999\n",
      "Episode 32/200: Reward = 795.035555437068\n",
      "Episode 33/200: Reward = 1397.9206933080516\n",
      "Episode 34/200: Reward = 929.29807823327\n",
      "Episode 35/200: Reward = 1016.5690925697073\n",
      "Episode 36/200: Reward = 983.6673386725898\n",
      "Episode 37/200: Reward = 941.6714262559228\n",
      "Episode 38/200: Reward = 1261.891028313188\n",
      "Episode 39/200: Reward = 939.4254540153821\n",
      "Episode 40/200: Reward = 760.9267665621473\n",
      "Episode 41/200: Reward = 1399.8279132332477\n",
      "Episode 42/200: Reward = 1279.6340470242128\n",
      "Episode 43/200: Reward = 1460.163226902441\n",
      "Episode 44/200: Reward = 784.5809474643074\n",
      "Episode 45/200: Reward = 803.1618807118299\n",
      "Episode 46/200: Reward = 703.0256418721336\n",
      "Episode 47/200: Reward = 792.1167043252793\n",
      "Episode 48/200: Reward = 807.0720159037969\n",
      "Episode 49/200: Reward = 803.015959538416\n",
      "Episode 50/200: Reward = 1111.9196841481275\n",
      "Episode 51/200: Reward = 1437.5945414261796\n",
      "Episode 52/200: Reward = 1405.918948894195\n",
      "Episode 53/200: Reward = 684.0659927771698\n",
      "Episode 54/200: Reward = 2152.2494548176214\n",
      "Episode 55/200: Reward = 1020.5738995480565\n",
      "Episode 56/200: Reward = 1442.1263759569943\n",
      "Episode 57/200: Reward = 1439.8899113087848\n",
      "Episode 58/200: Reward = 1050.5419817606705\n",
      "Episode 59/200: Reward = 1011.8633899719371\n",
      "Episode 60/200: Reward = 809.9162157306234\n",
      "Episode 61/200: Reward = 1607.2690173876692\n",
      "Episode 62/200: Reward = 801.1279644535839\n",
      "Episode 63/200: Reward = 800.3805238037838\n",
      "Episode 64/200: Reward = 937.0878134865669\n",
      "Episode 65/200: Reward = 2148.375230730227\n",
      "Episode 66/200: Reward = 1255.027945864738\n",
      "Episode 67/200: Reward = 801.5095489478838\n",
      "Episode 68/200: Reward = 1458.797224880335\n",
      "Episode 69/200: Reward = 1010.4629349595897\n",
      "Episode 70/200: Reward = 1226.9731034603933\n",
      "Episode 71/200: Reward = 1450.2125257698992\n",
      "Episode 72/200: Reward = 1005.3941484616088\n",
      "Episode 73/200: Reward = 2034.4638405951498\n",
      "Episode 74/200: Reward = 4231.739902067258\n",
      "Episode 75/200: Reward = 1451.8404104725344\n",
      "Episode 76/200: Reward = 1023.8519318113597\n",
      "Episode 77/200: Reward = 1004.9784755288521\n",
      "Episode 78/200: Reward = 935.1038014144125\n",
      "Episode 79/200: Reward = 928.3572634553366\n",
      "Episode 80/200: Reward = 2470.43555803574\n",
      "Episode 81/200: Reward = 1126.8096216390804\n",
      "Episode 82/200: Reward = 1013.2430543446509\n",
      "Episode 83/200: Reward = 1456.4194181369573\n",
      "Episode 84/200: Reward = 805.8890826598903\n",
      "Episode 85/200: Reward = 987.601933775251\n",
      "Episode 86/200: Reward = 916.8130249747325\n",
      "Episode 87/200: Reward = 805.9491566325896\n",
      "Episode 88/200: Reward = 1475.8971966277431\n",
      "Episode 89/200: Reward = 1004.0779942998313\n",
      "Episode 90/200: Reward = 782.024735120866\n",
      "Episode 91/200: Reward = 1443.2736555518522\n",
      "Episode 92/200: Reward = 1802.311510037669\n",
      "Episode 93/200: Reward = 801.9384775956943\n",
      "Episode 94/200: Reward = 1003.6033469887764\n",
      "Episode 95/200: Reward = 1419.0951206652915\n",
      "Episode 96/200: Reward = 778.6650470295507\n",
      "Episode 97/200: Reward = 1355.356605232497\n",
      "Episode 98/200: Reward = 996.861740161087\n",
      "Episode 99/200: Reward = 1156.2575438806005\n",
      "Episode 100/200: Reward = 1064.371739821809\n",
      "Episode 101/200: Reward = 1017.4637927626029\n",
      "Episode 102/200: Reward = 1793.7940398619744\n",
      "Episode 103/200: Reward = 1005.9053771767888\n",
      "Episode 104/200: Reward = 1220.5743582808504\n",
      "Episode 105/200: Reward = 1252.7707249177804\n",
      "Episode 106/200: Reward = 1546.5698324132068\n",
      "Episode 107/200: Reward = 949.3083009982313\n",
      "Episode 108/200: Reward = 792.0652946708998\n",
      "Episode 109/200: Reward = 1024.8659955274513\n",
      "Episode 110/200: Reward = 2217.1421622773023\n",
      "Episode 111/200: Reward = 1005.3235019039334\n",
      "Episode 112/200: Reward = 803.1981623824904\n",
      "Episode 113/200: Reward = 953.9197844824466\n",
      "Episode 114/200: Reward = 3685.4745025923207\n",
      "Episode 115/200: Reward = 1031.2334803187516\n",
      "Episode 116/200: Reward = 948.5088647515842\n",
      "Episode 117/200: Reward = 791.9426924506103\n",
      "Episode 118/200: Reward = 1018.7466356194965\n",
      "Episode 119/200: Reward = 803.2835778715598\n",
      "Episode 120/200: Reward = 944.8495387912347\n",
      "Episode 121/200: Reward = 959.2455985324837\n",
      "Episode 122/200: Reward = 1026.579318711314\n",
      "Episode 123/200: Reward = 794.3399440986641\n",
      "Episode 124/200: Reward = 809.4747375982727\n",
      "Episode 125/200: Reward = 810.3507375138714\n",
      "Episode 126/200: Reward = 1025.2721262100845\n",
      "Episode 127/200: Reward = 1022.0341946852504\n",
      "Episode 128/200: Reward = 945.8722279899262\n",
      "Episode 129/200: Reward = 1431.2029826893804\n",
      "Episode 130/200: Reward = 1647.783787994738\n",
      "Episode 131/200: Reward = 796.7914136637696\n",
      "Episode 132/200: Reward = 5178.68986344135\n",
      "Episode 133/200: Reward = 940.1986516449032\n",
      "Episode 134/200: Reward = 952.1139440844931\n",
      "Episode 135/200: Reward = 790.1968924317738\n",
      "Episode 136/200: Reward = 928.3000014958513\n",
      "Episode 137/200: Reward = 1478.89000179302\n",
      "Episode 138/200: Reward = 803.2079787303229\n",
      "Episode 139/200: Reward = 791.4415129501389\n",
      "Episode 140/200: Reward = 789.1597238285372\n",
      "Episode 141/200: Reward = 803.082974515774\n",
      "Episode 142/200: Reward = 800.7756315083594\n",
      "Episode 143/200: Reward = 1042.746332411112\n",
      "Episode 144/200: Reward = 1018.8545961819335\n",
      "Episode 145/200: Reward = 1423.717786104202\n",
      "Episode 146/200: Reward = 1815.321971491299\n",
      "Episode 147/200: Reward = 936.1150443413396\n",
      "Episode 148/200: Reward = 1008.3353396375062\n",
      "Episode 149/200: Reward = 1024.530834093466\n",
      "Episode 150/200: Reward = 786.4455636808323\n",
      "Episode 151/200: Reward = 1009.2666908996689\n",
      "Episode 152/200: Reward = 1114.740115935901\n",
      "Episode 153/200: Reward = 687.9473987690537\n",
      "Episode 154/200: Reward = 942.3521097506865\n",
      "Episode 155/200: Reward = 1008.5331808263768\n",
      "Episode 156/200: Reward = 936.4856626416288\n",
      "Episode 157/200: Reward = 924.0714793391426\n",
      "Episode 158/200: Reward = 2461.4912453458296\n",
      "Episode 159/200: Reward = 804.4538615405021\n",
      "Episode 160/200: Reward = 802.3017059146958\n",
      "Episode 161/200: Reward = 803.4258367257085\n",
      "Episode 162/200: Reward = 937.4328874820694\n",
      "Episode 163/200: Reward = 791.3832615567406\n",
      "Episode 164/200: Reward = 692.2762388772062\n",
      "Episode 165/200: Reward = 1421.1502011853304\n",
      "Episode 166/200: Reward = 792.7282385623097\n",
      "Episode 167/200: Reward = 1244.0473937905795\n",
      "Episode 168/200: Reward = 5013.716595886616\n",
      "Episode 169/200: Reward = 956.4302477835389\n",
      "Episode 170/200: Reward = 801.871350117024\n",
      "Episode 171/200: Reward = 1852.262712634873\n",
      "Episode 172/200: Reward = 904.8792227965854\n",
      "Episode 173/200: Reward = 1259.4646798506028\n",
      "Episode 174/200: Reward = 975.4172887731419\n",
      "Episode 175/200: Reward = 1131.3609129550925\n",
      "Episode 176/200: Reward = 1153.6287841193844\n",
      "Episode 177/200: Reward = 1139.0156609299854\n",
      "Episode 178/200: Reward = 1438.4658217792426\n",
      "Episode 179/200: Reward = 820.4496825903215\n",
      "Episode 180/200: Reward = 796.9440463502414\n",
      "Episode 181/200: Reward = 1419.4524395192846\n",
      "Episode 182/200: Reward = 689.8607385596704\n",
      "Episode 183/200: Reward = 794.4044114239891\n",
      "Episode 184/200: Reward = 1401.2343354629438\n",
      "Episode 185/200: Reward = 1831.1157228003492\n",
      "Episode 186/200: Reward = 929.2951657781939\n",
      "Episode 187/200: Reward = 791.3751357104194\n",
      "Episode 188/200: Reward = 1001.3316863544719\n",
      "Episode 189/200: Reward = 809.2326843796222\n",
      "Episode 190/200: Reward = 787.3300557820866\n",
      "Episode 191/200: Reward = 1160.1698818020905\n",
      "Episode 192/200: Reward = 2464.0785511772133\n",
      "Episode 193/200: Reward = 1015.2901951907215\n",
      "Episode 194/200: Reward = 684.6909980998996\n",
      "Episode 195/200: Reward = 790.998465489527\n",
      "Episode 196/200: Reward = 793.1596796718785\n",
      "Episode 197/200: Reward = 1014.9109032364505\n",
      "Episode 198/200: Reward = 791.8575241736315\n",
      "Episode 199/200: Reward = 1108.687168427954\n",
      "Episode 200/200: Reward = 1017.4130170984185\n",
      "Average Reward under State Value Attack: 1172.5673444983927\n",
      "Average Reward under State Action Value Attack: 1172.5673444983927\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` and `q_net` are already defined and trained\n",
    "# Example environment\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_sav = evaluate_agent_with_state_value_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    value_net=VanillaAgent.value_net,  # Trained Q-value network (critic)\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under State Action Value Attack: {average_reward_sav}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:54:39.084885Z",
     "iopub.status.busy": "2024-12-05T15:54:39.084177Z",
     "iopub.status.idle": "2024-12-05T15:59:35.788862Z",
     "shell.execute_reply": "2024-12-05T15:59:35.787912Z",
     "shell.execute_reply.started": "2024-12-05T15:54:39.084847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1613303488.py:43: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 540.8979549206526\n",
      "Episode 2/200: Reward = 291.45180702343197\n",
      "Episode 3/200: Reward = 572.5371309498485\n",
      "Episode 4/200: Reward = 530.4878440667834\n",
      "Episode 5/200: Reward = 330.52730376023527\n",
      "Episode 6/200: Reward = 275.5857559463875\n",
      "Episode 7/200: Reward = 362.19963430002804\n",
      "Episode 8/200: Reward = 552.1260354189199\n",
      "Episode 9/200: Reward = 528.6464012847537\n",
      "Episode 10/200: Reward = 471.75512259478666\n",
      "Episode 11/200: Reward = 339.9922232510131\n",
      "Episode 12/200: Reward = 284.2048776719928\n",
      "Episode 13/200: Reward = 339.89770776161885\n",
      "Episode 14/200: Reward = 342.9773512187943\n",
      "Episode 15/200: Reward = 582.8833178046004\n",
      "Episode 16/200: Reward = 522.3506287805708\n",
      "Episode 17/200: Reward = 405.1830865614185\n",
      "Episode 18/200: Reward = 538.3877261843035\n",
      "Episode 19/200: Reward = 465.85547333297325\n",
      "Episode 20/200: Reward = 539.268505708799\n",
      "Episode 21/200: Reward = 561.9085265823605\n",
      "Episode 22/200: Reward = 572.0256215262482\n",
      "Episode 23/200: Reward = 529.0603065444302\n",
      "Episode 24/200: Reward = 286.6918122219777\n",
      "Episode 25/200: Reward = 507.4641777864901\n",
      "Episode 26/200: Reward = 551.1716708006113\n",
      "Episode 27/200: Reward = 497.2231176608011\n",
      "Episode 28/200: Reward = 576.9652250067971\n",
      "Episode 29/200: Reward = 501.8185358283288\n",
      "Episode 30/200: Reward = 581.3530037585392\n",
      "Episode 31/200: Reward = 263.9205572836663\n",
      "Episode 32/200: Reward = 529.9782538758928\n",
      "Episode 33/200: Reward = 337.4500986440204\n",
      "Episode 34/200: Reward = 512.4353146816618\n",
      "Episode 35/200: Reward = 339.64223269554765\n",
      "Episode 36/200: Reward = 498.10718535407585\n",
      "Episode 37/200: Reward = 559.8733462515304\n",
      "Episode 38/200: Reward = 347.3373111126578\n",
      "Episode 39/200: Reward = 572.1271408447009\n",
      "Episode 40/200: Reward = 328.15510208017895\n",
      "Episode 41/200: Reward = 572.8708456875089\n",
      "Episode 42/200: Reward = 539.9982720946033\n",
      "Episode 43/200: Reward = 338.9558385796538\n",
      "Episode 44/200: Reward = 567.2134988755531\n",
      "Episode 45/200: Reward = 513.5683583846921\n",
      "Episode 46/200: Reward = 366.3389159137551\n",
      "Episode 47/200: Reward = 507.9918166374405\n",
      "Episode 48/200: Reward = 530.0328176035018\n",
      "Episode 49/200: Reward = 343.18723436128846\n",
      "Episode 50/200: Reward = 326.7869706533354\n",
      "Episode 51/200: Reward = 547.5842124905998\n",
      "Episode 52/200: Reward = 537.5648240916656\n",
      "Episode 53/200: Reward = 572.8068601240359\n",
      "Episode 54/200: Reward = 346.4987635824279\n",
      "Episode 55/200: Reward = 554.933602853925\n",
      "Episode 56/200: Reward = 518.7401938033153\n",
      "Episode 57/200: Reward = 527.9275746013396\n",
      "Episode 58/200: Reward = 525.039626518311\n",
      "Episode 59/200: Reward = 489.34344092276416\n",
      "Episode 60/200: Reward = 511.7253317800072\n",
      "Episode 61/200: Reward = 376.5175934213503\n",
      "Episode 62/200: Reward = 516.9323899097433\n",
      "Episode 63/200: Reward = 539.9595761783908\n",
      "Episode 64/200: Reward = 496.2380539169782\n",
      "Episode 65/200: Reward = 514.9864363667195\n",
      "Episode 66/200: Reward = 529.1691314013306\n",
      "Episode 67/200: Reward = 280.1659210236079\n",
      "Episode 68/200: Reward = 511.7171766184352\n",
      "Episode 69/200: Reward = 280.9178221132619\n",
      "Episode 70/200: Reward = 311.9734337357478\n",
      "Episode 71/200: Reward = 293.6296374039304\n",
      "Episode 72/200: Reward = 512.2682986075044\n",
      "Episode 73/200: Reward = 483.93236186092133\n",
      "Episode 74/200: Reward = 301.55353764830335\n",
      "Episode 75/200: Reward = 557.4430169517509\n",
      "Episode 76/200: Reward = 553.9863059912406\n",
      "Episode 77/200: Reward = 362.8078845346156\n",
      "Episode 78/200: Reward = 556.5674432597509\n",
      "Episode 79/200: Reward = 383.83606366812876\n",
      "Episode 80/200: Reward = 529.6397192062238\n",
      "Episode 81/200: Reward = 439.307406897886\n",
      "Episode 82/200: Reward = 331.5718641006895\n",
      "Episode 83/200: Reward = 322.0469629413415\n",
      "Episode 84/200: Reward = 454.0447520791745\n",
      "Episode 85/200: Reward = 534.6113605616096\n",
      "Episode 86/200: Reward = 551.0766113881252\n",
      "Episode 87/200: Reward = 402.16150791999485\n",
      "Episode 88/200: Reward = 554.9280420131131\n",
      "Episode 89/200: Reward = 512.8515532742483\n",
      "Episode 90/200: Reward = 370.7248833963124\n",
      "Episode 91/200: Reward = 332.82007791367596\n",
      "Episode 92/200: Reward = 512.7524997071448\n",
      "Episode 93/200: Reward = 424.4734583271217\n",
      "Episode 94/200: Reward = 390.0961858000075\n",
      "Episode 95/200: Reward = 521.6990866476515\n",
      "Episode 96/200: Reward = 336.8302756160926\n",
      "Episode 97/200: Reward = 344.78868337986387\n",
      "Episode 98/200: Reward = 531.6002342826129\n",
      "Episode 99/200: Reward = 384.6236144249274\n",
      "Episode 100/200: Reward = 506.3007836965962\n",
      "Episode 101/200: Reward = 529.5035655610701\n",
      "Episode 102/200: Reward = 528.7403209239242\n",
      "Episode 103/200: Reward = 276.8760127268461\n",
      "Episode 104/200: Reward = 552.1480031699831\n",
      "Episode 105/200: Reward = 497.04850035482104\n",
      "Episode 106/200: Reward = 414.9586062383939\n",
      "Episode 107/200: Reward = 446.91878957580633\n",
      "Episode 108/200: Reward = 336.11676129219603\n",
      "Episode 109/200: Reward = 439.64358057946333\n",
      "Episode 110/200: Reward = 314.5428122267114\n",
      "Episode 111/200: Reward = 280.85194745413594\n",
      "Episode 112/200: Reward = 342.8461673747949\n",
      "Episode 113/200: Reward = 293.3483714098616\n",
      "Episode 114/200: Reward = 323.6693888253246\n",
      "Episode 115/200: Reward = 567.9445109282265\n",
      "Episode 116/200: Reward = 572.4705519896485\n",
      "Episode 117/200: Reward = 525.9882672222187\n",
      "Episode 118/200: Reward = 333.7746463648963\n",
      "Episode 119/200: Reward = 508.7725770306076\n",
      "Episode 120/200: Reward = 522.2788875764547\n",
      "Episode 121/200: Reward = 554.8235712327751\n",
      "Episode 122/200: Reward = 545.8280422557967\n",
      "Episode 123/200: Reward = 535.4136548546192\n",
      "Episode 124/200: Reward = 344.08775410693835\n",
      "Episode 125/200: Reward = 536.8016484613476\n",
      "Episode 126/200: Reward = 528.4433434936093\n",
      "Episode 127/200: Reward = 419.0616954061051\n",
      "Episode 128/200: Reward = 568.9328041424558\n",
      "Episode 129/200: Reward = 577.363771039598\n",
      "Episode 130/200: Reward = 416.5605395903116\n",
      "Episode 131/200: Reward = 298.0741055269509\n",
      "Episode 132/200: Reward = 400.25645038608434\n",
      "Episode 133/200: Reward = 541.7823865327621\n",
      "Episode 134/200: Reward = 339.56294887720605\n",
      "Episode 135/200: Reward = 557.3631631068407\n",
      "Episode 136/200: Reward = 518.146711788392\n",
      "Episode 137/200: Reward = 426.3119099105335\n",
      "Episode 138/200: Reward = 285.82049244987013\n",
      "Episode 139/200: Reward = 519.5211326873815\n",
      "Episode 140/200: Reward = 523.1840758195323\n",
      "Episode 141/200: Reward = 546.250068418819\n",
      "Episode 142/200: Reward = 512.9817897316584\n",
      "Episode 143/200: Reward = 463.3059541170278\n",
      "Episode 144/200: Reward = 456.08188813921737\n",
      "Episode 145/200: Reward = 556.3101526011666\n",
      "Episode 146/200: Reward = 526.7250289703026\n",
      "Episode 147/200: Reward = 576.7098486224321\n",
      "Episode 148/200: Reward = 336.26584240701857\n",
      "Episode 149/200: Reward = 558.4084650360629\n",
      "Episode 150/200: Reward = 536.1341352756472\n",
      "Episode 151/200: Reward = 581.6829911582731\n",
      "Episode 152/200: Reward = 337.263940877409\n",
      "Episode 153/200: Reward = 549.8661291317818\n",
      "Episode 154/200: Reward = 524.8810784085612\n",
      "Episode 155/200: Reward = 412.9327745536047\n",
      "Episode 156/200: Reward = 517.1816652760294\n",
      "Episode 157/200: Reward = 329.92559941117855\n",
      "Episode 158/200: Reward = 359.60855599488457\n",
      "Episode 159/200: Reward = 534.4303376949928\n",
      "Episode 160/200: Reward = 342.6192185270638\n",
      "Episode 161/200: Reward = 522.8183737375035\n",
      "Episode 162/200: Reward = 391.14888452873447\n",
      "Episode 163/200: Reward = 587.3408780344603\n",
      "Episode 164/200: Reward = 330.85449398455853\n",
      "Episode 165/200: Reward = 445.9145032811078\n",
      "Episode 166/200: Reward = 538.6470423157892\n",
      "Episode 167/200: Reward = 541.168495152108\n",
      "Episode 168/200: Reward = 408.6596743678694\n",
      "Episode 169/200: Reward = 405.3946251689509\n",
      "Episode 170/200: Reward = 514.5002591108432\n",
      "Episode 171/200: Reward = 502.0671499085769\n",
      "Episode 172/200: Reward = 339.1950760168035\n",
      "Episode 173/200: Reward = 316.916820426233\n",
      "Episode 174/200: Reward = 487.9067813979281\n",
      "Episode 175/200: Reward = 403.64422606327963\n",
      "Episode 176/200: Reward = 442.35802175962726\n",
      "Episode 177/200: Reward = 579.1385791497098\n",
      "Episode 178/200: Reward = 341.9299842665634\n",
      "Episode 179/200: Reward = 335.9756908823876\n",
      "Episode 180/200: Reward = 580.7088825418277\n",
      "Episode 181/200: Reward = 329.37347097322197\n",
      "Episode 182/200: Reward = 577.4684961563761\n",
      "Episode 183/200: Reward = 563.0864915434368\n",
      "Episode 184/200: Reward = 510.9534719132046\n",
      "Episode 185/200: Reward = 493.3892310026643\n",
      "Episode 186/200: Reward = 383.49378044018613\n",
      "Episode 187/200: Reward = 521.7823588021937\n",
      "Episode 188/200: Reward = 278.28319069038304\n",
      "Episode 189/200: Reward = 580.9889101365202\n",
      "Episode 190/200: Reward = 336.6209631034632\n",
      "Episode 191/200: Reward = 505.1252394517858\n",
      "Episode 192/200: Reward = 364.4388587618221\n",
      "Episode 193/200: Reward = 546.3740976009966\n",
      "Episode 194/200: Reward = 568.4663813978482\n",
      "Episode 195/200: Reward = 427.96724998817814\n",
      "Episode 196/200: Reward = 584.405191507998\n",
      "Episode 197/200: Reward = 281.1991517770046\n",
      "Episode 198/200: Reward = 354.7418687470514\n",
      "Episode 199/200: Reward = 459.7463091898503\n",
      "Episode 200/200: Reward = 400.09741842044923\n",
      "Average Reward under Target Policy Misclassification attack: 456.77111792687714\n",
      "Average Reward under Target Policy Misclassification Attack: 456.77111792687714\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=RobustAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T16:00:32.110318Z",
     "iopub.status.busy": "2024-12-05T16:00:32.109979Z",
     "iopub.status.idle": "2024-12-05T16:08:11.387556Z",
     "shell.execute_reply": "2024-12-05T16:08:11.386604Z",
     "shell.execute_reply.started": "2024-12-05T16:00:32.110288Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1613303488.py:43: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(mean, target_action)  # MSE loss for continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 851.2155009095103\n",
      "Episode 2/200: Reward = 446.6654249788073\n",
      "Episode 3/200: Reward = 894.1109610415114\n",
      "Episode 4/200: Reward = 709.7281598350978\n",
      "Episode 5/200: Reward = 447.6573526450699\n",
      "Episode 6/200: Reward = 750.4678470816741\n",
      "Episode 7/200: Reward = 759.936914581194\n",
      "Episode 8/200: Reward = 445.7425048354033\n",
      "Episode 9/200: Reward = 448.50549179633003\n",
      "Episode 10/200: Reward = 812.0598545110121\n",
      "Episode 11/200: Reward = 893.5769316810646\n",
      "Episode 12/200: Reward = 448.49509921332367\n",
      "Episode 13/200: Reward = 850.2912963774294\n",
      "Episode 14/200: Reward = 447.80719252713385\n",
      "Episode 15/200: Reward = 882.3931046516011\n",
      "Episode 16/200: Reward = 444.147921032004\n",
      "Episode 17/200: Reward = 451.4708515160012\n",
      "Episode 18/200: Reward = 449.5205657391279\n",
      "Episode 19/200: Reward = 448.6740457840399\n",
      "Episode 20/200: Reward = 449.0255019355837\n",
      "Episode 21/200: Reward = 459.19684671072906\n",
      "Episode 22/200: Reward = 447.3894553482221\n",
      "Episode 23/200: Reward = 455.83682885436747\n",
      "Episode 24/200: Reward = 445.6079299001361\n",
      "Episode 25/200: Reward = 478.45248547066393\n",
      "Episode 26/200: Reward = 442.7686333066482\n",
      "Episode 27/200: Reward = 446.02982543796753\n",
      "Episode 28/200: Reward = 451.8840412886141\n",
      "Episode 29/200: Reward = 454.29794944864466\n",
      "Episode 30/200: Reward = 443.9769313981593\n",
      "Episode 31/200: Reward = 798.7936123998039\n",
      "Episode 32/200: Reward = 711.7367728400143\n",
      "Episode 33/200: Reward = 442.4009957749803\n",
      "Episode 34/200: Reward = 463.5738092764829\n",
      "Episode 35/200: Reward = 448.00775630746216\n",
      "Episode 36/200: Reward = 857.0957707483409\n",
      "Episode 37/200: Reward = 448.20653493978637\n",
      "Episode 38/200: Reward = 443.37869056937575\n",
      "Episode 39/200: Reward = 448.1281109678865\n",
      "Episode 40/200: Reward = 850.1127463288374\n",
      "Episode 41/200: Reward = 449.69813516494685\n",
      "Episode 42/200: Reward = 443.93473013237565\n",
      "Episode 43/200: Reward = 455.8697339918939\n",
      "Episode 44/200: Reward = 457.7271888596478\n",
      "Episode 45/200: Reward = 474.850162900973\n",
      "Episode 46/200: Reward = 448.08852921304674\n",
      "Episode 47/200: Reward = 468.30904186752844\n",
      "Episode 48/200: Reward = 451.83294817427856\n",
      "Episode 49/200: Reward = 443.2526064342945\n",
      "Episode 50/200: Reward = 450.0880853373449\n",
      "Episode 51/200: Reward = 445.0820109313124\n",
      "Episode 52/200: Reward = 880.8035315790663\n",
      "Episode 53/200: Reward = 444.08902705559154\n",
      "Episode 54/200: Reward = 443.43681649788\n",
      "Episode 55/200: Reward = 442.820236477168\n",
      "Episode 56/200: Reward = 457.6376652134708\n",
      "Episode 57/200: Reward = 450.59950575251656\n",
      "Episode 58/200: Reward = 449.6152260556996\n",
      "Episode 59/200: Reward = 469.80070951979593\n",
      "Episode 60/200: Reward = 1146.3099476234277\n",
      "Episode 61/200: Reward = 442.981606515737\n",
      "Episode 62/200: Reward = 445.0287886352149\n",
      "Episode 63/200: Reward = 455.0338068277619\n",
      "Episode 64/200: Reward = 470.1992401036289\n",
      "Episode 65/200: Reward = 451.9113107921487\n",
      "Episode 66/200: Reward = 444.34437114077076\n",
      "Episode 67/200: Reward = 467.84193783221195\n",
      "Episode 68/200: Reward = 468.4419896375593\n",
      "Episode 69/200: Reward = 462.2080944396358\n",
      "Episode 70/200: Reward = 444.67054049592764\n",
      "Episode 71/200: Reward = 463.5185725898691\n",
      "Episode 72/200: Reward = 467.82326280613614\n",
      "Episode 73/200: Reward = 450.02826767686867\n",
      "Episode 74/200: Reward = 448.00217466583314\n",
      "Episode 75/200: Reward = 444.0459440731311\n",
      "Episode 76/200: Reward = 450.72559696890295\n",
      "Episode 77/200: Reward = 698.7145694835402\n",
      "Episode 78/200: Reward = 450.20601146784026\n",
      "Episode 79/200: Reward = 452.4904087305608\n",
      "Episode 80/200: Reward = 886.7494977564453\n",
      "Episode 81/200: Reward = 446.31722836289515\n",
      "Episode 82/200: Reward = 765.3300415050521\n",
      "Episode 83/200: Reward = 448.40884213248444\n",
      "Episode 84/200: Reward = 447.38022871415757\n",
      "Episode 85/200: Reward = 449.38490051043954\n",
      "Episode 86/200: Reward = 449.2415892525413\n",
      "Episode 87/200: Reward = 447.08114630246877\n",
      "Episode 88/200: Reward = 447.6645004504685\n",
      "Episode 89/200: Reward = 448.3009047306141\n",
      "Episode 90/200: Reward = 969.6328476092872\n",
      "Episode 91/200: Reward = 449.33539041436734\n",
      "Episode 92/200: Reward = 449.0743891181574\n",
      "Episode 93/200: Reward = 480.4339756271758\n",
      "Episode 94/200: Reward = 847.2619273749895\n",
      "Episode 95/200: Reward = 468.0086753457474\n",
      "Episode 96/200: Reward = 447.5348836019689\n",
      "Episode 97/200: Reward = 467.2049612668617\n",
      "Episode 98/200: Reward = 442.5838221685312\n",
      "Episode 99/200: Reward = 444.8965971098566\n",
      "Episode 100/200: Reward = 453.73800947221514\n",
      "Episode 101/200: Reward = 758.8874710875473\n",
      "Episode 102/200: Reward = 449.32668411086\n",
      "Episode 103/200: Reward = 473.10809828692805\n",
      "Episode 104/200: Reward = 452.4121116549467\n",
      "Episode 105/200: Reward = 1140.528743138277\n",
      "Episode 106/200: Reward = 463.2211153398685\n",
      "Episode 107/200: Reward = 448.8840267489366\n",
      "Episode 108/200: Reward = 447.2586236487167\n",
      "Episode 109/200: Reward = 732.319769915303\n",
      "Episode 110/200: Reward = 440.5218848371916\n",
      "Episode 111/200: Reward = 1424.607789610913\n",
      "Episode 112/200: Reward = 447.83933871643137\n",
      "Episode 113/200: Reward = 447.8030974686725\n",
      "Episode 114/200: Reward = 450.9238797815136\n",
      "Episode 115/200: Reward = 835.6126964900701\n",
      "Episode 116/200: Reward = 451.0163501925776\n",
      "Episode 117/200: Reward = 464.7828330071222\n",
      "Episode 118/200: Reward = 492.7336538849841\n",
      "Episode 119/200: Reward = 445.9341064902131\n",
      "Episode 120/200: Reward = 479.6604820601366\n",
      "Episode 121/200: Reward = 440.80370811607395\n",
      "Episode 122/200: Reward = 442.4974840915176\n",
      "Episode 123/200: Reward = 448.8948178566687\n",
      "Episode 124/200: Reward = 452.70663963646587\n",
      "Episode 125/200: Reward = 470.94744094002345\n",
      "Episode 126/200: Reward = 446.4951033390072\n",
      "Episode 127/200: Reward = 472.69676973048837\n",
      "Episode 128/200: Reward = 454.42333344375305\n",
      "Episode 129/200: Reward = 449.1429549751618\n",
      "Episode 130/200: Reward = 445.048459612954\n",
      "Episode 131/200: Reward = 448.0842459687534\n",
      "Episode 132/200: Reward = 446.0068979514927\n",
      "Episode 133/200: Reward = 448.8779706553448\n",
      "Episode 134/200: Reward = 477.6171277779544\n",
      "Episode 135/200: Reward = 452.2562673596481\n",
      "Episode 136/200: Reward = 448.1692400661055\n",
      "Episode 137/200: Reward = 1138.6066125527082\n",
      "Episode 138/200: Reward = 452.1666088250092\n",
      "Episode 139/200: Reward = 1131.6604754274645\n",
      "Episode 140/200: Reward = 440.53697514087577\n",
      "Episode 141/200: Reward = 461.71764849223297\n",
      "Episode 142/200: Reward = 448.2211972307621\n",
      "Episode 143/200: Reward = 449.0187645969325\n",
      "Episode 144/200: Reward = 448.5927014855108\n",
      "Episode 145/200: Reward = 445.7183314798339\n",
      "Episode 146/200: Reward = 773.3387180834683\n",
      "Episode 147/200: Reward = 459.9395693659131\n",
      "Episode 148/200: Reward = 450.4199070782465\n",
      "Episode 149/200: Reward = 454.8418207987567\n",
      "Episode 150/200: Reward = 449.51519162100954\n",
      "Episode 151/200: Reward = 849.2996995980668\n",
      "Episode 152/200: Reward = 440.6710764853192\n",
      "Episode 153/200: Reward = 448.67158007261617\n",
      "Episode 154/200: Reward = 478.33931583842724\n",
      "Episode 155/200: Reward = 444.6712487942179\n",
      "Episode 156/200: Reward = 447.5443539106231\n",
      "Episode 157/200: Reward = 1147.681000755892\n",
      "Episode 158/200: Reward = 444.50896074350163\n",
      "Episode 159/200: Reward = 464.1215021672928\n",
      "Episode 160/200: Reward = 755.764558070136\n",
      "Episode 161/200: Reward = 447.9872609127858\n",
      "Episode 162/200: Reward = 440.8503743639913\n",
      "Episode 163/200: Reward = 447.83208754630357\n",
      "Episode 164/200: Reward = 443.51143668062394\n",
      "Episode 165/200: Reward = 447.2552824572452\n",
      "Episode 166/200: Reward = 775.0693454317625\n",
      "Episode 167/200: Reward = 1266.3552700518355\n",
      "Episode 168/200: Reward = 763.0090844331221\n",
      "Episode 169/200: Reward = 768.6345443658444\n",
      "Episode 170/200: Reward = 1261.93246050271\n",
      "Episode 171/200: Reward = 445.5383036842904\n",
      "Episode 172/200: Reward = 448.5709645635988\n",
      "Episode 173/200: Reward = 453.1225319256167\n",
      "Episode 174/200: Reward = 440.20576369332673\n",
      "Episode 175/200: Reward = 455.2806707113315\n",
      "Episode 176/200: Reward = 452.0220151793246\n",
      "Episode 177/200: Reward = 449.5588758950735\n",
      "Episode 178/200: Reward = 453.4130218750071\n",
      "Episode 179/200: Reward = 457.19766888933987\n",
      "Episode 180/200: Reward = 803.6332658008625\n",
      "Episode 181/200: Reward = 449.7846768659059\n",
      "Episode 182/200: Reward = 450.74366907143576\n",
      "Episode 183/200: Reward = 703.8637545464792\n",
      "Episode 184/200: Reward = 450.4460474114566\n",
      "Episode 185/200: Reward = 450.4939783303305\n",
      "Episode 186/200: Reward = 441.2272071953323\n",
      "Episode 187/200: Reward = 855.5205827885338\n",
      "Episode 188/200: Reward = 450.77608391703643\n",
      "Episode 189/200: Reward = 447.7888123834201\n",
      "Episode 190/200: Reward = 485.18767262922347\n",
      "Episode 191/200: Reward = 766.1066539463562\n",
      "Episode 192/200: Reward = 450.10192024487696\n",
      "Episode 193/200: Reward = 1145.7489678551187\n",
      "Episode 194/200: Reward = 450.49142906170607\n",
      "Episode 195/200: Reward = 840.8048988450045\n",
      "Episode 196/200: Reward = 1158.5924325381964\n",
      "Episode 197/200: Reward = 449.17703856584376\n",
      "Episode 198/200: Reward = 448.12590681390384\n",
      "Episode 199/200: Reward = 440.32150574431483\n",
      "Episode 200/200: Reward = 448.44183330906844\n",
      "Average Reward under Target Policy Misclassification attack: 547.9916964005868\n",
      "Average Reward under Target Policy Misclassification Attack: 547.9916964005868\n"
     ]
    }
   ],
   "source": [
    "# Assuming `policy_net` is already defined and trained\n",
    "# Example environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "num_episodes = 200  # Number of episodes to evaluate\n",
    "attack_steps = 10  # Number of attack gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Define the target action for the misclassification attack\n",
    "# For discrete actions: target_action is the action index\n",
    "if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "    target_action = torch.tensor([1], dtype=torch.long).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "# For continuous actions: target_action is a vector of desired action values\n",
    "elif isinstance(env.action_space, gym.spaces.Box):\n",
    "    target_action = torch.tensor([0.5] * env.action_space.shape[0], dtype=torch.float32).to(next(VanillaAgent.policy_net.parameters()).device)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported action space type.\")\n",
    "\n",
    "# Call the attack evaluation function\n",
    "average_reward_tpm = evaluate_agent_with_target_policy_attack(\n",
    "    env=env,\n",
    "    policy_net=VanillaAgent.policy_net,  # Trained policy network\n",
    "    target_action=target_action,\n",
    "    epsilon=epsilon,\n",
    "    num_episodes=num_episodes,\n",
    "    attack_steps=attack_steps,\n",
    "    step_epsilon=step_epsilon\n",
    ")\n",
    "\n",
    "print(f\"Average Reward under Target Policy Misclassification Attack: {average_reward_tpm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
