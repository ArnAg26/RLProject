{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:29.039924Z",
     "iopub.status.busy": "2024-12-05T10:17:29.039557Z",
     "iopub.status.idle": "2024-12-05T10:17:30.058449Z",
     "shell.execute_reply": "2024-12-05T10:17:30.057554Z",
     "shell.execute_reply.started": "2024-12-05T10:17:29.039883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:30.060609Z",
     "iopub.status.busy": "2024-12-05T10:17:30.060138Z",
     "iopub.status.idle": "2024-12-05T10:17:42.434827Z",
     "shell.execute_reply": "2024-12-05T10:17:42.433751Z",
     "shell.execute_reply.started": "2024-12-05T10:17:30.060572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[mujoco] in /opt/conda/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
      "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
      "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[mujoco]) (2.34.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (10.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.10/site-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pyopengl (from mujoco>=2.3.3->gymnasium[mujoco])\n",
      "  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
      "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyopengl, glfw, mujoco\n",
      "Successfully installed glfw-2.8.0 mujoco-3.2.6 pyopengl-3.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:42.436860Z",
     "iopub.status.busy": "2024-12-05T10:17:42.436428Z",
     "iopub.status.idle": "2024-12-05T10:17:46.172343Z",
     "shell.execute_reply": "2024-12-05T10:17:46.171672Z",
     "shell.execute_reply.started": "2024-12-05T10:17:42.436818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import mujoco\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Initialize the Walker2d environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:46.181383Z",
     "iopub.status.busy": "2024-12-05T10:17:46.181059Z",
     "iopub.status.idle": "2024-12-05T10:17:46.207507Z",
     "shell.execute_reply": "2024-12-05T10:17:46.206616Z",
     "shell.execute_reply.started": "2024-12-05T10:17:46.181346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Simplified Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Hidden layers\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        value = self.output_layer(x)\n",
    "        return value\n",
    "\n",
    "\n",
    "# Simplified Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, discrete=True, hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.discrete = discrete\n",
    "        self.activation = activation()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Hidden layers\n",
    "        input_size = state_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        if self.discrete:\n",
    "            # Discrete actions: output probabilities for each action\n",
    "            self.output_layer = nn.Linear(input_size, action_dim)\n",
    "        else:\n",
    "            # Continuous actions: output mean and log_std for each action\n",
    "            self.mean_layer = nn.Linear(input_size, action_dim)\n",
    "            self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        if self.discrete:\n",
    "            # Discrete actions: apply softmax for probabilities\n",
    "            logits = self.output_layer(x)\n",
    "            action_probs = F.softmax(logits, dim=-1)\n",
    "            return action_probs\n",
    "        else:\n",
    "            # Continuous actions: return mean and std\n",
    "            mean = self.mean_layer(x)\n",
    "            std = torch.exp(self.log_std)\n",
    "            return mean, std\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, discrete, gamma=0.99, lam=0.95, eps_clip=0.2, lr=3e-4, k_epochs=4):\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lam = lam  # GAE lambda\n",
    "        self.eps_clip = eps_clip  # Clipping epsilon\n",
    "        self.k_epochs = k_epochs  # Number of PPO epochs\n",
    "        \n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, discrete).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.value_net = ValueNetwork(state_dim).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        if isinstance(self.policy_net, PolicyNetwork) and self.policy_net.discrete:\n",
    "            action_probs = self.policy_net(state)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            return action.item(), action_dist.log_prob(action)\n",
    "        else:\n",
    "            mean, std = self.policy_net(state)\n",
    "            action_dist = torch.distributions.Normal(mean, std)\n",
    "            action = action_dist.sample()\n",
    "            return action.cpu().numpy(), action_dist.log_prob(action).sum()\n",
    "\n",
    "    def compute_advantages(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            advantage = delta + self.gamma * self.lam * (1 - dones[t]) * advantage\n",
    "            advantages.insert(0, advantage)\n",
    "        return torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def train(self, env, max_episodes=1000, rollout_steps=2048, batch_size=64):\n",
    "        for episode in range(max_episodes):\n",
    "            # Initialize trajectory variables\n",
    "            states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "            \n",
    "            # Reset environment and get the initial state\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Collect trajectories\n",
    "            for _ in range(rollout_steps):\n",
    "                with torch.no_grad():\n",
    "                    value = self.value_net(state).squeeze(0)\n",
    "                    action, log_prob = self.select_action(state.cpu().numpy())\n",
    "                \n",
    "                # Interact with the environment\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Store trajectory data\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done or truncated)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                \n",
    "                # Update the state\n",
    "                state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "                if done or truncated:\n",
    "                    state, _ = env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Ensure valid trajectory data\n",
    "            if len(states) == 0:\n",
    "                print(\"No valid states collected; skipping this episode.\")\n",
    "                continue\n",
    "            \n",
    "            # Compute advantages and returns\n",
    "            values.append(torch.tensor([0], device=self.device))  # Bootstrap value\n",
    "            advantages = self.compute_advantages(rewards, values, dones)\n",
    "            returns = advantages + torch.tensor(values[:-1], device=self.device)\n",
    "            \n",
    "            # Optimize policy and value networks\n",
    "            states = torch.stack(states).to(self.device)\n",
    "            actions = torch.tensor(np.array(actions), dtype=torch.float32 if not self.policy_net.discrete else torch.long).to(self.device)\n",
    "            log_probs = torch.stack(log_probs).to(self.device)\n",
    "        \n",
    "            for _ in range(self.k_epochs):\n",
    "                for i in range(0, rollout_steps, batch_size):\n",
    "                    batch_states = states[i:i+batch_size]\n",
    "                    batch_actions = actions[i:i+batch_size]\n",
    "                    batch_log_probs = log_probs[i:i+batch_size]\n",
    "                    batch_advantages = advantages[i:i+batch_size]\n",
    "                    batch_returns = returns[i:i+batch_size]\n",
    "    \n",
    "                    # Policy update\n",
    "                    if self.policy_net.discrete:\n",
    "                        # Discrete action space\n",
    "                        action_probs = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Categorical(action_probs)\n",
    "                        new_log_probs = dist.log_prob(batch_actions)\n",
    "                    else:\n",
    "                        # Continuous action space\n",
    "                        mean, std = self.policy_net(batch_states)\n",
    "                        dist = torch.distributions.Normal(mean, std)\n",
    "                        new_log_probs = dist.log_prob(batch_actions).sum(dim=-1)\n",
    "    \n",
    "                    # PPO objective\n",
    "                    ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "                    surr1 = ratio * batch_advantages\n",
    "                    surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * batch_advantages\n",
    "                    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                    # Value update\n",
    "                    value_preds = self.value_net(batch_states).squeeze(-1)\n",
    "                    value_loss = nn.MSELoss()(value_preds, batch_returns)\n",
    "    \n",
    "                    # Backpropagation\n",
    "                    self.policy_optimizer.zero_grad()\n",
    "                    self.value_optimizer.zero_grad()\n",
    "                    (policy_loss + 0.5 * value_loss).backward()\n",
    "                    self.policy_optimizer.step()\n",
    "                    self.value_optimizer.step()\n",
    "    \n",
    "            print(f\"Episode {episode + 1}: Policy Loss = {policy_loss.item()}, Value Loss = {value_loss.item()}\")\n",
    "\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:17:46.208714Z",
     "iopub.status.busy": "2024-12-05T10:17:46.208417Z",
     "iopub.status.idle": "2024-12-05T10:23:11.866148Z",
     "shell.execute_reply": "2024-12-05T10:23:11.865185Z",
     "shell.execute_reply.started": "2024-12-05T10:17:46.208665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -3.828803062438965, Value Loss = 16.592451095581055\n",
      "Episode 2: Policy Loss = -1.9623545408248901, Value Loss = 9.722156524658203\n",
      "Episode 3: Policy Loss = -23.57123565673828, Value Loss = 429.6669921875\n",
      "Episode 4: Policy Loss = -29.276615142822266, Value Loss = 732.6014404296875\n",
      "Episode 5: Policy Loss = -8.263506889343262, Value Loss = 99.85221862792969\n",
      "Episode 6: Policy Loss = -8.829984664916992, Value Loss = 132.50885009765625\n",
      "Episode 7: Policy Loss = -12.414569854736328, Value Loss = 257.19964599609375\n",
      "Episode 8: Policy Loss = -6.319770812988281, Value Loss = 138.45005798339844\n",
      "Episode 9: Policy Loss = -19.753684997558594, Value Loss = 469.4028625488281\n",
      "Episode 10: Policy Loss = -5.664707183837891, Value Loss = 213.6244354248047\n",
      "Episode 11: Policy Loss = -6.490495681762695, Value Loss = 338.0513000488281\n",
      "Episode 12: Policy Loss = -12.929885864257812, Value Loss = 609.2012939453125\n",
      "Episode 13: Policy Loss = -17.888696670532227, Value Loss = 550.2659912109375\n",
      "Episode 14: Policy Loss = -13.38083553314209, Value Loss = 450.40435791015625\n",
      "Episode 15: Policy Loss = -17.219223022460938, Value Loss = 536.8333740234375\n",
      "Episode 16: Policy Loss = -9.902587890625, Value Loss = 390.81988525390625\n",
      "Episode 17: Policy Loss = 3.13165283203125, Value Loss = 351.6329345703125\n",
      "Episode 18: Policy Loss = -7.631697654724121, Value Loss = 348.07794189453125\n",
      "Episode 19: Policy Loss = 0.9118648767471313, Value Loss = 451.0177001953125\n",
      "Episode 20: Policy Loss = 9.408246994018555, Value Loss = 481.4249267578125\n",
      "Episode 21: Policy Loss = -11.320169448852539, Value Loss = 691.3192138671875\n",
      "Episode 22: Policy Loss = -15.763089179992676, Value Loss = 721.3375244140625\n",
      "Episode 23: Policy Loss = -3.8036394119262695, Value Loss = 365.8081359863281\n",
      "Episode 24: Policy Loss = 14.632440567016602, Value Loss = 664.7171630859375\n",
      "Episode 25: Policy Loss = -6.322854995727539, Value Loss = 456.09228515625\n",
      "Episode 26: Policy Loss = 8.00445556640625, Value Loss = 682.5498046875\n",
      "Episode 27: Policy Loss = 7.894208908081055, Value Loss = 831.3675537109375\n",
      "Episode 28: Policy Loss = 13.839112281799316, Value Loss = 756.0115356445312\n",
      "Episode 29: Policy Loss = 6.929753303527832, Value Loss = 1037.0234375\n",
      "Episode 30: Policy Loss = -4.264264106750488, Value Loss = 504.3500061035156\n",
      "Episode 31: Policy Loss = 1.6576759815216064, Value Loss = 472.9248046875\n",
      "Episode 32: Policy Loss = 1.6909971237182617, Value Loss = 961.722412109375\n",
      "Episode 33: Policy Loss = 18.262371063232422, Value Loss = 938.4915771484375\n",
      "Episode 34: Policy Loss = 10.842750549316406, Value Loss = 1041.8004150390625\n",
      "Episode 35: Policy Loss = 2.0822067260742188, Value Loss = 545.3984375\n",
      "Episode 36: Policy Loss = 20.09497833251953, Value Loss = 1170.63525390625\n",
      "Episode 37: Policy Loss = 21.79718017578125, Value Loss = 1147.0750732421875\n",
      "Episode 38: Policy Loss = 19.270647048950195, Value Loss = 1305.68603515625\n",
      "Episode 39: Policy Loss = 17.43842887878418, Value Loss = 1304.718017578125\n",
      "Episode 40: Policy Loss = 22.179824829101562, Value Loss = 1296.0587158203125\n",
      "Episode 41: Policy Loss = 5.2036237716674805, Value Loss = 649.0142822265625\n",
      "Episode 42: Policy Loss = 6.484785556793213, Value Loss = 689.5772094726562\n",
      "Episode 43: Policy Loss = 4.027165412902832, Value Loss = 805.0535888671875\n",
      "Episode 44: Policy Loss = 19.779403686523438, Value Loss = 1500.3966064453125\n",
      "Episode 45: Policy Loss = 20.261791229248047, Value Loss = 1545.133544921875\n",
      "Episode 46: Policy Loss = 23.491592407226562, Value Loss = 1478.531494140625\n",
      "Episode 47: Policy Loss = 2.751681327819824, Value Loss = 795.2328491210938\n",
      "Episode 48: Policy Loss = 3.7407360076904297, Value Loss = 777.7838134765625\n",
      "Episode 49: Policy Loss = 7.001745700836182, Value Loss = 787.9857788085938\n",
      "Episode 50: Policy Loss = 26.652263641357422, Value Loss = 1596.614501953125\n",
      "Episode 51: Policy Loss = 30.11235809326172, Value Loss = 1701.749755859375\n",
      "Episode 52: Policy Loss = 24.83462905883789, Value Loss = 1729.8284912109375\n",
      "Episode 53: Policy Loss = 9.79791259765625, Value Loss = 835.7703247070312\n",
      "Episode 54: Policy Loss = 8.94044017791748, Value Loss = 855.783203125\n",
      "Episode 55: Policy Loss = 2.1210784912109375, Value Loss = 884.9895629882812\n",
      "Episode 56: Policy Loss = 4.68801212310791, Value Loss = 1040.668701171875\n",
      "Episode 57: Policy Loss = 27.840221405029297, Value Loss = 1791.0841064453125\n",
      "Episode 58: Policy Loss = 7.004539966583252, Value Loss = 637.3316040039062\n",
      "Episode 59: Policy Loss = 17.91110610961914, Value Loss = 1236.849609375\n",
      "Episode 60: Policy Loss = 30.25045394897461, Value Loss = 1474.9124755859375\n",
      "Episode 61: Policy Loss = 8.540788650512695, Value Loss = 607.629638671875\n",
      "Episode 62: Policy Loss = 13.301705360412598, Value Loss = 1186.72412109375\n",
      "Episode 63: Policy Loss = 12.385406494140625, Value Loss = 959.2727661132812\n",
      "Episode 64: Policy Loss = 9.654803276062012, Value Loss = 468.4938659667969\n",
      "Episode 65: Policy Loss = 21.000186920166016, Value Loss = 1253.318359375\n",
      "Episode 66: Policy Loss = 5.068056106567383, Value Loss = 1086.36669921875\n",
      "Episode 67: Policy Loss = 32.058387756347656, Value Loss = 2219.96142578125\n",
      "Episode 68: Policy Loss = 14.85246753692627, Value Loss = 1023.3949584960938\n",
      "Episode 69: Policy Loss = 1.068021297454834, Value Loss = 896.3355102539062\n",
      "Episode 70: Policy Loss = 19.813335418701172, Value Loss = 1448.9927978515625\n",
      "Episode 71: Policy Loss = 20.22530746459961, Value Loss = 1549.3677978515625\n",
      "Episode 72: Policy Loss = -3.894308090209961, Value Loss = 274.558837890625\n",
      "Episode 73: Policy Loss = 19.528976440429688, Value Loss = 1556.8116455078125\n",
      "Episode 74: Policy Loss = 20.101085662841797, Value Loss = 1556.82470703125\n",
      "Episode 75: Policy Loss = 16.04122543334961, Value Loss = 1295.3050537109375\n",
      "Episode 76: Policy Loss = 24.866039276123047, Value Loss = 1818.0615234375\n",
      "Episode 77: Policy Loss = 24.854347229003906, Value Loss = 1794.6876220703125\n",
      "Episode 78: Policy Loss = 14.89593505859375, Value Loss = 1422.7559814453125\n",
      "Episode 79: Policy Loss = 21.772668838500977, Value Loss = 1688.193603515625\n",
      "Episode 80: Policy Loss = 0.758120059967041, Value Loss = 946.517578125\n",
      "Episode 81: Policy Loss = 20.155912399291992, Value Loss = 1749.487060546875\n",
      "Episode 82: Policy Loss = 7.910177230834961, Value Loss = 824.127685546875\n",
      "Episode 83: Policy Loss = 22.05097007751465, Value Loss = 1907.5595703125\n",
      "Episode 84: Policy Loss = 13.150921821594238, Value Loss = 1347.41357421875\n",
      "Episode 85: Policy Loss = 18.775592803955078, Value Loss = 1870.8773193359375\n",
      "Episode 86: Policy Loss = 27.617294311523438, Value Loss = 2168.57275390625\n",
      "Episode 87: Policy Loss = 1.2348830699920654, Value Loss = 110.12130737304688\n",
      "Episode 88: Policy Loss = 12.670645713806152, Value Loss = 653.6749267578125\n",
      "Episode 89: Policy Loss = 33.107688903808594, Value Loss = 2599.2314453125\n",
      "Episode 90: Policy Loss = 29.567073822021484, Value Loss = 2650.46630859375\n",
      "Episode 91: Policy Loss = 30.171363830566406, Value Loss = 2548.855712890625\n",
      "Episode 92: Policy Loss = 18.518869400024414, Value Loss = 2310.859130859375\n",
      "Episode 93: Policy Loss = 29.852794647216797, Value Loss = 2597.81103515625\n",
      "Episode 94: Policy Loss = 22.921049118041992, Value Loss = 2079.132080078125\n",
      "Episode 95: Policy Loss = 49.56884002685547, Value Loss = 4819.4453125\n",
      "Episode 96: Policy Loss = 29.076526641845703, Value Loss = 2593.8134765625\n",
      "Episode 97: Policy Loss = 17.661510467529297, Value Loss = 2372.15673828125\n",
      "Episode 98: Policy Loss = 46.49919128417969, Value Loss = 3478.080810546875\n",
      "Episode 99: Policy Loss = 31.96190643310547, Value Loss = 2842.14453125\n",
      "Episode 100: Policy Loss = 16.588895797729492, Value Loss = 1361.607421875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    VanillaAgent = PPOAgent(state_dim, action_dim, discrete)\n",
    "    VanillaAgent.train(env, max_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:11.868944Z",
     "iopub.status.busy": "2024-12-05T10:23:11.868237Z",
     "iopub.status.idle": "2024-12-05T10:23:11.877448Z",
     "shell.execute_reply": "2024-12-05T10:23:11.876585Z",
     "shell.execute_reply.started": "2024-12-05T10:23:11.868913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, policy_net, num_episodes=200, render=False):\n",
    "    \"\"\"\n",
    "    Evaluates the trained policy network on the environment.\n",
    "\n",
    "    Args:\n",
    "    - env: The Gym environment.\n",
    "    - policy_net: The trained policy network.\n",
    "    - num_episodes: Number of episodes to evaluate.\n",
    "    - render: Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Average reward over the evaluated episodes.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            state = reset_result[0]\n",
    "        else:\n",
    "            state = reset_result\n",
    "\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        episode_reward = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            # Move the state tensor to the same device as the policy network\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    # Assuming (mean, std) for continuous action spaces\n",
    "                    action_mean, action_std = policy_output\n",
    "                    action = torch.normal(action_mean, action_std).cpu().numpy()\n",
    "                else:\n",
    "                    # Assuming logits for discrete action spaces\n",
    "                    action_prob = torch.softmax(policy_output, dim=-1)\n",
    "                    action = torch.argmax(action_prob, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Squeeze the action to ensure proper shape\n",
    "            action = action.squeeze()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, terminated, truncated, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated = next_step_result[:4]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:11.878755Z",
     "iopub.status.busy": "2024-12-05T10:23:11.878472Z",
     "iopub.status.idle": "2024-12-05T10:23:34.008380Z",
     "shell.execute_reply": "2024-12-05T10:23:34.007491Z",
     "shell.execute_reply.started": "2024-12-05T10:23:11.878729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 600.2398146535725\n",
      "Episode 2: Reward = 341.52519992374107\n",
      "Episode 3: Reward = 409.0947660692718\n",
      "Episode 4: Reward = 551.5519040029926\n",
      "Episode 5: Reward = 552.576896863094\n",
      "Episode 6: Reward = 588.5949767764788\n",
      "Episode 7: Reward = 381.61276213358894\n",
      "Episode 8: Reward = 407.5234209859817\n",
      "Episode 9: Reward = 372.27349274294085\n",
      "Episode 10: Reward = 536.6862008014074\n",
      "Episode 11: Reward = 425.61479161241124\n",
      "Episode 12: Reward = 415.3001253288772\n",
      "Episode 13: Reward = 566.4374495535066\n",
      "Episode 14: Reward = 671.0655144897112\n",
      "Episode 15: Reward = 558.2680156118614\n",
      "Episode 16: Reward = 370.2221960029768\n",
      "Episode 17: Reward = 476.19752972554863\n",
      "Episode 18: Reward = 481.17654825362126\n",
      "Episode 19: Reward = 437.986067454201\n",
      "Episode 20: Reward = 410.22141879605755\n",
      "Episode 21: Reward = 531.8176816101375\n",
      "Episode 22: Reward = 577.5455386843926\n",
      "Episode 23: Reward = 667.7627800232236\n",
      "Episode 24: Reward = 521.9555006199785\n",
      "Episode 25: Reward = 550.2605808833861\n",
      "Episode 26: Reward = 394.19790087123454\n",
      "Episode 27: Reward = 354.69870514901504\n",
      "Episode 28: Reward = 352.0837454017276\n",
      "Episode 29: Reward = 370.46519208762226\n",
      "Episode 30: Reward = 623.0117232109764\n",
      "Episode 31: Reward = 354.42885817243933\n",
      "Episode 32: Reward = 387.6495691610826\n",
      "Episode 33: Reward = 332.45304525123095\n",
      "Episode 34: Reward = 458.76472111427455\n",
      "Episode 35: Reward = 509.1163459056869\n",
      "Episode 36: Reward = 621.027204469978\n",
      "Episode 37: Reward = 408.87613096245\n",
      "Episode 38: Reward = 513.5375345120295\n",
      "Episode 39: Reward = 347.8109425713316\n",
      "Episode 40: Reward = 319.88437001347074\n",
      "Episode 41: Reward = 558.4788106398644\n",
      "Episode 42: Reward = 436.1005920274635\n",
      "Episode 43: Reward = 504.73453097756686\n",
      "Episode 44: Reward = 350.9459329240633\n",
      "Episode 45: Reward = 319.47077490421395\n",
      "Episode 46: Reward = 323.4114813443093\n",
      "Episode 47: Reward = 327.19002418491965\n",
      "Episode 48: Reward = 525.7954558578258\n",
      "Episode 49: Reward = 338.7531087984271\n",
      "Episode 50: Reward = 425.9250576095821\n",
      "Episode 51: Reward = 606.5970286171271\n",
      "Episode 52: Reward = 397.4338338481032\n",
      "Episode 53: Reward = 430.763629805023\n",
      "Episode 54: Reward = 352.9636402832865\n",
      "Episode 55: Reward = 547.6683316560985\n",
      "Episode 56: Reward = 736.1867461725637\n",
      "Episode 57: Reward = 712.1464753012705\n",
      "Episode 58: Reward = 374.41731122053085\n",
      "Episode 59: Reward = 531.1309089828569\n",
      "Episode 60: Reward = 528.4464336548394\n",
      "Episode 61: Reward = 526.265494770324\n",
      "Episode 62: Reward = 565.0500441449975\n",
      "Episode 63: Reward = 479.39803566734844\n",
      "Episode 64: Reward = 342.34069735414016\n",
      "Episode 65: Reward = 401.84044766451916\n",
      "Episode 66: Reward = 311.5748322574097\n",
      "Episode 67: Reward = 352.2603668318213\n",
      "Episode 68: Reward = 618.0828183721559\n",
      "Episode 69: Reward = 557.3471698435117\n",
      "Episode 70: Reward = 379.7635526164155\n",
      "Episode 71: Reward = 337.0214706640864\n",
      "Episode 72: Reward = 219.19118418356268\n",
      "Episode 73: Reward = 551.1277535498172\n",
      "Episode 74: Reward = 321.8538332085904\n",
      "Episode 75: Reward = 366.9371741174546\n",
      "Episode 76: Reward = 534.0420297835882\n",
      "Episode 77: Reward = 370.55525390331223\n",
      "Episode 78: Reward = 489.5719942441091\n",
      "Episode 79: Reward = 391.8075197144439\n",
      "Episode 80: Reward = 431.12154352989825\n",
      "Episode 81: Reward = 468.7577025608792\n",
      "Episode 82: Reward = 457.43203191330804\n",
      "Episode 83: Reward = 372.1702404589707\n",
      "Episode 84: Reward = 793.7340371669607\n",
      "Episode 85: Reward = 492.9143550252839\n",
      "Episode 86: Reward = 385.05823214522104\n",
      "Episode 87: Reward = 358.61727610797215\n",
      "Episode 88: Reward = 496.23699826543105\n",
      "Episode 89: Reward = 335.28021463646786\n",
      "Episode 90: Reward = 547.5650782163078\n",
      "Episode 91: Reward = 355.20995122667074\n",
      "Episode 92: Reward = 309.5248376732184\n",
      "Episode 93: Reward = 445.76698621587065\n",
      "Episode 94: Reward = 364.21685536273606\n",
      "Episode 95: Reward = 336.357018808956\n",
      "Episode 96: Reward = 512.4606000373808\n",
      "Episode 97: Reward = 364.0790952093725\n",
      "Episode 98: Reward = 360.75441886001107\n",
      "Episode 99: Reward = 546.9159328019026\n",
      "Episode 100: Reward = 318.93188677143786\n",
      "Episode 101: Reward = 1027.311784743537\n",
      "Episode 102: Reward = 568.1578442682464\n",
      "Episode 103: Reward = 401.08715405247557\n",
      "Episode 104: Reward = 336.07128566840953\n",
      "Episode 105: Reward = 361.1775305721307\n",
      "Episode 106: Reward = 457.2483451350621\n",
      "Episode 107: Reward = 394.38116070707974\n",
      "Episode 108: Reward = 409.5598736786357\n",
      "Episode 109: Reward = 365.8784314569845\n",
      "Episode 110: Reward = 376.02023286766166\n",
      "Episode 111: Reward = 309.9284358174776\n",
      "Episode 112: Reward = 505.4373070927047\n",
      "Episode 113: Reward = 404.1740467146476\n",
      "Episode 114: Reward = 507.80250763263115\n",
      "Episode 115: Reward = 441.9367619367315\n",
      "Episode 116: Reward = 561.1935065996424\n",
      "Episode 117: Reward = 394.37003038974217\n",
      "Episode 118: Reward = 538.0976050073015\n",
      "Episode 119: Reward = 590.3777762019345\n",
      "Episode 120: Reward = 504.2373374807757\n",
      "Episode 121: Reward = 548.632670067128\n",
      "Episode 122: Reward = 410.1647749527574\n",
      "Episode 123: Reward = 463.1157314813127\n",
      "Episode 124: Reward = 346.3832040212762\n",
      "Episode 125: Reward = 474.31347183010814\n",
      "Episode 126: Reward = 547.4795422547194\n",
      "Episode 127: Reward = 572.3713802116272\n",
      "Episode 128: Reward = 455.17600209209513\n",
      "Episode 129: Reward = 545.3301224364042\n",
      "Episode 130: Reward = 416.921301781102\n",
      "Episode 131: Reward = 588.1712094239758\n",
      "Episode 132: Reward = 549.4270596831574\n",
      "Episode 133: Reward = 518.8383586254309\n",
      "Episode 134: Reward = 503.20005985547806\n",
      "Episode 135: Reward = 361.27052913649567\n",
      "Episode 136: Reward = 466.90942683747227\n",
      "Episode 137: Reward = 407.555069009803\n",
      "Episode 138: Reward = 442.616547860898\n",
      "Episode 139: Reward = 346.77149801623636\n",
      "Episode 140: Reward = 514.3481102659744\n",
      "Episode 141: Reward = 534.3204784447064\n",
      "Episode 142: Reward = 529.473975998475\n",
      "Episode 143: Reward = 584.6016417885195\n",
      "Episode 144: Reward = 567.5405638509363\n",
      "Episode 145: Reward = 461.8693718103481\n",
      "Episode 146: Reward = 321.7799711979894\n",
      "Episode 147: Reward = 613.8951206902997\n",
      "Episode 148: Reward = 302.9490917339441\n",
      "Episode 149: Reward = 466.8628740210881\n",
      "Episode 150: Reward = 470.6271300316245\n",
      "Episode 151: Reward = 431.3749807171985\n",
      "Episode 152: Reward = 526.8043562032517\n",
      "Episode 153: Reward = 442.7728130916012\n",
      "Episode 154: Reward = 417.4942118968986\n",
      "Episode 155: Reward = 713.34213466403\n",
      "Episode 156: Reward = 528.7371657648628\n",
      "Episode 157: Reward = 451.4001439737661\n",
      "Episode 158: Reward = 314.4728605117667\n",
      "Episode 159: Reward = 453.76442207717736\n",
      "Episode 160: Reward = 367.948389918941\n",
      "Episode 161: Reward = 500.6975537978351\n",
      "Episode 162: Reward = 513.1465572651196\n",
      "Episode 163: Reward = 561.4183857135498\n",
      "Episode 164: Reward = 569.2805971489258\n",
      "Episode 165: Reward = 291.29697326578685\n",
      "Episode 166: Reward = 382.988504519365\n",
      "Episode 167: Reward = 538.4655294249568\n",
      "Episode 168: Reward = 731.1312056402478\n",
      "Episode 169: Reward = 415.352069450368\n",
      "Episode 170: Reward = 565.5512569723936\n",
      "Episode 171: Reward = 564.1093798057717\n",
      "Episode 172: Reward = 452.7854475521174\n",
      "Episode 173: Reward = 390.3104386835889\n",
      "Episode 174: Reward = 540.0747892909196\n",
      "Episode 175: Reward = 331.4312174263245\n",
      "Episode 176: Reward = 501.7110965420823\n",
      "Episode 177: Reward = 587.1016714233444\n",
      "Episode 178: Reward = 323.69926593107544\n",
      "Episode 179: Reward = 436.8283335579403\n",
      "Episode 180: Reward = 320.20667774490335\n",
      "Episode 181: Reward = 584.7851695995264\n",
      "Episode 182: Reward = 345.8978153390527\n",
      "Episode 183: Reward = 547.3701211244157\n",
      "Episode 184: Reward = 363.5007562937321\n",
      "Episode 185: Reward = 439.9571070056027\n",
      "Episode 186: Reward = 559.7043947711345\n",
      "Episode 187: Reward = 371.0362146514426\n",
      "Episode 188: Reward = 743.9995873470842\n",
      "Episode 189: Reward = 521.8097152713403\n",
      "Episode 190: Reward = 523.549280668597\n",
      "Episode 191: Reward = 548.40212193698\n",
      "Episode 192: Reward = 390.1445229073906\n",
      "Episode 193: Reward = 374.5772726288418\n",
      "Episode 194: Reward = 330.7879010634331\n",
      "Episode 195: Reward = 412.0480406450378\n",
      "Episode 196: Reward = 586.777523682581\n",
      "Episode 197: Reward = 571.1837759385564\n",
      "Episode 198: Reward = 329.6184320795718\n",
      "Episode 199: Reward = 546.7845367482057\n",
      "Episode 200: Reward = 516.6644127774038\n",
      "Average Reward over 200 Episodes: 464.05438308522275\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Evaluate the agent using the trained policy network\n",
    "average_reward = evaluate_agent(env, VanillaAgent.policy_net, num_episodes=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:34.009932Z",
     "iopub.status.busy": "2024-12-05T10:23:34.009621Z",
     "iopub.status.idle": "2024-12-05T10:23:34.018932Z",
     "shell.execute_reply": "2024-12-05T10:23:34.018060Z",
     "shell.execute_reply.started": "2024-12-05T10:23:34.009903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_perturbation(attack_method, state, params, policy_model=None, sarsa_model=None):\n",
    "    \"\"\"\n",
    "    Apply perturbation to the state based on the attack method.\n",
    "\n",
    "    Args:\n",
    "        attack_method (str): The type of attack ('robust_sarsa', 'mad', 'random').\n",
    "        state (torch.Tensor): The current state tensor.\n",
    "        params (object): Parameters for the attack (e.g., epsilon, steps, etc.).\n",
    "        policy_model (nn.Module): The policy model (for MAD and Sarsa+MAD).\n",
    "        sarsa_model (nn.Module): The Sarsa model (for Robust Sarsa).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The perturbed state.\n",
    "    \"\"\"\n",
    "    eps = params.get(\"epsilon\", 0.1)\n",
    "    steps = params.get(\"steps\", 10)\n",
    "    step_eps = eps / steps\n",
    "    clamp_min = state - eps\n",
    "    clamp_max = state + eps\n",
    "\n",
    "    if attack_method == \"robust_sarsa\":\n",
    "        assert sarsa_model is not None, \"Sarsa model is required for Robust Sarsa attack.\"\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            actions = policy_model(perturbed_state)[0]  # Assuming policy returns action logits\n",
    "            value = sarsa_model(torch.cat((state, actions), dim=1)).mean(dim=1)\n",
    "            value.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state - update, clamp_min), clamp_max)\n",
    "            sarsa_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"mad\":\n",
    "        assert policy_model is not None, \"Policy model is required for MAD attack.\"\n",
    "        original_action = policy_model(state)[0].detach()\n",
    "        perturbed_state = state.clone().detach().requires_grad_()\n",
    "        for _ in range(steps):\n",
    "            new_action = policy_model(perturbed_state)[0]\n",
    "            action_diff = ((new_action - original_action) ** 2).sum(dim=1)\n",
    "            action_diff.backward()\n",
    "            update = perturbed_state.grad.sign() * step_eps\n",
    "            perturbed_state.data = torch.min(torch.max(perturbed_state + update, clamp_min), clamp_max)\n",
    "            policy_model.zero_grad()\n",
    "        return perturbed_state.detach()\n",
    "\n",
    "    elif attack_method == \"random\":\n",
    "        noise = torch.empty_like(state).uniform_(-eps, eps)\n",
    "        return (state + noise).detach()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attack method: {attack_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:34.021771Z",
     "iopub.status.busy": "2024-12-05T10:23:34.021480Z",
     "iopub.status.idle": "2024-12-05T10:23:34.033970Z",
     "shell.execute_reply": "2024-12-05T10:23:34.033266Z",
     "shell.execute_reply.started": "2024-12-05T10:23:34.021745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "attack_params = {\n",
    "    \"epsilon\": 0.1,  # Maximum perturbation magnitude\n",
    "    \"steps\": 5,      # Number of iterative steps\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:34.035426Z",
     "iopub.status.busy": "2024-12-05T10:23:34.035141Z",
     "iopub.status.idle": "2024-12-05T10:23:59.149864Z",
     "shell.execute_reply": "2024-12-05T10:23:59.149000Z",
     "shell.execute_reply.started": "2024-12-05T10:23:34.035399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 643.2179596799366\n",
      "Episode 2: Reward = 618.1933928958904\n",
      "Episode 3: Reward = 841.4080132820873\n",
      "Episode 4: Reward = 803.9976001779669\n",
      "Episode 5: Reward = 832.6988657782032\n",
      "Episode 6: Reward = 614.9587093459965\n",
      "Episode 7: Reward = 780.4425837788779\n",
      "Episode 8: Reward = 681.2243236692553\n",
      "Episode 9: Reward = 816.6033682256289\n",
      "Episode 10: Reward = 782.6961444127606\n",
      "Episode 11: Reward = 622.0367075702169\n",
      "Episode 12: Reward = 652.2318490121122\n",
      "Episode 13: Reward = 673.8185128354237\n",
      "Episode 14: Reward = 808.3609056900214\n",
      "Episode 15: Reward = 809.9699393608144\n",
      "Episode 16: Reward = 676.4921687786939\n",
      "Episode 17: Reward = 616.2496717034784\n",
      "Episode 18: Reward = 621.1462726134849\n",
      "Episode 19: Reward = 660.9665110276775\n",
      "Episode 20: Reward = 615.5766437813744\n",
      "Episode 21: Reward = 794.578690604192\n",
      "Episode 22: Reward = 694.1522397845266\n",
      "Episode 23: Reward = 686.5307736542325\n",
      "Episode 24: Reward = 806.076180772443\n",
      "Episode 25: Reward = 689.3570309546707\n",
      "Episode 26: Reward = 787.301661716881\n",
      "Episode 27: Reward = 779.9417497147288\n",
      "Episode 28: Reward = 820.4140287088983\n",
      "Episode 29: Reward = 816.9702968732569\n",
      "Episode 30: Reward = 799.0550463690067\n",
      "Episode 31: Reward = 824.1544076846188\n",
      "Episode 32: Reward = 635.588518710309\n",
      "Episode 33: Reward = 821.6382293759746\n",
      "Episode 34: Reward = 616.0817397317871\n",
      "Episode 35: Reward = 631.6137058634317\n",
      "Episode 36: Reward = 807.6614625269658\n",
      "Episode 37: Reward = 825.0278026883841\n",
      "Episode 38: Reward = 701.6363839387703\n",
      "Episode 39: Reward = 611.9561119504776\n",
      "Episode 40: Reward = 807.164344563894\n",
      "Episode 41: Reward = 695.6351951697883\n",
      "Episode 42: Reward = 807.7510500198794\n",
      "Episode 43: Reward = 827.9993725013792\n",
      "Episode 44: Reward = 618.5451954567307\n",
      "Episode 45: Reward = 677.4414484155145\n",
      "Episode 46: Reward = 758.5529424341717\n",
      "Episode 47: Reward = 817.7865202647183\n",
      "Episode 48: Reward = 645.613264761749\n",
      "Episode 49: Reward = 836.4040558009125\n",
      "Episode 50: Reward = 609.0106196728847\n",
      "Episode 51: Reward = 733.5822268966444\n",
      "Episode 52: Reward = 627.0479590136907\n",
      "Episode 53: Reward = 688.4222182514835\n",
      "Episode 54: Reward = 737.7626429750511\n",
      "Episode 55: Reward = 701.2016609499133\n",
      "Episode 56: Reward = 792.9597744777881\n",
      "Episode 57: Reward = 698.4028245545561\n",
      "Episode 58: Reward = 640.917499294913\n",
      "Episode 59: Reward = 831.7635121155031\n",
      "Episode 60: Reward = 621.2241165299874\n",
      "Episode 61: Reward = 646.3490465512301\n",
      "Episode 62: Reward = 631.8100089604811\n",
      "Episode 63: Reward = 833.8615633878204\n",
      "Episode 64: Reward = 824.136751459695\n",
      "Episode 65: Reward = 818.6092691080003\n",
      "Episode 66: Reward = 807.3561182392132\n",
      "Episode 67: Reward = 654.8387428724724\n",
      "Episode 68: Reward = 616.3579449650967\n",
      "Episode 69: Reward = 650.5373816022781\n",
      "Episode 70: Reward = 691.9178392140415\n",
      "Episode 71: Reward = 819.9037739815398\n",
      "Episode 72: Reward = 804.2907631834263\n",
      "Episode 73: Reward = 686.9858894655706\n",
      "Episode 74: Reward = 750.3191221655106\n",
      "Episode 75: Reward = 770.9232496381524\n",
      "Episode 76: Reward = 780.5249930039279\n",
      "Episode 77: Reward = 695.5960526097945\n",
      "Episode 78: Reward = 688.0003908943008\n",
      "Episode 79: Reward = 810.5382578263784\n",
      "Episode 80: Reward = 850.0379275176857\n",
      "Episode 81: Reward = 818.6215849849674\n",
      "Episode 82: Reward = 751.836849144952\n",
      "Episode 83: Reward = 836.2408089445064\n",
      "Episode 84: Reward = 829.3096461555085\n",
      "Episode 85: Reward = 663.8093313343338\n",
      "Episode 86: Reward = 654.7179321439503\n",
      "Episode 87: Reward = 767.9659970141918\n",
      "Episode 88: Reward = 627.892709340486\n",
      "Episode 89: Reward = 666.9344790989755\n",
      "Episode 90: Reward = 620.7467214256775\n",
      "Episode 91: Reward = 674.4818760307035\n",
      "Episode 92: Reward = 617.2757958838852\n",
      "Episode 93: Reward = 814.3314492131709\n",
      "Episode 94: Reward = 720.8755214860105\n",
      "Episode 95: Reward = 820.5106220832986\n",
      "Episode 96: Reward = 822.6609881653804\n",
      "Episode 97: Reward = 847.765726755321\n",
      "Episode 98: Reward = 622.707800244296\n",
      "Episode 99: Reward = 766.4084281949149\n",
      "Episode 100: Reward = 820.2494168351292\n",
      "Episode 101: Reward = 626.6221819666199\n",
      "Episode 102: Reward = 815.6880004053952\n",
      "Episode 103: Reward = 777.2764111063425\n",
      "Episode 104: Reward = 621.9182983417088\n",
      "Episode 105: Reward = 828.7415873408189\n",
      "Episode 106: Reward = 709.8980447087339\n",
      "Episode 107: Reward = 681.1073224403613\n",
      "Episode 108: Reward = 794.49840892898\n",
      "Episode 109: Reward = 839.20496234945\n",
      "Episode 110: Reward = 708.0753249400425\n",
      "Episode 111: Reward = 680.4861130399772\n",
      "Episode 112: Reward = 661.5557578995757\n",
      "Episode 113: Reward = 627.7861289414891\n",
      "Episode 114: Reward = 777.5384126176309\n",
      "Episode 115: Reward = 828.5857808874765\n",
      "Episode 116: Reward = 694.4055032373595\n",
      "Episode 117: Reward = 817.8763978064828\n",
      "Episode 118: Reward = 832.6075639238306\n",
      "Episode 119: Reward = 754.1003300663012\n",
      "Episode 120: Reward = 673.8476223686595\n",
      "Episode 121: Reward = 689.8284157468737\n",
      "Episode 122: Reward = 768.0360238275165\n",
      "Episode 123: Reward = 631.2438953509834\n",
      "Episode 124: Reward = 802.2573853556667\n",
      "Episode 125: Reward = 618.6208079642781\n",
      "Episode 126: Reward = 826.7323983935246\n",
      "Episode 127: Reward = 617.047272650854\n",
      "Episode 128: Reward = 704.3298012831006\n",
      "Episode 129: Reward = 614.0040700928006\n",
      "Episode 130: Reward = 628.5074260448165\n",
      "Episode 131: Reward = 649.1660253918897\n",
      "Episode 132: Reward = 822.7013703845556\n",
      "Episode 133: Reward = 820.7742045033195\n",
      "Episode 134: Reward = 764.0588652447536\n",
      "Episode 135: Reward = 642.2664194414538\n",
      "Episode 136: Reward = 818.8972798922182\n",
      "Episode 137: Reward = 730.697646926905\n",
      "Episode 138: Reward = 818.6309388024209\n",
      "Episode 139: Reward = 829.1466894640976\n",
      "Episode 140: Reward = 767.2036946301488\n",
      "Episode 141: Reward = 840.1630937144047\n",
      "Episode 142: Reward = 811.7815395604198\n",
      "Episode 143: Reward = 802.1547170786716\n",
      "Episode 144: Reward = 821.4533529230065\n",
      "Episode 145: Reward = 810.7088546189229\n",
      "Episode 146: Reward = 833.7464161100374\n",
      "Episode 147: Reward = 812.3331567669861\n",
      "Episode 148: Reward = 675.1539244550744\n",
      "Episode 149: Reward = 791.9332164699872\n",
      "Episode 150: Reward = 797.9045559362594\n",
      "Episode 151: Reward = 687.9184770100777\n",
      "Episode 152: Reward = 700.8299901270024\n",
      "Episode 153: Reward = 823.6182636428626\n",
      "Episode 154: Reward = 622.1841855969283\n",
      "Episode 155: Reward = 845.7064764065989\n",
      "Episode 156: Reward = 645.568830732262\n",
      "Episode 157: Reward = 742.8611210885259\n",
      "Episode 158: Reward = 687.5128494849631\n",
      "Episode 159: Reward = 776.4498194834941\n",
      "Episode 160: Reward = 751.580756727345\n",
      "Episode 161: Reward = 805.8229324727441\n",
      "Episode 162: Reward = 683.756410580099\n",
      "Episode 163: Reward = 665.7049122926429\n",
      "Episode 164: Reward = 818.9244866211696\n",
      "Episode 165: Reward = 681.3544734345888\n",
      "Episode 166: Reward = 720.7177308686977\n",
      "Episode 167: Reward = 785.6266342098799\n",
      "Episode 168: Reward = 615.6020023186333\n",
      "Episode 169: Reward = 685.0830409957708\n",
      "Episode 170: Reward = 691.188163451526\n",
      "Episode 171: Reward = 789.398491775953\n",
      "Episode 172: Reward = 628.9819518775078\n",
      "Episode 173: Reward = 839.3764300804347\n",
      "Episode 174: Reward = 846.8324494500501\n",
      "Episode 175: Reward = 814.198108117611\n",
      "Episode 176: Reward = 628.0490921861765\n",
      "Episode 177: Reward = 631.3602719188668\n",
      "Episode 178: Reward = 618.6173985612369\n",
      "Episode 179: Reward = 818.4473286080001\n",
      "Episode 180: Reward = 803.1748840370718\n",
      "Episode 181: Reward = 800.6809628439734\n",
      "Episode 182: Reward = 779.6408151487208\n",
      "Episode 183: Reward = 694.1218853047042\n",
      "Episode 184: Reward = 804.7651987077835\n",
      "Episode 185: Reward = 684.3358449900311\n",
      "Episode 186: Reward = 772.9107184542563\n",
      "Episode 187: Reward = 621.5481747511418\n",
      "Episode 188: Reward = 704.6444155413631\n",
      "Episode 189: Reward = 615.3995413016188\n",
      "Episode 190: Reward = 831.7356109190714\n",
      "Episode 191: Reward = 736.7310865606813\n",
      "Episode 192: Reward = 702.3578567568883\n",
      "Episode 193: Reward = 618.3164858250148\n",
      "Episode 194: Reward = 815.0105182482315\n",
      "Episode 195: Reward = 624.9927382402188\n",
      "Episode 196: Reward = 620.8920192109824\n",
      "Episode 197: Reward = 621.333049530778\n",
      "Episode 198: Reward = 753.1167383265733\n",
      "Episode 199: Reward = 613.1302828187974\n",
      "Episode 200: Reward = 802.9700182727424\n",
      "Average Reward over 200 episodes: 732.4191947937323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "732.4191947937323"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "def random_perturbation(state, epsilon):\n",
    "    \"\"\"\n",
    "    Apply random perturbation to the state.\n",
    "    Args:\n",
    "        state: The original state.\n",
    "        epsilon: The maximum magnitude of random noise.\n",
    "    Returns:\n",
    "        Perturbed state.\n",
    "    \"\"\"\n",
    "    noise = np.random.uniform(-epsilon, epsilon, size=state.shape)\n",
    "    perturbed_state = state + noise\n",
    "    return perturbed_state\n",
    "\n",
    "def evaluate_agent_with_random_attack(env, policy_net, epsilon=5, num_episodes=200):\n",
    "    \"\"\"\n",
    "    Evaluate the agent with random perturbation applied to states during testing.\n",
    "    Args:\n",
    "        env: The environment to test the agent.\n",
    "        policy_net: The trained policy network.\n",
    "        epsilon: Maximum magnitude of random noise for perturbation.\n",
    "        num_episodes: Number of episodes for evaluation.\n",
    "    Returns:\n",
    "        Average reward over the episodes.\n",
    "    \"\"\"\n",
    "    # Ensure policy network is on the same device as input tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net.to(device)\n",
    "    policy_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):  # Handle Gymnasium's (observation, info) format\n",
    "            state = state[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Apply random perturbation to the state\n",
    "            perturbed_state = random_perturbation(state, epsilon)\n",
    "\n",
    "            # Convert perturbed state to tensor and send to the same device as the policy network\n",
    "            state_tensor = torch.tensor(perturbed_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            # Get action from the policy network\n",
    "            with torch.no_grad():\n",
    "                policy_output = policy_net(state_tensor)\n",
    "                # If the policy network outputs a tuple, extract the action vector\n",
    "                if isinstance(policy_output, tuple):\n",
    "                    action = policy_output[0].cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    action = policy_output.cpu().numpy().squeeze()\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "evaluate_agent_with_random_attack(env, policy_net, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:59.151305Z",
     "iopub.status.busy": "2024-12-05T10:23:59.151037Z",
     "iopub.status.idle": "2024-12-05T10:23:59.161904Z",
     "shell.execute_reply": "2024-12-05T10:23:59.160864Z",
     "shell.execute_reply.started": "2024-12-05T10:23:59.151280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=10, step_epsilon=0.01, beta=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a MAD (Maximizing Action Discrepancy) attack for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon (float): Maximum perturbation magnitude.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of gradient steps for the attack.\n",
    "        step_epsilon (float): Step size for each gradient step.\n",
    "        beta (float): Inverse temperature parameter for SGLD noise.\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under MAD attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Start with the original state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            for _ in range(attack_steps):\n",
    "                # Compute the policy outputs for original and perturbed states\n",
    "                original_mean, original_std = policy_net(state.unsqueeze(0))\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "\n",
    "                # Compute KL divergence between original and perturbed distributions\n",
    "                loss = -torch.distributions.kl.kl_divergence(\n",
    "                    torch.distributions.Normal(original_mean, original_std),\n",
    "                    torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                ).mean()\n",
    "                loss.backward()\n",
    "\n",
    "                # Compute gradient and add noise for SGLD\n",
    "                grad = perturbed_state.grad\n",
    "                noise = torch.randn_like(perturbed_state) * torch.sqrt(torch.tensor(2 / (beta * step_epsilon), device=perturbed_state.device))\n",
    "                perturbation = step_epsilon * grad + noise\n",
    "\n",
    "                # Update the perturbed state\n",
    "                perturbed_state = (perturbed_state + perturbation).detach().requires_grad_(True)\n",
    "\n",
    "                # Clamp the perturbed state to within the epsilon-ball\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)\n",
    "\n",
    "            # Use the perturbed state to select the action\n",
    "            with torch.no_grad():\n",
    "                perturbed_mean, perturbed_std = policy_net(perturbed_state.unsqueeze(0))\n",
    "                action_dist = torch.distributions.Normal(perturbed_mean, perturbed_std)\n",
    "                action = action_dist.sample().squeeze().cpu().numpy()  # Match expected shape (e.g., (3,) for continuous action)\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under MAD attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:23:59.163073Z",
     "iopub.status.busy": "2024-12-05T10:23:59.162846Z",
     "iopub.status.idle": "2024-12-05T10:36:51.119554Z",
     "shell.execute_reply": "2024-12-05T10:36:51.118667Z",
     "shell.execute_reply.started": "2024-12-05T10:23:59.163050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 490.76059256410656\n",
      "Episode 2/200: Reward = 496.86761995695855\n",
      "Episode 3/200: Reward = 429.37915979316773\n",
      "Episode 4/200: Reward = 459.09612971450804\n",
      "Episode 5/200: Reward = 367.54676269930314\n",
      "Episode 6/200: Reward = 388.07699477480634\n",
      "Episode 7/200: Reward = 338.32899818937847\n",
      "Episode 8/200: Reward = 589.12682051278\n",
      "Episode 9/200: Reward = 411.90132819399577\n",
      "Episode 10/200: Reward = 373.74116294645074\n",
      "Episode 11/200: Reward = 284.2926478503728\n",
      "Episode 12/200: Reward = 504.4106117489463\n",
      "Episode 13/200: Reward = 432.28539027170746\n",
      "Episode 14/200: Reward = 497.0653769091538\n",
      "Episode 15/200: Reward = 467.2525191339566\n",
      "Episode 16/200: Reward = 358.23119336553907\n",
      "Episode 17/200: Reward = 343.82899772499235\n",
      "Episode 18/200: Reward = 358.57689000938353\n",
      "Episode 19/200: Reward = 568.3745288485321\n",
      "Episode 20/200: Reward = 511.613030794185\n",
      "Episode 21/200: Reward = 529.1848236726398\n",
      "Episode 22/200: Reward = 323.27726996684015\n",
      "Episode 23/200: Reward = 365.1999875101244\n",
      "Episode 24/200: Reward = 353.1944982331651\n",
      "Episode 25/200: Reward = 321.8023124172313\n",
      "Episode 26/200: Reward = 297.3925855372546\n",
      "Episode 27/200: Reward = 508.1773390163613\n",
      "Episode 28/200: Reward = 346.3279147607521\n",
      "Episode 29/200: Reward = 324.31440632963813\n",
      "Episode 30/200: Reward = 531.0534843601212\n",
      "Episode 31/200: Reward = 318.4090431478149\n",
      "Episode 32/200: Reward = 472.5304931404864\n",
      "Episode 33/200: Reward = 507.37051517073496\n",
      "Episode 34/200: Reward = 539.7905912350461\n",
      "Episode 35/200: Reward = 447.6917068619946\n",
      "Episode 36/200: Reward = 444.9145397072889\n",
      "Episode 37/200: Reward = 313.7431214364424\n",
      "Episode 38/200: Reward = 525.3350333530137\n",
      "Episode 39/200: Reward = 422.75666447703503\n",
      "Episode 40/200: Reward = 393.54571971816625\n",
      "Episode 41/200: Reward = 568.3468057918969\n",
      "Episode 42/200: Reward = 405.2754291049666\n",
      "Episode 43/200: Reward = 367.5473932664304\n",
      "Episode 44/200: Reward = 537.143013729522\n",
      "Episode 45/200: Reward = 536.5650932161924\n",
      "Episode 46/200: Reward = 319.92154112241275\n",
      "Episode 47/200: Reward = 452.7349000846474\n",
      "Episode 48/200: Reward = 494.2686363346043\n",
      "Episode 49/200: Reward = 532.928440820109\n",
      "Episode 50/200: Reward = 362.5077053339432\n",
      "Episode 51/200: Reward = 501.75631527234134\n",
      "Episode 52/200: Reward = 528.0315024909705\n",
      "Episode 53/200: Reward = 740.6192436001393\n",
      "Episode 54/200: Reward = 346.23730679791856\n",
      "Episode 55/200: Reward = 537.4993435688135\n",
      "Episode 56/200: Reward = 354.20662678841995\n",
      "Episode 57/200: Reward = 473.7824365510767\n",
      "Episode 58/200: Reward = 390.08474068504654\n",
      "Episode 59/200: Reward = 378.20223568752004\n",
      "Episode 60/200: Reward = 358.86729834889564\n",
      "Episode 61/200: Reward = 406.4572552935983\n",
      "Episode 62/200: Reward = 386.78724257472066\n",
      "Episode 63/200: Reward = 369.18268976333655\n",
      "Episode 64/200: Reward = 404.5220413723885\n",
      "Episode 65/200: Reward = 538.1721077443075\n",
      "Episode 66/200: Reward = 393.18396657167375\n",
      "Episode 67/200: Reward = 619.4926599127502\n",
      "Episode 68/200: Reward = 422.0574965329068\n",
      "Episode 69/200: Reward = 380.2976122911805\n",
      "Episode 70/200: Reward = 570.4346785094972\n",
      "Episode 71/200: Reward = 544.2099235054845\n",
      "Episode 72/200: Reward = 580.225777200705\n",
      "Episode 73/200: Reward = 522.0817019238151\n",
      "Episode 74/200: Reward = 484.22511726674844\n",
      "Episode 75/200: Reward = 360.0029526567704\n",
      "Episode 76/200: Reward = 585.2846204579486\n",
      "Episode 77/200: Reward = 428.36995214573255\n",
      "Episode 78/200: Reward = 486.8684254121858\n",
      "Episode 79/200: Reward = 547.8738656563456\n",
      "Episode 80/200: Reward = 379.18369156734855\n",
      "Episode 81/200: Reward = 436.9399929637071\n",
      "Episode 82/200: Reward = 601.7045788355188\n",
      "Episode 83/200: Reward = 437.1211669367297\n",
      "Episode 84/200: Reward = 422.55565783415085\n",
      "Episode 85/200: Reward = 543.5872505263482\n",
      "Episode 86/200: Reward = 558.5949974757174\n",
      "Episode 87/200: Reward = 549.6790289759152\n",
      "Episode 88/200: Reward = 514.6654653461235\n",
      "Episode 89/200: Reward = 506.0703352746861\n",
      "Episode 90/200: Reward = 426.3676852068425\n",
      "Episode 91/200: Reward = 283.69333395276647\n",
      "Episode 92/200: Reward = 591.0254332769741\n",
      "Episode 93/200: Reward = 560.8966524182948\n",
      "Episode 94/200: Reward = 398.5323938615591\n",
      "Episode 95/200: Reward = 347.90689997389416\n",
      "Episode 96/200: Reward = 552.741858060512\n",
      "Episode 97/200: Reward = 580.7033852189705\n",
      "Episode 98/200: Reward = 592.7595201252782\n",
      "Episode 99/200: Reward = 405.5999170802556\n",
      "Episode 100/200: Reward = 404.62803426711173\n",
      "Episode 101/200: Reward = 340.0414855952222\n",
      "Episode 102/200: Reward = 498.4295076465438\n",
      "Episode 103/200: Reward = 551.6648951099771\n",
      "Episode 104/200: Reward = 339.7155388860477\n",
      "Episode 105/200: Reward = 340.6520983865333\n",
      "Episode 106/200: Reward = 316.552966097074\n",
      "Episode 107/200: Reward = 658.6791547294505\n",
      "Episode 108/200: Reward = 347.78952033942954\n",
      "Episode 109/200: Reward = 592.1192658310904\n",
      "Episode 110/200: Reward = 327.8807567369765\n",
      "Episode 111/200: Reward = 681.5198856705562\n",
      "Episode 112/200: Reward = 532.317193250192\n",
      "Episode 113/200: Reward = 575.2747948602538\n",
      "Episode 114/200: Reward = 553.4096780604488\n",
      "Episode 115/200: Reward = 404.9391071527595\n",
      "Episode 116/200: Reward = 539.746197836607\n",
      "Episode 117/200: Reward = 549.5023716418822\n",
      "Episode 118/200: Reward = 371.85270711368617\n",
      "Episode 119/200: Reward = 382.2547418078538\n",
      "Episode 120/200: Reward = 366.03521171942106\n",
      "Episode 121/200: Reward = 548.2148102866653\n",
      "Episode 122/200: Reward = 319.29485926167166\n",
      "Episode 123/200: Reward = 326.8285899788155\n",
      "Episode 124/200: Reward = 442.41888143165465\n",
      "Episode 125/200: Reward = 368.8329246894296\n",
      "Episode 126/200: Reward = 568.4318188749916\n",
      "Episode 127/200: Reward = 545.5142526665194\n",
      "Episode 128/200: Reward = 590.2274137982348\n",
      "Episode 129/200: Reward = 330.7343185207242\n",
      "Episode 130/200: Reward = 340.0528293933414\n",
      "Episode 131/200: Reward = 580.4284218154004\n",
      "Episode 132/200: Reward = 446.8726899851314\n",
      "Episode 133/200: Reward = 545.8552221801201\n",
      "Episode 134/200: Reward = 408.79177373800417\n",
      "Episode 135/200: Reward = 768.4143785853896\n",
      "Episode 136/200: Reward = 384.88556777048956\n",
      "Episode 137/200: Reward = 764.351676579239\n",
      "Episode 138/200: Reward = 192.99332234182748\n",
      "Episode 139/200: Reward = 526.0356097838172\n",
      "Episode 140/200: Reward = 532.0812443735821\n",
      "Episode 141/200: Reward = 433.6544264056932\n",
      "Episode 142/200: Reward = 524.526841890829\n",
      "Episode 143/200: Reward = 560.3883687606865\n",
      "Episode 144/200: Reward = 532.0393809352813\n",
      "Episode 145/200: Reward = 542.6663364011021\n",
      "Episode 146/200: Reward = 464.2480864511139\n",
      "Episode 147/200: Reward = 337.9658565966512\n",
      "Episode 148/200: Reward = 333.5711022565681\n",
      "Episode 149/200: Reward = 555.0974546503936\n",
      "Episode 150/200: Reward = 432.23551348633566\n",
      "Episode 151/200: Reward = 315.28390500341783\n",
      "Episode 152/200: Reward = 405.5314072016471\n",
      "Episode 153/200: Reward = 441.5823002278028\n",
      "Episode 154/200: Reward = 537.8199394887013\n",
      "Episode 155/200: Reward = 704.8184954652005\n",
      "Episode 156/200: Reward = 588.7111380354728\n",
      "Episode 157/200: Reward = 367.9998562575958\n",
      "Episode 158/200: Reward = 370.38632235837133\n",
      "Episode 159/200: Reward = 352.67281130804514\n",
      "Episode 160/200: Reward = 559.2495463789122\n",
      "Episode 161/200: Reward = 480.97227791337616\n",
      "Episode 162/200: Reward = 481.546266266663\n",
      "Episode 163/200: Reward = 591.6999394298349\n",
      "Episode 164/200: Reward = 500.2277368488328\n",
      "Episode 165/200: Reward = 381.2530200488685\n",
      "Episode 166/200: Reward = 367.85039313227196\n",
      "Episode 167/200: Reward = 539.8096417861802\n",
      "Episode 168/200: Reward = 337.9824650108665\n",
      "Episode 169/200: Reward = 370.17336151621816\n",
      "Episode 170/200: Reward = 408.1410345246944\n",
      "Episode 171/200: Reward = 517.95378173282\n",
      "Episode 172/200: Reward = 386.60935957731135\n",
      "Episode 173/200: Reward = 346.749111969336\n",
      "Episode 174/200: Reward = 409.6940796964946\n",
      "Episode 175/200: Reward = 509.7468433410659\n",
      "Episode 176/200: Reward = 328.5538902627194\n",
      "Episode 177/200: Reward = 320.0167615237614\n",
      "Episode 178/200: Reward = 451.5786501862122\n",
      "Episode 179/200: Reward = 514.6450475287144\n",
      "Episode 180/200: Reward = 374.53712963002886\n",
      "Episode 181/200: Reward = 450.1751057256541\n",
      "Episode 182/200: Reward = 552.8022950646903\n",
      "Episode 183/200: Reward = 365.18044886146663\n",
      "Episode 184/200: Reward = 412.8427812143012\n",
      "Episode 185/200: Reward = 590.5896104945565\n",
      "Episode 186/200: Reward = 358.48224873120995\n",
      "Episode 187/200: Reward = 539.7650629766775\n",
      "Episode 188/200: Reward = 553.1288873086334\n",
      "Episode 189/200: Reward = 367.1805889851823\n",
      "Episode 190/200: Reward = 539.4861357350537\n",
      "Episode 191/200: Reward = 549.9908134974742\n",
      "Episode 192/200: Reward = 323.0522335852059\n",
      "Episode 193/200: Reward = 344.6448258809658\n",
      "Episode 194/200: Reward = 347.2798072608608\n",
      "Episode 195/200: Reward = 388.20815795050805\n",
      "Episode 196/200: Reward = 545.8086260663738\n",
      "Episode 197/200: Reward = 476.8208472719908\n",
      "Episode 198/200: Reward = 348.8575884383451\n",
      "Episode 199/200: Reward = 509.1709291374085\n",
      "Episode 200/200: Reward = 394.54199364837007\n",
      "Average Reward under MAD attack: 455.0390936723454\n",
      "Final Average Reward under MAD Attack: 455.0390936723454\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "\n",
    "# Evaluate the policy under MAD attack\n",
    "average_reward = evaluate_agent_with_mad_attack(env, policy_net, epsilon, num_episodes=200, attack_steps=attack_steps, step_epsilon=step_epsilon)\n",
    "print(f\"Final Average Reward under MAD Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:36:51.121466Z",
     "iopub.status.busy": "2024-12-05T10:36:51.121047Z",
     "iopub.status.idle": "2024-12-05T10:36:51.137624Z",
     "shell.execute_reply": "2024-12-05T10:36:51.136717Z",
     "shell.execute_reply.started": "2024-12-05T10:36:51.121412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def robust_sarsa_attack(env, policy_net, epsilon_schedule, num_steps=10000, lambda_rs=0.1, batch_size=64, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train a robust value function for a policy under attack using Robust Sarsa.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        epsilon_schedule (list): Schedule for perturbation magnitudes.\n",
    "        num_steps (int): Number of training steps.\n",
    "        lambda_rs (float): Regularization parameter for the robust objective.\n",
    "        batch_size (int): Number of transitions sampled per update.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The robust Q-value network.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "\n",
    "    # Detect action space type\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        action_dim = env.action_space.n  # Discrete action space\n",
    "    elif isinstance(env.action_space, gym.spaces.Box):\n",
    "        action_dim = env.action_space.shape[0]  # Continuous action space\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported action space type. Only Discrete and Box spaces are supported.\")\n",
    "\n",
    "    # Initialize Q-function (robust critic) as a neural network\n",
    "    q_net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(env.observation_space.shape[0] + action_dim, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)  # Single Q-value output\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-3)\n",
    "\n",
    "    # Replay buffer\n",
    "    replay_buffer = []\n",
    "\n",
    "    def collect_trajectory():\n",
    "        \"\"\"Collect one trajectory and add to the replay buffer.\"\"\"\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if necessary\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    # For discrete actions, pass only the state to Q-network\n",
    "                    q_values = torch.cat([q_net(torch.cat([state, torch.eye(action_dim)[a].to(device)], dim=0))\n",
    "                                          for a in range(action_dim)])\n",
    "                    action = torch.argmax(q_values).item()\n",
    "                else:\n",
    "                    # For continuous actions, extract mean from policy network\n",
    "                    policy_output = policy_net(state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output  # Extract mean and ignore std\n",
    "                    else:\n",
    "                        mean = policy_output  # If single output, it's the mean\n",
    "                    action = mean.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    \n",
    "            # Step the environment\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated  # Combine termination conditions\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "    \n",
    "            replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "            if len(replay_buffer) > 10000:\n",
    "                replay_buffer.pop(0)\n",
    "    \n",
    "            state = next_state\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Collect new trajectories periodically\n",
    "        if len(replay_buffer) < batch_size or step % 10 == 0:\n",
    "            collect_trajectory()\n",
    "\n",
    "        # Ensure the buffer has enough samples for a batch\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            continue  # Skip training step until buffer has enough data\n",
    "\n",
    "        # Sample batch\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Prepare inputs for Q-network\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).to(device)  # Discrete actions\n",
    "            state_action_pairs = torch.cat([states, torch.eye(action_dim).to(device)[actions]], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, torch.eye(action_dim).to(device)], dim=1)\n",
    "        else:\n",
    "            actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n",
    "            state_action_pairs = torch.cat([states, actions], dim=1)\n",
    "            next_state_action_pairs = torch.cat([next_states, actions], dim=1)\n",
    "\n",
    "        # Temporal Difference Loss\n",
    "        q_values = q_net(state_action_pairs).squeeze()\n",
    "        q_values_next = q_net(next_state_action_pairs).squeeze()\n",
    "        td_loss = (rewards + gamma * (1 - dones) * q_values_next - q_values).pow(2).mean()\n",
    "\n",
    "        # Robustness Loss\n",
    "        epsilon = epsilon_schedule[min(step, len(epsilon_schedule) - 1)]\n",
    "        robust_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            perturbation = (torch.rand_like(states[i]) * 2 - 1) * epsilon\n",
    "            perturbed_state = states[i] + perturbation\n",
    "            perturbed_state_action = torch.cat([perturbed_state, actions[i]], dim=0)\n",
    "            robust_loss += (q_net(perturbed_state_action.unsqueeze(0)) - q_values[i]).pow(2).mean()\n",
    "        robust_loss /= batch_size\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = td_loss + lambda_rs * robust_loss\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{num_steps}, TD Loss: {td_loss.item():.4f}, Robust Loss: {robust_loss.item():.4f}\")\n",
    "\n",
    "    return q_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:36:51.139138Z",
     "iopub.status.busy": "2024-12-05T10:36:51.138850Z",
     "iopub.status.idle": "2024-12-05T10:36:51.153219Z",
     "shell.execute_reply": "2024-12-05T10:36:51.152380Z",
     "shell.execute_reply.started": "2024-12-05T10:36:51.139113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_agent_with_robust_sarsa_attack(env, policy_net, robust_q_net, epsilon, step_size, num_episodes=100, attack_steps=10):\n",
    "    \"\"\"\n",
    "    Evaluate the agent under a Robust Sarsa Critic-based attack.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        policy_net (torch.nn.Module): The trained policy network.\n",
    "        robust_q_net (torch.nn.Module): The robust Q-value network trained with Robust Sarsa.\n",
    "        epsilon (float): Maximum perturbation magnitude for the attack.\n",
    "        step_size (float): Step size for the gradient update.\n",
    "        num_episodes (int): Number of episodes for evaluation.\n",
    "        attack_steps (int): Number of attack steps (K in the pseudocode).\n",
    "\n",
    "    Returns:\n",
    "        float: Average reward over the episodes under Robust Sarsa Critic-based attack.\n",
    "    \"\"\"\n",
    "    device = next(policy_net.parameters()).device\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Unpack if state is returned as (observation, info)\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Initialize the perturbed state\n",
    "            perturbed_state = state.clone().detach().requires_grad_(True)\n",
    "\n",
    "            # Perform the attack as per Algorithm 2\n",
    "            for _ in range(attack_steps):\n",
    "                # Forward pass through the policy to get the action\n",
    "                with torch.no_grad():\n",
    "                    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                        action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        action = torch.argmax(action_probs, dim=-1)\n",
    "                    else:\n",
    "                        policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                        if isinstance(policy_output, tuple):\n",
    "                            mean, _ = policy_output  # Extract mean and ignore std\n",
    "                        else:\n",
    "                            mean = policy_output\n",
    "                        action = mean.squeeze()\n",
    "\n",
    "                # Compute Q(s, a) for the critic\n",
    "                state_action = torch.cat([perturbed_state, action.float().to(device)]) if isinstance(env.action_space, gym.spaces.Box) else \\\n",
    "                               torch.cat([perturbed_state, torch.eye(env.action_space.n)[action].to(device)], dim=0)\n",
    "                q_value = robust_q_net(state_action.unsqueeze(0))\n",
    "\n",
    "                # Backpropagate the gradient\n",
    "                q_value.backward()\n",
    "                grad = perturbed_state.grad\n",
    "\n",
    "                # Update the perturbed state based on the gradient and step size\n",
    "                perturbed_state = perturbed_state - step_size * grad.sign()\n",
    "                perturbed_state = torch.max(\n",
    "                    torch.min(perturbed_state, state + epsilon), state - epsilon\n",
    "                ).detach().requires_grad_(True)  # Clamp to the epsilon-ball\n",
    "\n",
    "            # Use the adversarially perturbed state to select the final action\n",
    "            with torch.no_grad():\n",
    "                if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                    action_probs = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    action = torch.argmax(action_probs, dim=-1).item()\n",
    "                else:\n",
    "                    policy_output = policy_net(perturbed_state.unsqueeze(0))\n",
    "                    if isinstance(policy_output, tuple):\n",
    "                        mean, _ = policy_output\n",
    "                    else:\n",
    "                        mean = policy_output\n",
    "                    action = mean.squeeze().cpu().numpy()\n",
    "\n",
    "            # Step the environment\n",
    "            next_step_result = env.step(action)\n",
    "            if isinstance(next_step_result, tuple):\n",
    "                next_state, reward, done, _, _ = next_step_result\n",
    "            else:\n",
    "                next_state, reward, done = next_step_result[:3]\n",
    "\n",
    "            # Accumulate the reward\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    print(f\"Average Reward under Robust Sarsa Critic-based attack: {average_reward}\")\n",
    "    return average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:36:51.154647Z",
     "iopub.status.busy": "2024-12-05T10:36:51.154178Z",
     "iopub.status.idle": "2024-12-05T10:41:22.639972Z",
     "shell.execute_reply": "2024-12-05T10:41:22.639082Z",
     "shell.execute_reply.started": "2024-12-05T10:36:51.154621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4284193347.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32).to(device)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000, TD Loss: 10.2370, Robust Loss: 0.0000\n",
      "Step 100/5000, TD Loss: 9.6587, Robust Loss: 0.1440\n",
      "Step 200/5000, TD Loss: 11.1174, Robust Loss: 0.5529\n",
      "Step 300/5000, TD Loss: 10.4458, Robust Loss: 0.9164\n",
      "Step 400/5000, TD Loss: 8.8024, Robust Loss: 1.7482\n",
      "Step 500/5000, TD Loss: 7.7610, Robust Loss: 2.3427\n",
      "Step 600/5000, TD Loss: 8.8608, Robust Loss: 2.0869\n",
      "Step 700/5000, TD Loss: 8.5722, Robust Loss: 3.4817\n",
      "Step 800/5000, TD Loss: 6.7933, Robust Loss: 3.5360\n",
      "Step 900/5000, TD Loss: 7.3825, Robust Loss: 5.0513\n",
      "Step 1000/5000, TD Loss: 5.7984, Robust Loss: 5.9390\n",
      "Step 1100/5000, TD Loss: 6.9415, Robust Loss: 6.2464\n",
      "Step 1200/5000, TD Loss: 5.1660, Robust Loss: 7.2570\n",
      "Step 1300/5000, TD Loss: 6.2220, Robust Loss: 10.2141\n",
      "Step 1400/5000, TD Loss: 5.3497, Robust Loss: 9.2060\n",
      "Step 1500/5000, TD Loss: 5.4343, Robust Loss: 8.2461\n",
      "Step 1600/5000, TD Loss: 6.5990, Robust Loss: 10.2942\n",
      "Step 1700/5000, TD Loss: 4.8554, Robust Loss: 9.1940\n",
      "Step 1800/5000, TD Loss: 4.1388, Robust Loss: 13.4062\n",
      "Step 1900/5000, TD Loss: 4.0088, Robust Loss: 7.0937\n",
      "Step 2000/5000, TD Loss: 3.6801, Robust Loss: 10.4049\n",
      "Step 2100/5000, TD Loss: 3.8216, Robust Loss: 9.1513\n",
      "Step 2200/5000, TD Loss: 3.8754, Robust Loss: 17.4665\n",
      "Step 2300/5000, TD Loss: 3.3470, Robust Loss: 10.4591\n",
      "Step 2400/5000, TD Loss: 3.1977, Robust Loss: 16.1658\n",
      "Step 2500/5000, TD Loss: 3.2615, Robust Loss: 11.3426\n",
      "Step 2600/5000, TD Loss: 4.2895, Robust Loss: 11.2495\n",
      "Step 2700/5000, TD Loss: 3.0105, Robust Loss: 11.1808\n",
      "Step 2800/5000, TD Loss: 3.6767, Robust Loss: 12.4273\n",
      "Step 2900/5000, TD Loss: 3.5990, Robust Loss: 14.1426\n",
      "Step 3000/5000, TD Loss: 3.2715, Robust Loss: 9.8738\n",
      "Step 3100/5000, TD Loss: 3.1821, Robust Loss: 14.0603\n",
      "Step 3200/5000, TD Loss: 3.9274, Robust Loss: 11.4414\n",
      "Step 3300/5000, TD Loss: 3.2633, Robust Loss: 8.3722\n",
      "Step 3400/5000, TD Loss: 2.5626, Robust Loss: 10.7993\n",
      "Step 3500/5000, TD Loss: 2.5076, Robust Loss: 12.4304\n",
      "Step 3600/5000, TD Loss: 2.0379, Robust Loss: 10.6195\n",
      "Step 3700/5000, TD Loss: 1.8952, Robust Loss: 12.6779\n",
      "Step 3800/5000, TD Loss: 2.2786, Robust Loss: 10.6627\n",
      "Step 3900/5000, TD Loss: 2.1921, Robust Loss: 13.1435\n",
      "Step 4000/5000, TD Loss: 2.0770, Robust Loss: 9.0072\n",
      "Step 4100/5000, TD Loss: 2.5812, Robust Loss: 9.9939\n",
      "Step 4200/5000, TD Loss: 1.9408, Robust Loss: 10.9128\n",
      "Step 4300/5000, TD Loss: 2.1082, Robust Loss: 10.8455\n",
      "Step 4400/5000, TD Loss: 1.8041, Robust Loss: 7.8556\n",
      "Step 4500/5000, TD Loss: 1.8263, Robust Loss: 10.4810\n",
      "Step 4600/5000, TD Loss: 1.9547, Robust Loss: 9.9229\n",
      "Step 4700/5000, TD Loss: 1.4773, Robust Loss: 8.6215\n",
      "Step 4800/5000, TD Loss: 3.5002, Robust Loss: 10.1851\n",
      "Step 4900/5000, TD Loss: 1.7376, Robust Loss: 10.8838\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = gym.make(\"Hopper-v4\")\n",
    "\n",
    "# Initialize the trained policy network\n",
    "policy_net = VanillaAgent.policy_net  # Use your trained policy network here\n",
    "\n",
    "# Parameters for MAD attack\n",
    "epsilon = 0.1  # Maximum perturbation magnitude\n",
    "attack_steps = 10  # Number of gradient steps\n",
    "step_epsilon = 0.01  # Step size for each gradient step\n",
    "epsilon_schedule = [0.01 * i for i in range(1, 101)]\n",
    "# Evaluate the policy under MAD attack\n",
    "\n",
    "robust_q_net=robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    epsilon_schedule=epsilon_schedule,\n",
    "    num_steps=5000,        # Number of training steps\n",
    "    lambda_rs=0.1,         # Regularization parameter for robust loss\n",
    "    batch_size=64,         # Batch size for training\n",
    "    gamma=0.99             # Discount factor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:41:22.641994Z",
     "iopub.status.busy": "2024-12-05T10:41:22.641573Z",
     "iopub.status.idle": "2024-12-05T10:49:36.303289Z",
     "shell.execute_reply": "2024-12-05T10:49:36.302351Z",
     "shell.execute_reply.started": "2024-12-05T10:41:22.641952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200: Reward = 707.6355391862724\n",
      "Episode 2/200: Reward = 782.3928646176172\n",
      "Episode 3/200: Reward = 730.4273015490278\n",
      "Episode 4/200: Reward = 792.0566380946177\n",
      "Episode 5/200: Reward = 795.3140327485272\n",
      "Episode 6/200: Reward = 757.1318450993338\n",
      "Episode 7/200: Reward = 732.0321066416775\n",
      "Episode 8/200: Reward = 751.6878242035779\n",
      "Episode 9/200: Reward = 762.0241656887515\n",
      "Episode 10/200: Reward = 725.4236209713097\n",
      "Episode 11/200: Reward = 723.6604742676035\n",
      "Episode 12/200: Reward = 789.0231857055218\n",
      "Episode 13/200: Reward = 765.1176826890908\n",
      "Episode 14/200: Reward = 757.3359642031894\n",
      "Episode 15/200: Reward = 769.0141742093816\n",
      "Episode 16/200: Reward = 799.6117915357756\n",
      "Episode 17/200: Reward = 777.4660001811559\n",
      "Episode 18/200: Reward = 792.4818356604893\n",
      "Episode 19/200: Reward = 792.3570785940767\n",
      "Episode 20/200: Reward = 786.5296506815274\n",
      "Episode 21/200: Reward = 778.4934676414505\n",
      "Episode 22/200: Reward = 797.7870627998079\n",
      "Episode 23/200: Reward = 778.0045903774476\n",
      "Episode 24/200: Reward = 764.9877625471144\n",
      "Episode 25/200: Reward = 736.7816906653434\n",
      "Episode 26/200: Reward = 733.5806320482747\n",
      "Episode 27/200: Reward = 708.4709752255216\n",
      "Episode 28/200: Reward = 713.8170762650595\n",
      "Episode 29/200: Reward = 781.3603518747299\n",
      "Episode 30/200: Reward = 802.1686273092066\n",
      "Episode 31/200: Reward = 792.6804208742501\n",
      "Episode 32/200: Reward = 758.3250185803588\n",
      "Episode 33/200: Reward = 753.3074264659213\n",
      "Episode 34/200: Reward = 742.477578062988\n",
      "Episode 35/200: Reward = 796.6714148702063\n",
      "Episode 36/200: Reward = 780.9058273604068\n",
      "Episode 37/200: Reward = 780.7119073316962\n",
      "Episode 38/200: Reward = 777.9030318944526\n",
      "Episode 39/200: Reward = 737.6859488001304\n",
      "Episode 40/200: Reward = 762.4607214002666\n",
      "Episode 41/200: Reward = 766.1235210320787\n",
      "Episode 42/200: Reward = 798.6300512467432\n",
      "Episode 43/200: Reward = 802.9574875745275\n",
      "Episode 44/200: Reward = 745.9467363683789\n",
      "Episode 45/200: Reward = 760.0485584587457\n",
      "Episode 46/200: Reward = 799.2081299777349\n",
      "Episode 47/200: Reward = 764.2829592296972\n",
      "Episode 48/200: Reward = 775.2920922539952\n",
      "Episode 49/200: Reward = 805.7879350459176\n",
      "Episode 50/200: Reward = 778.815674406176\n",
      "Episode 51/200: Reward = 756.4615501005713\n",
      "Episode 52/200: Reward = 774.5302590311005\n",
      "Episode 53/200: Reward = 750.6901818751229\n",
      "Episode 54/200: Reward = 752.8979935919956\n",
      "Episode 55/200: Reward = 777.8920636599081\n",
      "Episode 56/200: Reward = 764.2556827358626\n",
      "Episode 57/200: Reward = 786.674282540218\n",
      "Episode 58/200: Reward = 758.711989986317\n",
      "Episode 59/200: Reward = 707.9603921052027\n",
      "Episode 60/200: Reward = 801.3153353973321\n",
      "Episode 61/200: Reward = 763.826791319686\n",
      "Episode 62/200: Reward = 742.1004648055621\n",
      "Episode 63/200: Reward = 769.0441242631142\n",
      "Episode 64/200: Reward = 803.5902054412741\n",
      "Episode 65/200: Reward = 798.7653738847349\n",
      "Episode 66/200: Reward = 805.1959128816569\n",
      "Episode 67/200: Reward = 800.071230428698\n",
      "Episode 68/200: Reward = 749.6000642969042\n",
      "Episode 69/200: Reward = 782.0063776817633\n",
      "Episode 70/200: Reward = 733.2506497132638\n",
      "Episode 71/200: Reward = 741.715998514521\n",
      "Episode 72/200: Reward = 775.4508774238712\n",
      "Episode 73/200: Reward = 796.3606882658105\n",
      "Episode 74/200: Reward = 783.694004287228\n",
      "Episode 75/200: Reward = 779.4224849864015\n",
      "Episode 76/200: Reward = 705.3114712346998\n",
      "Episode 77/200: Reward = 749.1272234440196\n",
      "Episode 78/200: Reward = 795.7059289318607\n",
      "Episode 79/200: Reward = 772.5760545932992\n",
      "Episode 80/200: Reward = 773.0631445095838\n",
      "Episode 81/200: Reward = 747.4129192851952\n",
      "Episode 82/200: Reward = 794.6836109072027\n",
      "Episode 83/200: Reward = 730.3601460086501\n",
      "Episode 84/200: Reward = 796.8452477169355\n",
      "Episode 85/200: Reward = 733.5004744190774\n",
      "Episode 86/200: Reward = 759.5938383955305\n",
      "Episode 87/200: Reward = 803.9994144894458\n",
      "Episode 88/200: Reward = 794.2428561420294\n",
      "Episode 89/200: Reward = 799.7716087377436\n",
      "Episode 90/200: Reward = 741.7462612946294\n",
      "Episode 91/200: Reward = 759.2489943543745\n",
      "Episode 92/200: Reward = 777.8980506522407\n",
      "Episode 93/200: Reward = 784.3290535122646\n",
      "Episode 94/200: Reward = 788.3322132396422\n",
      "Episode 95/200: Reward = 729.8925423521666\n",
      "Episode 96/200: Reward = 754.9316843678286\n",
      "Episode 97/200: Reward = 755.8610677293477\n",
      "Episode 98/200: Reward = 752.5508724393081\n",
      "Episode 99/200: Reward = 728.2238138163871\n",
      "Episode 100/200: Reward = 699.1930699193766\n",
      "Episode 101/200: Reward = 776.9238917647767\n",
      "Episode 102/200: Reward = 782.5225007027186\n",
      "Episode 103/200: Reward = 768.2546018416391\n",
      "Episode 104/200: Reward = 728.9769850172557\n",
      "Episode 105/200: Reward = 778.1973531310219\n",
      "Episode 106/200: Reward = 796.7110109390433\n",
      "Episode 107/200: Reward = 768.7417262254964\n",
      "Episode 108/200: Reward = 781.2119406945798\n",
      "Episode 109/200: Reward = 706.2280548785218\n",
      "Episode 110/200: Reward = 800.5449647187746\n",
      "Episode 111/200: Reward = 793.8782216309481\n",
      "Episode 112/200: Reward = 750.9942835598636\n",
      "Episode 113/200: Reward = 740.9655392969978\n",
      "Episode 114/200: Reward = 792.1528270487083\n",
      "Episode 115/200: Reward = 755.9781050701237\n",
      "Episode 116/200: Reward = 768.0739792067691\n",
      "Episode 117/200: Reward = 800.9022330100944\n",
      "Episode 118/200: Reward = 784.0580570250942\n",
      "Episode 119/200: Reward = 761.4673284081167\n",
      "Episode 120/200: Reward = 739.9043951507346\n",
      "Episode 121/200: Reward = 789.4230286511487\n",
      "Episode 122/200: Reward = 800.4975340403282\n",
      "Episode 123/200: Reward = 720.3740821466408\n",
      "Episode 124/200: Reward = 785.1868586900741\n",
      "Episode 125/200: Reward = 798.4116085678146\n",
      "Episode 126/200: Reward = 774.3881300149843\n",
      "Episode 127/200: Reward = 799.4440236341625\n",
      "Episode 128/200: Reward = 784.4480450927981\n",
      "Episode 129/200: Reward = 774.3352780571487\n",
      "Episode 130/200: Reward = 811.6160606938522\n",
      "Episode 131/200: Reward = 794.7558783360456\n",
      "Episode 132/200: Reward = 789.20665781054\n",
      "Episode 133/200: Reward = 776.2166532582644\n",
      "Episode 134/200: Reward = 792.3787478573985\n",
      "Episode 135/200: Reward = 795.5359034361232\n",
      "Episode 136/200: Reward = 762.7304251144209\n",
      "Episode 137/200: Reward = 748.0896459683304\n",
      "Episode 138/200: Reward = 785.9491026778912\n",
      "Episode 139/200: Reward = 754.8148473109517\n",
      "Episode 140/200: Reward = 760.0941212063397\n",
      "Episode 141/200: Reward = 803.1656217024936\n",
      "Episode 142/200: Reward = 776.0973447578365\n",
      "Episode 143/200: Reward = 803.1496004616531\n",
      "Episode 144/200: Reward = 751.2198721834521\n",
      "Episode 145/200: Reward = 745.6112375529767\n",
      "Episode 146/200: Reward = 769.552926314568\n",
      "Episode 147/200: Reward = 740.0339681430717\n",
      "Episode 148/200: Reward = 739.5320031176376\n",
      "Episode 149/200: Reward = 784.4627268662197\n",
      "Episode 150/200: Reward = 714.3925679201996\n",
      "Episode 151/200: Reward = 803.9741700831005\n",
      "Episode 152/200: Reward = 795.8616067525622\n",
      "Episode 153/200: Reward = 778.3783717260104\n",
      "Episode 154/200: Reward = 789.998446680449\n",
      "Episode 155/200: Reward = 771.7129530005874\n",
      "Episode 156/200: Reward = 753.8765239410124\n",
      "Episode 157/200: Reward = 805.8534921218323\n",
      "Episode 158/200: Reward = 714.0597286595187\n",
      "Episode 159/200: Reward = 744.5872091726552\n",
      "Episode 160/200: Reward = 790.6131915106827\n",
      "Episode 161/200: Reward = 800.7427021537258\n",
      "Episode 162/200: Reward = 757.1865356562511\n",
      "Episode 163/200: Reward = 797.7648864178541\n",
      "Episode 164/200: Reward = 740.594709138836\n",
      "Episode 165/200: Reward = 735.5508510266543\n",
      "Episode 166/200: Reward = 793.0193650351537\n",
      "Episode 167/200: Reward = 790.8599650543874\n",
      "Episode 168/200: Reward = 777.6792548234704\n",
      "Episode 169/200: Reward = 795.8070036444341\n",
      "Episode 170/200: Reward = 790.9135514484831\n",
      "Episode 171/200: Reward = 767.141472945998\n",
      "Episode 172/200: Reward = 780.6904922246324\n",
      "Episode 173/200: Reward = 767.2574483055548\n",
      "Episode 174/200: Reward = 763.2704667191714\n",
      "Episode 175/200: Reward = 772.0042635254825\n",
      "Episode 176/200: Reward = 738.04049481497\n",
      "Episode 177/200: Reward = 802.9433224021033\n",
      "Episode 178/200: Reward = 770.4548294569908\n",
      "Episode 179/200: Reward = 790.2435918889225\n",
      "Episode 180/200: Reward = 744.4546248680814\n",
      "Episode 181/200: Reward = 749.7042719888469\n",
      "Episode 182/200: Reward = 736.5489356212478\n",
      "Episode 183/200: Reward = 792.5912863507967\n",
      "Episode 184/200: Reward = 806.5533376286573\n",
      "Episode 185/200: Reward = 789.5840195345861\n",
      "Episode 186/200: Reward = 760.1119365134878\n",
      "Episode 187/200: Reward = 756.52788554083\n",
      "Episode 188/200: Reward = 782.4604013589337\n",
      "Episode 189/200: Reward = 797.1360340339829\n",
      "Episode 190/200: Reward = 765.7968744440393\n",
      "Episode 191/200: Reward = 765.7169473416908\n",
      "Episode 192/200: Reward = 771.9538016220084\n",
      "Episode 193/200: Reward = 772.1197200993635\n",
      "Episode 194/200: Reward = 731.6684972578147\n",
      "Episode 195/200: Reward = 743.9324440812806\n",
      "Episode 196/200: Reward = 742.1657212808897\n",
      "Episode 197/200: Reward = 798.6624075198896\n",
      "Episode 198/200: Reward = 754.4777184194857\n",
      "Episode 199/200: Reward = 756.9670935801116\n",
      "Episode 200/200: Reward = 761.8005992029051\n",
      "Average Reward under Robust Sarsa Critic-based attack: 769.0605147787755\n",
      "Final Average Reward under Robust Sarsa Attack: 769.0605147787755\n"
     ]
    }
   ],
   "source": [
    "average_reward = average_reward = evaluate_agent_with_robust_sarsa_attack(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    robust_q_net=robust_q_net,\n",
    "    epsilon=0.05,\n",
    "    num_episodes=200,\n",
    "    step_size=0.01\n",
    ")\n",
    "print(f\"Final Average Reward under Robust Sarsa Attack: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T10:49:36.336552Z",
     "iopub.status.busy": "2024-12-05T10:49:36.335657Z",
     "iopub.status.idle": "2024-12-05T15:01:21.908446Z",
     "shell.execute_reply": "2024-12-05T15:01:21.907370Z",
     "shell.execute_reply.started": "2024-12-05T10:49:36.336508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Policy Loss = -8.539791107177734, Value Loss = 91.50924682617188, KL Reg = 1.0124143955181353e-05\n",
      "Episode 2: Policy Loss = -14.921133041381836, Value Loss = 221.65118408203125, KL Reg = 1.2055054867232684e-05\n",
      "Episode 3: Policy Loss = -4.782430648803711, Value Loss = 120.10716247558594, KL Reg = 1.5467019693460315e-05\n",
      "Episode 4: Policy Loss = -6.200763702392578, Value Loss = 64.98123168945312, KL Reg = 2.2710195480613038e-05\n",
      "Episode 5: Policy Loss = -9.923398971557617, Value Loss = 157.38397216796875, KL Reg = 3.756040314328857e-05\n",
      "Episode 6: Policy Loss = -10.554612159729004, Value Loss = 216.71499633789062, KL Reg = 5.482680717250332e-05\n",
      "Episode 7: Policy Loss = -10.816160202026367, Value Loss = 239.9801025390625, KL Reg = 7.194499630713835e-05\n",
      "Episode 8: Policy Loss = -8.576520919799805, Value Loss = 209.21029663085938, KL Reg = 9.797522943699732e-05\n",
      "Episode 9: Policy Loss = -5.71106481552124, Value Loss = 173.89205932617188, KL Reg = 0.0001289803913095966\n",
      "Episode 10: Policy Loss = -12.812941551208496, Value Loss = 266.5433654785156, KL Reg = 0.00015061003796290606\n",
      "Episode 11: Policy Loss = -2.8076770305633545, Value Loss = 231.26776123046875, KL Reg = 0.0001816509902710095\n",
      "Episode 12: Policy Loss = -10.489791870117188, Value Loss = 573.7066650390625, KL Reg = 0.0001898663176689297\n",
      "Episode 13: Policy Loss = -12.164995193481445, Value Loss = 589.9742431640625, KL Reg = 0.00021378588280640543\n",
      "Episode 14: Policy Loss = -13.248479843139648, Value Loss = 379.1156311035156, KL Reg = 0.0002116763062076643\n",
      "Episode 15: Policy Loss = -14.317716598510742, Value Loss = 499.4757080078125, KL Reg = 0.00024035133537836373\n",
      "Episode 16: Policy Loss = -12.438724517822266, Value Loss = 460.1611328125, KL Reg = 0.000274428486591205\n",
      "Episode 17: Policy Loss = -5.799797534942627, Value Loss = 297.166259765625, KL Reg = 0.0003383452130947262\n",
      "Episode 18: Policy Loss = -5.94990873336792, Value Loss = 295.5150451660156, KL Reg = 0.00037552075809799135\n",
      "Episode 19: Policy Loss = -11.739875793457031, Value Loss = 713.5875854492188, KL Reg = 0.00044451782014220953\n",
      "Episode 20: Policy Loss = -0.47879791259765625, Value Loss = 701.6289672851562, KL Reg = 0.00044759453157894313\n",
      "Episode 21: Policy Loss = 2.213884115219116, Value Loss = 488.9283447265625, KL Reg = 0.0004612741759046912\n",
      "Episode 22: Policy Loss = 4.343314170837402, Value Loss = 739.248046875, KL Reg = 0.00045996354310773313\n",
      "Episode 23: Policy Loss = 12.29129695892334, Value Loss = 649.2523193359375, KL Reg = 0.0005113395163789392\n",
      "Episode 24: Policy Loss = 0.6936101913452148, Value Loss = 882.4385375976562, KL Reg = 0.0005614898400381207\n",
      "Episode 25: Policy Loss = 11.710406303405762, Value Loss = 684.4908447265625, KL Reg = 0.00070335587952286\n",
      "Episode 26: Policy Loss = 5.247302055358887, Value Loss = 586.07568359375, KL Reg = 0.0007044323137961328\n",
      "Episode 27: Policy Loss = -5.698000431060791, Value Loss = 610.7299194335938, KL Reg = 0.0006763850105926394\n",
      "Episode 28: Policy Loss = 14.250818252563477, Value Loss = 786.2076416015625, KL Reg = 0.0007395363645628095\n",
      "Episode 29: Policy Loss = -4.655186176300049, Value Loss = 818.4962768554688, KL Reg = 0.0006696382770314813\n",
      "Episode 30: Policy Loss = -1.2687902450561523, Value Loss = 501.128662109375, KL Reg = 0.0007696251850575209\n",
      "Episode 31: Policy Loss = 12.46805191040039, Value Loss = 912.8328247070312, KL Reg = 0.000777280714828521\n",
      "Episode 32: Policy Loss = -10.163238525390625, Value Loss = 373.62469482421875, KL Reg = 0.0007649861508980393\n",
      "Episode 33: Policy Loss = 10.978716850280762, Value Loss = 700.9910888671875, KL Reg = 0.0007566884160041809\n",
      "Episode 34: Policy Loss = -9.965599060058594, Value Loss = 446.97259521484375, KL Reg = 0.0008472622139379382\n",
      "Episode 35: Policy Loss = 8.786140441894531, Value Loss = 665.8465576171875, KL Reg = 0.0009159791516140103\n",
      "Episode 36: Policy Loss = 11.652616500854492, Value Loss = 733.0987548828125, KL Reg = 0.0008399419602937996\n",
      "Episode 37: Policy Loss = 7.286022186279297, Value Loss = 869.4002075195312, KL Reg = 0.0008569994242861867\n",
      "Episode 38: Policy Loss = -4.962062358856201, Value Loss = 174.7106475830078, KL Reg = 0.0009145142394118011\n",
      "Episode 39: Policy Loss = 11.735840797424316, Value Loss = 819.856689453125, KL Reg = 0.001021173084154725\n",
      "Episode 40: Policy Loss = 17.078779220581055, Value Loss = 964.9102783203125, KL Reg = 0.0009804838337004185\n",
      "Episode 41: Policy Loss = 13.781349182128906, Value Loss = 935.5406494140625, KL Reg = 0.001036743400618434\n",
      "Episode 42: Policy Loss = 10.553094863891602, Value Loss = 884.4703979492188, KL Reg = 0.0011483316775411367\n",
      "Episode 43: Policy Loss = 3.1157283782958984, Value Loss = 867.44482421875, KL Reg = 0.0012756676878780127\n",
      "Episode 44: Policy Loss = 9.284795761108398, Value Loss = 932.34912109375, KL Reg = 0.0012348276795819402\n",
      "Episode 45: Policy Loss = 9.307263374328613, Value Loss = 1153.776611328125, KL Reg = 0.0012588530080392957\n",
      "Episode 46: Policy Loss = 9.336772918701172, Value Loss = 912.3176879882812, KL Reg = 0.0013474503066390753\n",
      "Episode 47: Policy Loss = 2.926954507827759, Value Loss = 915.063720703125, KL Reg = 0.0012593864230439067\n",
      "Episode 48: Policy Loss = 13.200201034545898, Value Loss = 1064.2320556640625, KL Reg = 0.001351115177385509\n",
      "Episode 49: Policy Loss = 22.01775550842285, Value Loss = 1372.95361328125, KL Reg = 0.0014206587802618742\n",
      "Episode 50: Policy Loss = 9.36373519897461, Value Loss = 1181.948974609375, KL Reg = 0.0014957495732232928\n",
      "Episode 51: Policy Loss = 17.38982391357422, Value Loss = 1282.2479248046875, KL Reg = 0.0014272576663643122\n",
      "Episode 52: Policy Loss = 29.680084228515625, Value Loss = 2016.447998046875, KL Reg = 0.0014719062019139528\n",
      "Episode 53: Policy Loss = 11.79954719543457, Value Loss = 1149.929931640625, KL Reg = 0.0014655576087534428\n",
      "Episode 54: Policy Loss = 23.00006866455078, Value Loss = 1661.5037841796875, KL Reg = 0.0015292044263333082\n",
      "Episode 55: Policy Loss = 23.547985076904297, Value Loss = 1753.907958984375, KL Reg = 0.001495844917371869\n",
      "Episode 56: Policy Loss = 24.94106674194336, Value Loss = 1835.0008544921875, KL Reg = 0.0015019116690382361\n",
      "Episode 57: Policy Loss = -5.149930000305176, Value Loss = 511.3501892089844, KL Reg = 0.0015296770725399256\n",
      "Episode 58: Policy Loss = 13.883113861083984, Value Loss = 1422.326171875, KL Reg = 0.0017102197743952274\n",
      "Episode 59: Policy Loss = 7.67513370513916, Value Loss = 732.6346435546875, KL Reg = 0.0017300830222666264\n",
      "Episode 60: Policy Loss = 12.059598922729492, Value Loss = 1125.691650390625, KL Reg = 0.0017340120393782854\n",
      "Episode 61: Policy Loss = 15.709847450256348, Value Loss = 1766.28857421875, KL Reg = 0.0018668725388124585\n",
      "Episode 62: Policy Loss = 5.4572014808654785, Value Loss = 1161.1790771484375, KL Reg = 0.002030283445492387\n",
      "Episode 63: Policy Loss = 23.395328521728516, Value Loss = 2186.6181640625, KL Reg = 0.00204614270478487\n",
      "Episode 64: Policy Loss = 16.78069305419922, Value Loss = 1870.279541015625, KL Reg = 0.0022501377388834953\n",
      "Episode 65: Policy Loss = 28.95863151550293, Value Loss = 2399.03662109375, KL Reg = 0.002246253890916705\n",
      "Episode 66: Policy Loss = 22.542354583740234, Value Loss = 2045.9261474609375, KL Reg = 0.0022481109481304884\n",
      "Episode 67: Policy Loss = -4.074537754058838, Value Loss = 280.06951904296875, KL Reg = 0.0022688701283186674\n",
      "Episode 68: Policy Loss = 27.708972930908203, Value Loss = 2781.60546875, KL Reg = 0.002218295121565461\n",
      "Episode 69: Policy Loss = 24.19673728942871, Value Loss = 2697.887939453125, KL Reg = 0.0022619501687586308\n",
      "Episode 70: Policy Loss = 22.97617530822754, Value Loss = 2200.4169921875, KL Reg = 0.002404967788606882\n",
      "Episode 71: Policy Loss = 22.628719329833984, Value Loss = 2179.709228515625, KL Reg = 0.0025632695760577917\n",
      "Episode 72: Policy Loss = 16.672555923461914, Value Loss = 1933.6915283203125, KL Reg = 0.002541090128943324\n",
      "Episode 73: Policy Loss = 33.51470947265625, Value Loss = 2963.567138671875, KL Reg = 0.0026352268178015947\n",
      "Episode 74: Policy Loss = 32.40113830566406, Value Loss = 2957.48583984375, KL Reg = 0.0027881667483597994\n",
      "Episode 75: Policy Loss = 28.953433990478516, Value Loss = 3006.818115234375, KL Reg = 0.002638637786731124\n",
      "Episode 76: Policy Loss = 35.806278228759766, Value Loss = 2963.43994140625, KL Reg = 0.002550369594246149\n",
      "Episode 77: Policy Loss = 29.11386489868164, Value Loss = 2511.348388671875, KL Reg = 0.0026460879016667604\n",
      "Episode 78: Policy Loss = 33.9842643737793, Value Loss = 3255.97314453125, KL Reg = 0.002722285222262144\n",
      "Episode 79: Policy Loss = 33.17262268066406, Value Loss = 3113.37451171875, KL Reg = 0.0028142950031906366\n",
      "Episode 80: Policy Loss = 9.76301097869873, Value Loss = 2635.149169921875, KL Reg = 0.002909358125180006\n",
      "Episode 81: Policy Loss = 35.51625061035156, Value Loss = 3211.10009765625, KL Reg = 0.003050327766686678\n",
      "Episode 82: Policy Loss = 35.1386604309082, Value Loss = 3105.076171875, KL Reg = 0.003427396062761545\n",
      "Episode 83: Policy Loss = 22.997570037841797, Value Loss = 2641.7509765625, KL Reg = 0.0031463780906051397\n",
      "Episode 84: Policy Loss = 24.973464965820312, Value Loss = 3490.331298828125, KL Reg = 0.0032278583385050297\n",
      "Episode 85: Policy Loss = 21.348371505737305, Value Loss = 2581.174560546875, KL Reg = 0.003498596139252186\n",
      "Episode 86: Policy Loss = 20.859275817871094, Value Loss = 2934.350341796875, KL Reg = 0.002934427931904793\n",
      "Episode 87: Policy Loss = 38.442054748535156, Value Loss = 3452.033935546875, KL Reg = 0.0032109126914292574\n",
      "Episode 88: Policy Loss = 40.94596862792969, Value Loss = 3632.5205078125, KL Reg = 0.0033267405815422535\n",
      "Episode 89: Policy Loss = 34.69622039794922, Value Loss = 3698.81689453125, KL Reg = 0.003082100534811616\n",
      "Episode 90: Policy Loss = 38.778076171875, Value Loss = 3792.481201171875, KL Reg = 0.003448265604674816\n",
      "Episode 91: Policy Loss = 37.18248748779297, Value Loss = 3832.8330078125, KL Reg = 0.003303602570667863\n",
      "Episode 92: Policy Loss = 35.48551559448242, Value Loss = 3166.802978515625, KL Reg = 0.003550510387867689\n",
      "Episode 93: Policy Loss = 20.86754608154297, Value Loss = 3463.888916015625, KL Reg = 0.0036803055554628372\n",
      "Episode 94: Policy Loss = 35.14580535888672, Value Loss = 3890.347900390625, KL Reg = 0.0035176482051610947\n",
      "Episode 95: Policy Loss = 34.446163177490234, Value Loss = 3992.851318359375, KL Reg = 0.003988741431385279\n",
      "Episode 96: Policy Loss = 11.324823379516602, Value Loss = 2572.237060546875, KL Reg = 0.0032906117849051952\n",
      "Episode 97: Policy Loss = 39.11376953125, Value Loss = 3243.10986328125, KL Reg = 0.0035628837067633867\n",
      "Episode 98: Policy Loss = 32.541053771972656, Value Loss = 3273.417236328125, KL Reg = 0.0034077567979693413\n",
      "Episode 99: Policy Loss = 51.95586395263672, Value Loss = 5815.44189453125, KL Reg = 0.0042447964660823345\n",
      "Episode 100: Policy Loss = 39.39640808105469, Value Loss = 3621.012939453125, KL Reg = 0.004401464946568012\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    discrete = False  # Set to True if action space is discrete\n",
    "    \n",
    "    RobustAgent = SAPPOAgent(state_dim, action_dim, discrete)\n",
    "    RobustAgent.train(env, max_episodes=100)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
